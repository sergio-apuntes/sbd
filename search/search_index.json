{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introducci\u00f3n","text":"<p>En este espacio se presentan algunos apuntes del m\u00f3dulo de Sistemas en Big Data en el Curso de espacializaci\u00f3n de Inteligencia Artificial y Big Data que se imparte en el I.E.S. Lluis Simarro de X\u00e0tiva.</p> <ul> <li>Elaboraci\u00f3n: Sergio Rey Mart\u00ednez</li> <li>Colaboraci\u00f3n : Jorge Soro Domenech</li> </ul> <p>Dpto. Inform\u00e1tica I.E.S. Dr. Lluis Simarro Lacabra X\u00e0tiva</p>"},{"location":"ApacheNiFi/1_ETL/","title":"1. ETL con Apache NiFi","text":"<p>Como ya vimos en el primer tema, ETL (Extract, Transform, Load) es un proceso fundamental en la gesti\u00f3n de datos que implica la extracci\u00f3n de datos de diversas fuentes, su transformaci\u00f3n para cumplir con los requisitos empresariales y su carga en un sistema de almacenamiento, como una base de datos relacional.</p> <p>En este caso, el proceso ETL lo vamos a realizar utilizando Apache NiFi, haremos usu de la base de datos NoSQL mongoDB que ya hemos visto en el tema anterior, pero tambi\u00e9n usaremos una Base de Datos Relacional como ser\u00e1 en este caso PostgreSQL, por lo que en este tema, veremos brevemente esta base de datos.</p> <p>As\u00ed pues los pasos que seguiremos:</p> <ol> <li>Instalaci\u00f3n y configuraci\u00f3n de PostgreSQL</li> <li>Instalaci\u00f3n y configuraci\u00f3n de Apache NiFi</li> <li>ETL usando Apache NiFi junto con PostgreSQL y MongoDB</li> </ol>"},{"location":"ApacheNiFi/2_PostgreSQL/","title":"2. PostgreSQL","text":"<p><code>PostgreSQL</code>, tambi\u00e9n llamado <code>Postgres</code>, es un sistema de gesti\u00f3n de bases de datos relacional orientado a objetos y de c\u00f3digo abierto, publicado bajo la licencia PostgreSQL,1\u200b similar a la BSD o la MIT. </p> <p>Los datos de los sistemas de Big Data se pueden guardar de formas diferentes. Para nuestro m\u00f3dulo en concreto, necesitamos una base de datos donde cargar (ETL) los datos extra\u00eddos antes de su tratamiento final, y utilizaremos esta base de datos.</p>"},{"location":"ApacheNiFi/2_PostgreSQL/#instalacion-de-postgresql","title":"Instalaci\u00f3n de <code>PostgreSQL</code>","text":""},{"location":"ApacheNiFi/2_PostgreSQL/#instalacion-en-ubuntu-derivados-debian","title":"Instalaci\u00f3n en Ubuntu (derivados Debian)","text":"<p>PostgreSQL se encuentra en el repositorio apt de Ubuntu por lo cual la instalaci\u00f3n es sencilla. Se recomienda seguir los pasos de instalacion y configuraci\u00f3n de la web de Ubuntu Server - Install PostgreSQL</p> <p>No obstante, los pasos son senciallos y para dejar el sistema preparado basta con seguir ejecutar: </p> <pre><code>sudo apt install postgresql\n</code></pre> <p>Para su configuraci\u00f3n, en primer lugar editamos el fichero de configuraci\u00f3n y permitimos que se pueda conectar desde cualqier direcci\u00f3n: </p> <pre><code>sudo nano /etc/postgresql/16/main/postgresql.conf\n</code></pre> <p>Localizamos la l\u00ednea </p> <pre><code>#listen_addresses = \u2018localhost\u2019\n</code></pre> <p>y la cambiamos por </p> <pre><code>listen_addresses = '*'\n</code></pre> <p>Continuamos asignando una contrase\u00f1a al usuario <code>postgres</code> para poder entrar en el sistema. Para ello ejecutamos:</p> <pre><code>sudo -u postgres psql template1\n</code></pre> <p>con esto entrmos en la consola de postgres, dentro de la base de datos template1, y ah\u00ed podemos ejecutar la siguiente instrucci\u00f3n donde podemos especificar nuestra contrase\u00f1a:</p> <pre><code>ALTER USER postgres with encrypted password 'your_password';\n</code></pre> <p>Despu\u00e9s de configurar la contrase\u00f1a, debemos permitir autentificarse en el servidor para lo cual editaremos el fichero <code>/etc/postgresql/16/main/pg_hba.conf</code> y permitiremos autentificacion mediante protocolo <code>md5</code>. Para ello</p> <p><pre><code>sudo nano /etc/postgresql/16/main/pg_hba.conf\n</code></pre> y localizamos la l\u00ednea </p> <pre><code>host    all     all     127.0.0.1/32    scram-sha-256\n</code></pre> <p>y la sustituimos por </p> <pre><code>host    all     all     0.0.0.0/0    md5\n</code></pre> <p>lo que permitir\u00e1 autentificarmos por medio de <code>md5</code> desde cualquier equipo. Quedar\u00e1 algo as\u00ed: </p> /etc/postgresql/16/main/pg_hba.conf <p>y ya tenemos el sistema preparado para conectarnos desde el exterior y con su contrase\u00f1a establacecida, as\u00ed que reiniciamos el servicio:</p> <pre><code>sudo systemctl restart postgresql.service\n</code></pre> <p>y ya tenemos el sistema preparado.</p>"},{"location":"ApacheNiFi/2_PostgreSQL/#instalacion-en-centos-derivados-red-hat","title":"Instalaci\u00f3n en CentOS (derivados Red Hat)","text":"<p>Para la instalaci\u00f3n de <code>Postgres</code> en <code>CentOS</code> seguiremos el siguiente Articulo: How to install PostgreSQL on CentOS</p> <p>Sobre esta gu\u00eda, cabe aconsejar realizar los siguientes cambios en el fichero de configuraci\u00f3n <code>/var/lib/pgsql/data/pg_hba.conf</code> (el cual debe editarse como <code>root</code>)</p> <p>Observar en la imagen que se ha cambiado peer e ident por md5 para las conexiones locales (desde el terminal) o mediante IPv4, de esta forma podremos conectando usando un usuario y contrase\u00f1a cualquiera que se encuentre en Postgres</p> <p>Nota: Si este cambio no se realiza, al intentar conectarnos, encontraremos un error del tipo: <code>PSQLException: FATAL: la autentificaci\u00f3n Ident fall\u00f3 para el usuario \u00absergio\u00bb</code></p>"},{"location":"ApacheNiFi/2_PostgreSQL/#creacion-y-acceso-de-rolesusuarios-y-bases-de-datos-desde-consola","title":"Creaci\u00f3n y acceso de roles/usuarios y bases de datos desde consola","text":"<p>Por defecto, la autentificaci\u00f3n es mediante roles. Al principio se asocia un rol a una cuenta existente en nuestro sistema Linux. Esta cuenta tiene el nombre de postgres y se crea autom\u00e1ticamente al instalar <code>PostgreSQL</code>. </p> <p>No hay una contrase\u00f1a por defecto del rol postgres. El modo de autenticaci\u00f3n por defecto para <code>PostgreSQL</code> est\u00e1 configurado como <code>ident</code>, no como sql DB user/password. Lo que realmente significa es que para conectarse correctamente a <code>PostgreSQL</code> debe iniciar sesi\u00f3n como el usuario correcto del sistema operativo que se utiliz\u00f3 para instalarlo. Pr\u00e1cticamente se autentifica con el usuario del SO, por eso hemos realizado los cambios descritos anteriormente.</p> <p>Por lo tanto, para comenzar la primera vez en el terminal de la base de datos PostgreSQL, debemos cambiarnos a este usuario postgres:</p> <p><pre><code>$ sudo -i -u postgres   # cambiamos al usuario postgres \n</code></pre> Una vez estamos dentro de la terminal del usuario postgres, ya tenemos permisos para entrar en la propia terminal de PostgreSQL, mediante el comando <code>psql</code></p> <pre><code>postgres@server:~$ psql # para entrar \n\npostgres=# \\q       # para salir\n</code></pre> <p>y ya que estamos</p> <pre><code>postgres=# \\l               # listado de base de datos\npostgres=# \\d               # listado de tablas de base de datos activa\npostgres=# \\c &lt;database&gt;    # para cambiar de base de datos\n</code></pre> <p>El rol de <code>postgres</code> es administrador, y aunque nosotros podemos utilizarlo, cabe indicar que no es correcto utilizarlo para acceder al sistema, es mejor crear nuevos roles, e incluso crear un rol para cada base de datos nueva que utilicemos.</p> <p>Para crear nuevos roles, normalmente lo hacemos desde el usuario linux <code>postgres</code>:</p> <p><pre><code>postgres@server:~$ createuser --interactive\n</code></pre> En este caso nos pide el usuario y la contrase\u00f1a</p> <p>Una vez creado el usuarios, le debemos asignar una contrase\u00f1a y darle los permisos necesarios:</p> <p><pre><code>postgres=# ALTER USER sergio WITH ENCRYPTED PASSWORD 'sergio'; \npostgres=# GRANT ALL PRIVILEGES ON DATABASE &lt;basededatos&gt; to &lt;usuario&gt;; \n</code></pre> por supuesto, antes de dar privilegios de acceso a una base de datos, se debe crear desde el usuario <code>postgres</code>:</p> <pre><code>$ createdb pruebas\n</code></pre> <p>Tip</p> <p>Todo lo anterior lo podemos realizar directamente una vez estamos el el terminal de <code>postgres</code>, creamos los roles y la base de datos utilizando sentencias SQL</p> <pre><code>postgres=# CREATE ROLE sergio LOGIN PASSWORD 'sergio';\nCREATE ROLE\npostgres=# CREATE DATABASE pruebas WITH OWNER = sergio;\nCREATE DATABASE\n</code></pre> <p>Nota</p> <p>Normalmente, si creamos un rol en Postgres, debemos tener un usuario con el mismo nombre en nuestro sistemas Linux, de forma que ambos est\u00e1n ligados.</p> <p>Para entrar en una base de datos desde el terminal</p> <pre><code>psql -d pruebas         # para entrar en la BBDD (si estamos en un usuario autorizado)\nsu sergio               # para cambiarnos a un usuario autorizada, antes de entrar en la BBDD \n\npsql -U sergio -d pruebas --password    # para entrar en la base de datos directamente con un usuario determinado.\n</code></pre> <p>Info</p> <p>Si el usuario no lo tenemos creado en el sistema, debemos crearlo <pre><code>sudo useradd -m sergio          # creamos el usuario\nsudo passwd sergio              # le damos contrase\u00f1a para activarlo.\nsudo usermod -aG sudo sergio   # para hacer meter el usuario en el grupo \"sudo\". Esto no es necesario.\n</code></pre></p> <p>Desde aqu\u00ed, ya podemos crear tablas, y trabajar datos desde este terminal.</p> <p>Una vez creada una base de datos, es interesante probar la conexi\u00f3n que hemos establecido para verificar que funciona, se podr\u00eda hacer desde el terminal, mediante el comando <code>psql</code>:</p> <p><pre><code>psql -d postgresql://127.0.0.1:5432/pruebasnifi?user=sergio\n</code></pre> donde especificamos la base de datos y el usuario, en este ejemplo son pruebasnifi y sergio respectivamente </p>"},{"location":"ApacheNiFi/2_PostgreSQL/#dbeaver","title":"DBeaver","text":"<p><code>DBeaver</code> es una aplicaci\u00f3n de software cliente de SQL y una herramienta de administraci\u00f3n de bases de datos muy ligera y sencilla. Es menos pesada que herramientas como <code>pgAdmin</code>, y adem\u00e1s mediante esta herramienta podemos instalar los drivers necesarios para la posterior conexi\u00f3n de <code>Apache NiFi</code> a nuestras bases de datos en <code>PostgreSQL</code>, por ejemplo, por lo que vamos a hacer uso de esta herramienta.</p> <p>Para su instalaci\u00f3n descargar versi\u00f3n Comunity desde : https://dbeaver.io/download/</p> <p>Por supuesto descargamos el paquete adecuado a nuestro sistemas operativo.</p> <p>Despu\u00e9s, abrimos un terminal y instalamos y ejecutamos (en CentOS):</p> <pre><code>sudosudo dpkg -i dbeaver-&lt;version&gt;.deb    # instalaci\u00f3n \ndbeaver &amp;                                 # ejecuci\u00f3n de dbeaver\n</code></pre> <p>Una vez tenemos instalado DBeaver, lo inciamos y configuramos la conexi\u00f3n a nuestra base de datos PostgreSQL:</p> Creando conexi\u00f3n con BBDD PostgreSQL <p>Cuaod Probemos la conexi\u00f3n o intentemos conectarnos, la primera vez nos dir\u00e1 que nos faltan los drivers para conectarse a PostgreSQL, y nos permite la instalaci\u00f3n automatica.</p> Instalaci\u00f3n autom\u00e1tica de drivers para PostgreSQL <p>A partir de este momento, ya tenemos conexi\u00f3n con la base de datos y podemos utilizar DBeaver para crear bases de datos y tablas.</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/","title":"3. Apache NiFi","text":"<p><code>Apache NiFi</code> es una plataforma de procesamiento y log\u00edstica de datos en tiempo real, que permite automatizar el movimiento de datos entre diferentes sistemas. </p> <p>Es un sistema distribuido, Open Source y desarrollado por la Apache Software Foundation. </p> <p><code>Apache NiFi</code> ofrece una gesti\u00f3n de flujos de datos complejos, con trazabilidad y control de los datos que se reciben, transforman, env\u00edan o descartan34.</p> <p>En la web del proyecto podemos encontrar la siguiente definici\u00f3n</p> <p>An easy to use, powerful, and reliable system to process and distribute data.</p> <p><code>NiFi</code> (o Ni-Fi) ha sido dise\u00f1ado para poder automatizar de una manera eficiente y visual los flujos de datos entre distintos sistemas: ingesta, enrutado y gesti\u00f3n. Para ello, cuenta con m\u00e1s de 300 conectores externos ya implementados y adem\u00e1s es posible a\u00f1adir conectores a medida.</p> <p>Uno de los puntos fuertes de <code>NiFi</code> es la capacidad para programar flujos de datos arrastrando y conectando los componentes necesarios sobre los canvas de la web de administraci\u00f3n. No es necesario por tanto tener conocimientos de programaci\u00f3n espec\u00edficos, sino entender y configurar correctamente cada uno de los componentes que se quieren usar.</p> <p>Aunque se pueda considerar una herramienta ETL, <code>NiFi</code> no est\u00e1 realmente optimizado para realizar transformaciones de datos complejas o pesadas. Es posible realizar transformaciones de datos ligeras pero no es un motor de transformaciones batch completo. A\u00fan as\u00ed es com\u00fan su uso integrado en sistemas Big Data, ya que ofrece muchas ventajas como herramienta de automatizaci\u00f3n de ingestas de datos y para realizar transformaciones y limpiezas sencillas.</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#instalacion-apache-nifi","title":"Instalaci\u00f3n <code>Apache Nifi</code>","text":"<p>Para la instalaci\u00f3n de NiFi, descargamos la \u00faltima versi\u00f3n desde la web de nifi; archivo binario.</p> <p>Atenci\u00f3n</p> <p>Cuidado, si descargamos la version 2.0 necesitamos realizar una configuraci\u00f3n m\u00e1s avanzada del servidor, incluidos ceertificados, por lo que nosotros vamos a trabajar con version 1.xxx</p> Descargamos versi\u00f3n 1.xx para evitar problemas de configuraci\u00f3n <p>Podemos descargar directamente desde la web mediante <code>wget</code>:</p> <p><pre><code>wget https://dlcdn.apache.org/nifi/1.28.0/nifi-1.28.0-bin.zip\n</code></pre> Lo descomprimimos y ...</p> <pre><code>unzip nifi-1.28.0-bin.zip\n</code></pre> <p>si no tenemos instalado 'unzip' en nuestro sistema, recordad que para instalar</p> <pre><code>sudo apt install unzip\n</code></pre> <p>y lo dejamos en la ubicaci\u00f3n adecuada y ya podemos ejecutarlo.</p> <pre><code>sudo mv nifi-1.28.0 /opt\ncd /opt/nifi-1.28.0/bin\n./nifi.sh start             # iniciamos nifi en background\n</code></pre> <p>El sistema debe tener instalada una m\u00e1quina virtual de Java (JRE) para poder ejecutarse, asi que recordad que para instalar, por ejemplo:</p> <pre><code>sudo apt install default-jre\n</code></pre> <p>El usuario se puede crear v\u00eda comandos por cmd, o se puede coger el que genera autom\u00e1ticamente en el log NiFi, algo asi (fichero nifi-app.log):</p> <pre><code>$ cat ../logs/nifi-app.log | grep Generated\nGenerated Username [80e91118-b222-4b47-8dab-63a8deb7905d]\nGenerated Password [zavwbGlRcYeky51Bxc0zbVN8hj2bE61u]\n</code></pre> Iniciando nifi y obteniendo usuario y contrase\u00f1a por defecto <p>Para establecer un usuario y contrase\u00f1a, lo podemos hacer con el siguiente comando</p> <p><pre><code>./bin/nifi.sh set-single-user-credentials sergio sergiosergio  \n</code></pre> usuario que desees y contrase\u00f1a de 12 caracteres m\u00ednimo</p> <p>Por \u00faltimo, comentar que hay diferentes forma de iniciar/operar con Apache Nifi, arrancando NiFi cada vez que deseemos usarlo desde la l\u00ednea de comandos o creando un servicio y que inicia de forma autom\u00e1tica</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#iniciando-desde-linea-de-comandos","title":"Iniciando desde l\u00ednea de comandos","text":"<p>Si queremos iniciar Nifi desde la l\u00ednea de comandos (recomendado sobre todo en nuestro primera arranque), tenemos los siguientes comandos:</p> <pre><code>./nifi.sh start   # ejecuci\u00f3n en segundo plano\n./nifi.sh run     # ejecuci\u00f3n en primer plano\n./nifi.sh status  # saber estado... si se esta ejecutando\n./nifi.sh stop    # para \n</code></pre> <p>Dependiendo de la m\u00e1quina donde la ejecutemos, puede tarda un poco en ponerse en marcha, hasta 5 minutos.</p> <p>Luego accedemos en <code>https://localhost:8443/nifi</code></p> <p>Note</p> <p>Si accedemos desde una m\u00e1quina remota, entonces debemos cambiar la configuraci\u00f3n del archivo conf/nifi.properties</p> <p>Debemos cambiar las siguientes l\u00edneas:</p> <pre><code>nifi.web.http.host=0.0.0.0\nnifi.web.http.port=8080\n</code></pre> <p> Iniciando nifi y obteniendo usuario y contrase\u00f1a por defecto </p> <p>Note</p> <p>Si queremos instalar la versi\u00f3n 2.0 o posterior tenenemos que cambiar los siguiente par\u00e1metros (para poder entrar como http en vez de https, porque si no, tenemos que instalar certificados y es complejo).</p> <pre><code># quitamos la seguridad\nnifi.remote.input.secure=false\n\n# configuramos para acceder desde cualquier equipo por http\nnifi.web.http.host=0.0.0.0\nnifi.web.http.port=8080\n\n# deshabilitamos acceder por https\nnifi.web.https.host=\nnifi.web.https.port=\n</code></pre>"},{"location":"ApacheNiFi/3_ApacheNiFi/#iniciando-como-servicio","title":"Iniciando como servicio","text":"<p>Por \u00faltimo, si queremos que Nifi se instale como servicio y se inicie de forma autom\u00e1tica cada vez que iniciamos el sistema, realizamos los suguientes pasos</p> <pre><code>sudo /opt/nifi/bin/nifi.sh install\nsudo systemctl daemon-reload\nsudo systemctl start nifi\nsudo systemctl enable nifi\n</code></pre> Iniciando NiFi como servicio <p>M\u00e1s informaci\u00f3n en la web de apache nifi</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#terminologia-nifi","title":"Terminolog\u00eda NiFi","text":""},{"location":"ApacheNiFi/3_ApacheNiFi/#los-conceptos-fundamentales-de-nifi","title":"Los conceptos fundamentales de NiFi","text":"<p><code>Nifi</code> esta basado en FBP (Flow Based Programming), que es un paradigma de programaci\u00f3n que define aplicaci\u00f3n como cajas negras (procesos), los cuales intercambian datos entre conexiones predefinidas.</p> <p>A continuaci\u00f3n se indican algunos de los conceptos principales de NiFi y c\u00f3mo se mapean a FBP:</p> T\u00e9rmino de NiFi T\u00e9rmino de FBP Descripci\u00f3n FlowFile Paquete de Informaci\u00f3n Un FlowFile representa cada objeto que se mueve a trav\u00e9s del sistema y para cada uno, NiFi realiza un seguimiento de un mapa de pares clave/valor de atributos y su contenido asociado de cero o m\u00e1s bytes. FlowFile Processor Caja Negra Los procesadores realizan el trabajo. En t\u00e9rminos de [eip], un procesador realiza alguna combinaci\u00f3n de enrutamiento de datos, transformaci\u00f3n o mediaci\u00f3n entre sistemas. Los procesadores tienen acceso a los atributos de un FlowFile dado y su flujo de contenido. Los procesadores pueden operar en cero o m\u00e1s FlowFiles en una unidad de trabajo dada y ya sea comprometer ese trabajo o revertirlo. Connection B\u00fafer Acotado Las conexiones proporcionan el enlace real entre procesadores. Act\u00faan como colas y permiten que varios procesos interact\u00faen a diferentes velocidades. Estas colas pueden priorizarse din\u00e1micamente y pueden tener l\u00edmites superiores de carga, lo que permite la retropresi\u00f3n. Flow Controller Programador El Flow Controller mantiene el conocimiento de c\u00f3mo se conectan los procesos y gestiona los hilos y las asignaciones de los mismos que todos los procesos utilizan. El Flow Controller act\u00faa como el intermediario que facilita el intercambio de FlowFiles entre procesadores. Process Group Subred Un Process Group es un conjunto espec\u00edfico de procesos y sus conexiones, que pueden recibir datos a trav\u00e9s de puertos de entrada y enviar datos a trav\u00e9s de puertos de salida. De esta manera, los grupos de procesos permiten la creaci\u00f3n de componentes completamente nuevos simplemente mediante la composici\u00f3n de otros componentes. <p>Estas cajas negras (procesos) pueden ser combinados en dataflows. </p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#dataflow","title":"DataFlow","text":"<ul> <li><code>Dataflow</code> o <code>flujo de dato</code> son los pasos de los datos desde el origen al destino, con o sin transformaci\u00f3n en medio.</li> </ul> <p>Los datos pueden ser de diversos formatos, csv, json, videos, audios, .... </p> <p><code>Apache Nifi</code> esta dise\u00f1ado para automatizar el movimiento de datos.</p> <p>No es recomendable para hacer transformaciones grandes o de tipo batch, en ese caso tenemos spark</p> <p>Esta basada en la programaci\u00f3n basada en flujo, basada en unos procesadores que se unen mediante conexiones.</p> <p>Estos procesadores son como cajas negras, y los conectores cogen los datos de un procesador y lo entregan a otro, de forma que a los procesadores no les interesa la salida, solo el proceso</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#flowfile","title":"FlowFile","text":"<p>Los ficheros que se generan entre diferentes procesadores se llaman <code>flowfiles</code>, con paquetes de datos (como ficheros) compuesto por contenido y atributos o metadatos. Los atributos contienen informaci\u00f3n del contenido; fecha de creaci\u00f3n, nombre, o informaci\u00f3n que a\u00f1adamos.</p> <p>Contiene datos. Abstracci\u00f3n de los datos en NiFi --&gt;  CSV,JSON,XML,SQL,Binary,etc</p> <p>Tiene dos componentes: - Content:  contiene los datos actuales. - Attributes: representa los metadatos del fichero</p> <p>Son ficheros persistentes en disco.</p> <p>Este paquete de datos que viaja por el flow entre los procesadores est\u00e1 compuesto por un puntero al propio dato \u00fatil o contenido (un array de bytes) y metadatos asociados llamados atributos. Los atributos pares clave-valor editables y NiFi los usa para enriquecer la informaci\u00f3n de provenance. Los metadatos m\u00e1s importantes son el identificador (uuid), el nombre del fichero (filename) y el path. Para acelerar el rendimiento del sistema, el flowfile no contiene el propio dato, sino que apunta al dato en el almacenamiento local. Muchas de las operaciones que se realizan en NiFi no alteran el propio dato ni necesitan cargarlo en memoria. En concreto, el dato se encuentra en el llamado repositorio de contenido (Content Repository) Flowfile de Apache NiFi.</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#processors","title":"Processors","text":"<ul> <li><code>Processor</code>: un Processor puede generar un nuevo \"FlowFile\" para procesar o ingestar un existente FlowFile desde cualquier origen. Todos los processors pueden ser conectados con otros. Estos, son enlazados v\u00eda conexiones con links. Cada conexi\u00f3n tendr\u00e1 una cola \"Queue for FlowFiles\".</li> </ul> <p>Puede a\u00f1adir, actualizar y borrar atributos de un FlowFile Puede cambiar el contenido a un FlowFile.</p> <ul> <li> <p>Input/Output: entrada y salida s\u00f3n utilizados para mover datos entre \"Process Group\".</p> </li> <li> <p>Controller Service: servicio compartido que puede usado por un Processor. Este servicio puede mantener conexiones con bases de datos. Por ejemplo, CSV Reader, JSON Writer, etc.</p> </li> </ul> <p>Los procesadores son los encargados de realizar las transformaciones o acciones sobre los datos. Se pueden configurar para que se ejecuten seg\u00fan una temporizaci\u00f3n, tipo cron. Proporcionan una interface para acceder a los flowfiles. Hay procesadores ya creados y publicados o podemos crear nuevos, utilizando java.</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#processors-mas-usados","title":"Processors m\u00e1s usados","text":"<p>Apache Nifi ofrece diferentes tipos de procesadores, y los m\u00e1s utilizados o importantes son: </p> <p>Ingesta de datos</p> <pre><code>GenerateFlowFiles\nGetFile\nGetFTP\nGetSFTP\nGetJMSQueue\nGetJMSTopic\nGetHTTP\nListenHTTP\nListenUDP\ngetHDFS\nGetKafka\nQueryDatabaseTable\nGetMongo\nGetTwitter\nListHDFS / FetchHDFS\n</code></pre> <p>Transformaci\u00f3n del datos. Permiten transformar datos antes de almacenar en destino</p> <pre><code>ConvertRecord\nUpdateRecord\nConvertJSONToSQL\nReplaceText\nCompressContent\nConvertCharacterSet\nEncryptContent\nTransformXml\nJoltTransformJSON\n</code></pre> <p>Env\u00edo de datos. Ayudan a enviar los datos a un destino</p> <pre><code>PutEmail\nPutFile\nPutFTP\nPutSFTP\nPutJMS\nPutSQL\nPutKafka\nPutMongo\nPutHDFS\n</code></pre> <p>Control y revisi\u00f3n. Enrutan el dato por el flujo adecuado</p> <pre><code>ControlRatte\nDetectDuplicate\nDistributeLoad\nRouteOnAttribute\nRouteOnAttribute\nRouteOnContent\nScanAttribute\nScanContent\nValidateXml\nValidateCSV\n</code></pre> <p>Acceso a base de datos</p> <pre><code>ConvertJSONToSQL\nExecuteSQL\nPutSQL\nSelectHiveQL\nPutHiveQL\nListDatabaseTables\n</code></pre> <p>Extracci\u00f3n de atributos. A\u00f1ade o extrae atributos de los FlowFiles</p> <pre><code>EvaluateJsonPath\nEvaluateXPath\nEvaluateXQuery\nExtractText\nHashAttribute\nHashContent\nIdentifyMimeType\nUpdateAttribute\nLogAttribute\n</code></pre> <p>Sistema</p> <pre><code>ExecuteProcess\nExecuteStreamCommand\n</code></pre> <p>Agregaciones y Splits</p> <pre><code>SplitText\nSplitJson\nSplitXml\nSplitRecord\nSplitContent\nUnpackContent\nSegmentContent\nMergeContent\nQueryRecord\n</code></pre> <p>Http y udp</p> <pre><code>GetHTTP\nListenHTTP\nInvokeHTTP\nPostHTTP\nHandleHttpRequest\nHandleHttpRespone\nListenUDP\nPutUDP\nListenUDPRecord\n</code></pre> <p>AWS</p> <pre><code>FetchS3Object\nPutS3Object\nPutSNS\nGetSQS\nPutSQS\nDeleteSQS\nGetDynamoDB\nPutDynamoDB\nPutLambda\n</code></pre>"},{"location":"ApacheNiFi/3_ApacheNiFi/#conexiones","title":"Conexiones","text":"<p>Las Conexiones permiten transmitir flowFiles entre procesadores. Se encargan de controlar colas y su caducidad, por ejemplo ante procesadores que van a diferentes velocidades, o encola o deciden que datos tienen m\u00e1s prioridad o que datos ya han quedado obsoletos.</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#process-groups","title":"Process Groups","text":"<p>Los Process Groups es la uni\u00f3n de varios procesadores que tiene una tarea de forma agrupada.</p> <p>Son conjuntos de componentes Processor combinados. Ayudan a mantener un gran y complejo dataflow.</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#controller-services","title":"Controller Services","text":"<p>Los <code>Controller Services</code> en Apache <code>NiFi</code> son servicios compartidos que pueden ser utilizados por los procesadores, tareas de informes y otros servicios de controlado. Estos servicios proporcionan una funcionalidad com\u00fan que puede ser utilizada por varios componentes de NiFi. </p> <p>Algunos ejemplos de servicios de controlador incluyen servicios de autenticaci\u00f3n, servicios de encriptaci\u00f3n y servicios de base de datos.</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#apache-nifi-expression-language","title":"Apache NiFi Expression Language","text":"<p>El lenguaje de expresiones en Apache NiFi es una forma de especificar patrones de texto que se pueden utilizar para buscar, reemplazar o extraer informaci\u00f3n de los atributos y el contenido de los archivos de flujo. El lenguaje de expresiones de NiFi permite referenciar estos atributos, compararlos con otros valores y manipular sus valores. </p> <p>El lenguaje de expresiones de NiFi siempre comienza con el delimitador de inicio <code>${</code> y termina con el delimitador final <code>}</code>. Entre los delimitadores se encuentra el texto de la expresi\u00f3n en s\u00ed. En su forma m\u00e1s b\u00e1sica, la expresi\u00f3n puede consistir en solo un nombre de atributo. </p> <p>Por ejemplo, <code>${filename}</code> devolver\u00e1 el valor del atributo filename. En un ejemplo un poco m\u00e1s complejo, podemos devolver una manipulaci\u00f3n de este valor. Podemos, por ejemplo, devolver una versi\u00f3n en may\u00fasculas del nombre de archivo llamando a la funci\u00f3n <code>${filename:toUpper()}</code>. En este caso, referenciamos el atributo y luego manipulamos este valor usando la funci\u00f3n toUpper. </p> <p>Otro ejemplo de uso de este tipo de expresiones lo encontramos en la pr\u00e1ctica 8, de forma que generamos un fichero json con valores aleatorios</p> <pre><code>{\n    \"title\": \"mr\",\n    \"first\": \"John ${random():mod(10):plus(1)}\",\n    \"last\": \"Doe ${random():mod(10):plus(1)}\",\n    \"email\": \"johndoe${random():mod(10):plus(1)}nail.com\",\n    \"created_on\": \"${now():toNumber()}\"\n}\n</code></pre> <p>Sirva como base el documento Apache Nifi Expression Language Cheat Sheet</p> <p>Mas Informaci\u00f3n en Apache NiFi Expression Language Guide</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#expresiones-regulares","title":"Expresiones regulares","text":"<p>Las expresiones regulares son una forma de especificar patrones de texto que se pueden utilizar para buscar, reemplazar o extraer informaci\u00f3n de cadenas. Apache NiFi es una plataforma de gesti\u00f3n de flujos de datos que permite procesar y distribuir datos de forma eficiente y fiable. NiFi tiene su propio lenguaje de expresi\u00f3n, que se puede utilizar para referenciar y manipular los atributos y el contenido de los archivos de flujo. El lenguaje de expresi\u00f3n de NiFi soporta el uso de expresiones regulares para realizar operaciones como:</p> <ul> <li>Validar el formato o el contenido de un atributo o una cadena usando la funci\u00f3n matchesRegex.</li> <li>Extraer una parte de un atributo o una cadena usando la funci\u00f3n replaceRegex.</li> <li>Reemplazar una parte de un atributo o una cadena usando la funci\u00f3n replaceAll.</li> <li>Dividir un atributo o una cadena en una lista usando la funci\u00f3n split.</li> </ul> <p>Algunos ejemplos de uso de expresiones regulares en NiFi son:</p> <p>Ejemplo 1</p> <p>Para extraer el nombre de dominio de una URL, se puede usar la expresi\u00f3n </p> <p><code>${url:replaceRegex('^(?:https?://)?([^/]+)(.*)$', '$1')}</code></p> <p>En este ejemplo, si el atributo <code>url</code> tiene el valor https://nifi.apache.org/some%20value%20with%20spaces, la expresi\u00f3n devolver\u00e1 nifi.apache.org.</p> <p>Ejemplo 2</p> <pre><code>Para validar que un atributo email tiene un formato v\u00e1lido, se puede usar la expresi\u00f3n\n</code></pre> <p><code>${email:matchesRegex('^[\\\\w.-]+@[\\\\w.-]+\\\\.[\\\\w]{2,}$')}</code></p> <pre><code>Por ejemplo, si el atributo email tiene el valor *user@example.com*, la expresi\u00f3n devolver\u00e1 true.\n</code></pre> <p>Ejemplo 3</p> <pre><code>Para reemplazar los espacios por guiones en un atributo filename, se puede usar la expresi\u00f3n\n\n`${filename:replaceAll('\\\\s', '-')}`\n\nPor ejemplo, si el atributo *filename* tiene el valor *my document.txt*, la expresi\u00f3n devolver\u00e1 *my-document.txt*.\n</code></pre> <p>Ejemplo 4</p> <pre><code>Para dividir un atributo tags en una lista separada por comas, se puede usar la expresi\u00f3n\n\n`${tags:split(',')}`\n\nPor ejemplo, si el atributo tags tiene el valor *nifi,data,regex*, la expresi\u00f3n devolver\u00e1 '[nifi, data, regex]'.\n</code></pre> <p>Ante cualquier duda, las expresiones regulares de Apache NiFi siguen las nomenclatura de Java</p> <p>A continuaci\u00f3n, te presento una tabla con algunos ejemplos de expresiones regulares de Java y su descripci\u00f3n:</p> Expresi\u00f3n regular Descripci\u00f3n \\d Coincide con un d\u00edgito del 0 al 9 \\w Coincide con una letra, un d\u00edgito o un gui\u00f3n bajo \\s Coincide con un espacio en blanco . Coincide con cualquier car\u00e1cter excepto el salto de l\u00ednea [abc] Coincide con cualquiera de los caracteres a, b o c [^abc] Coincide con cualquier car\u00e1cter que no sea a, b o c [a-z] Coincide con cualquier letra min\u00fascula del alfabeto ingl\u00e9s [A-Z] Coincide con cualquier letra may\u00fascula del alfabeto ingl\u00e9s [0-9] Coincide con cualquier d\u00edgito del 0 al 9 [a-zA-Z0-9] Coincide con cualquier letra o d\u00edgito ^ Coincide con el inicio de la cadena o de la l\u00ednea $ Coincide con el final de la cadena o de la l\u00ednea ? Indica que el car\u00e1cter o la expresi\u00f3n anterior puede aparecer cero o una vez + Indica que el car\u00e1cter o la expresi\u00f3n anterior puede aparecer una o m\u00e1s veces * Indica que el car\u00e1cter o la expresi\u00f3n anterior puede aparecer cero o m\u00e1s veces {n} Indica que el car\u00e1cter o la expresi\u00f3n anterior debe aparecer exactamente n veces {n,m} Indica que el car\u00e1cter o la expresi\u00f3n anterior debe aparecer entre n y m veces, ambos inclusive {n,} Indica que el car\u00e1cter o la expresi\u00f3n anterior debe aparecer al menos n veces {,m} Indica que el car\u00e1cter o la expresi\u00f3n anterior debe aparecer como m\u00e1ximo m veces <code></code> ( ) Agrupa una subexpresi\u00f3n dentro de la expresi\u00f3n regular \\ Escapa el car\u00e1cter siguiente para que se interprete como un car\u00e1cter normal <p>Otros ejemplos de uso: </p> <p>Example</p> <p>Por ejemplo, para validar que una cadena es una direcci\u00f3n de correo electr\u00f3nico v\u00e1lida, se puede usar la siguiente expresi\u00f3n regular:</p> <pre><code>^[\\\\w.-]+@[\\\\w.-]+\\\\.[\\\\w]{2,}$\n</code></pre> <p>Esta expresi\u00f3n regular significa lo siguiente:</p> <ul> <li>^: coincide con el inicio de la cadena.</li> <li>[\\w.-]+: coincide con una o m\u00e1s repeticiones de letras, d\u00edgitos, puntos o guiones.</li> <li>@: coincide con el s\u00edmbolo arroba.</li> <li>[\\w.-]+: coincide con una o m\u00e1s repeticiones de letras, d\u00edgitos, puntos o guiones.</li> <li>\\.: coincide con un punto literal.</li> <li>[\\w]{2,}: coincide con dos o m\u00e1s repeticiones de letras o d\u00edgitos.</li> <li>$: coincide con el final de la cadena.</li> </ul> <p>Example</p> <p>El siguiente ejemplo de una expresi\u00f3n regular en Java para identificar un fichero con extensi\u00f3n <code>.json</code> ser\u00eda la siguiente:</p> <pre><code>.*\\.json\n</code></pre> <p>Esta expresi\u00f3n regular significa lo siguiente:</p> <ul> <li>.*: coincide con cualquier n\u00famero de caracteres, excepto el salto de l\u00ednea.</li> <li>.: coincide con un punto literal. Se usa el car\u00e1cter  para escapar el punto, ya que este es un metacar\u00e1cter que tiene otro significado en las expresiones regulares.</li> <li>json: coincide con la cadena literal \u201cjson\u201d.</li> </ul> <p>De esta forma, la expresi\u00f3n regular coincide con cualquier fichero que termine en \u201c.json\u201d, como por ejemplo \u201cdatos.json\u201d o \u201cconfiguracion.json\u201d. En la tabla siguiente se muestran otros ejemplos de expresiones regulares en Java y su descripci\u00f3n:</p> Expresi\u00f3n regular Descripci\u00f3n ^\\\\d{9}-[A-Z]$ Coincide con un NIF formado por 9 d\u00edgitos, un gui\u00f3n y una letra may\u00fascula. ^\\\\d{2}/\\\\d{2}/\\\\d{4}$ Coincide con una fecha en formato dd/mm/aaaa. ^\\+?\\\\d{1,3}-\\\\d{3}-\\\\d{3}-\\\\d{3}$ Coincide con un n\u00famero de tel\u00e9fono internacional con c\u00f3digo de pa\u00eds opcional. <sup>(?=.*[A-Z])(?=.*[a-z])(?=.*[0-9])(?=.*[@#$%</sup>&amp;+=]).{8,}$ Coincide con una contrase\u00f1a que tenga al menos 8 caracteres, una letra may\u00fascula, una letra min\u00fascula, un d\u00edgito y un car\u00e1cter especial."},{"location":"ApacheNiFi/3_ApacheNiFi/#uso-de-mongodb-con-apache-nifi","title":"Uso de <code>MongoDB</code> con <code>Apache Nifi</code>","text":"<p>Otros de los usos t\u00edpicos que podemos tener es el almacenamiento de un ETL utilizando <code>Apache Nifi</code> y <code>MongoDB</code></p> <p>Veamos la configuraci\u00f3n de <code>Nifi</code> para poder almacenar datos en <code>MongoDB</code></p> <p>En primer lugar para la ingesta de datos en MongoDB, utilizamos el Processor <code>PutMongoRecord</code> o <code>PutMongo</code></p> <p>Dentro del processor, debemos configurar dos servicios: </p> <p>Los cuales quedan configurados como sigue:</p> <p>El servicio <code>JsonTreeReader</code> no es necesario modificarlo, y el <code>MongoDBControllerService</code> simplemente asignamos la propiedad de Mongo URL</p>"},{"location":"ApacheNiFi/3_ApacheNiFi/#ejercicio-en-nifi","title":"Ejercicio en NiFi","text":"<p>De la AEMET podemos recuperar el tiempo de X\u00e0tiva : https://www.el-tiempo.net/api/json/v2/provincias/46/municipios/46145</p> <p>Planteamos un ejercicio para obtener los datos desde esta web, meterlos en una base de datos MongoDB y desde ah\u00ed pasar datos a una base de datos relacional en PostgreSQL</p> <p>Uno de los puntos a realizar ser\u00eda hacerlo todo con Docker-compose:</p> <pre><code>version: '3.8'\n\nservices:\n  postgres_dc:\n    image: postgres:latest\n    container_name: postgres_dc\n    environment:\n      POSTGRES_USER: sergio\n      POSTGRES_PASSWORD: sergio\n      POSTGRES_DB: test\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n      - ./data/postgres:/data\n\n  mongodb_dc:\n    image: mongo:latest\n    container_name: mongodb_dc\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - mongo_data:/data/db\n      - ./data/mongodb:/data\n\n  nifi_dc:\n    image: apache/nifi:latest\n    container_name: nifi_dc\n    ports:\n      - \"8443:8443\"\n    environment:\n      SINGLE_USER_CREDENTIALS_USERNAME: nifi\n      SINGLE_USER_CREDENTIALS_PASSWORD: nifinifinifi\n      NIFI_JVM_HEAP_MAX: 2g\n    volumes:\n      - nifi_data:/opt/nifi/nifi-current\n      - ./data/nifi:/data\n\nvolumes:\n  postgres_data:\n  mongo_data:\n  nifi_data:\n</code></pre> <p>Los puntos a tener en cuenta son:</p> <ul> <li>Para conectar nifi con MongoDB, debemos saber la IP del contenedor de MongoDB. Para ello usamos </li> </ul> <pre><code>docker-compose ps -q | xargs -I {} docker inspect -f '{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' {}\n</code></pre> <ul> <li>En todas los contenedores tenemos la carpeta <code>/data</code> para dejar datos, por ejemmplo se puede usar en Nifi si hacemos un <code>PutFile</code></li> </ul>"},{"location":"ApacheNiFi/ODI/","title":"Oracle Data Integrator","text":"ODI: Oracle Data Integrator Tabla de contenidos"},{"location":"ApacheNiFi/ODI/#introduccion-a-oracle-data-integrator-odi","title":"Introducci\u00f3n a <code>Oracle Data Integrator</code>. <code>ODI</code>","text":"<p>En la m\u00e1quina hay un documento llamado \"Information about this machine\" que tiene las contrase\u00f1as de todo el sistema. De mitad para abajo, no lo vamos a usar. Al final todoas las contrase\u00f1as son <code>oracle</code>o <code>welcome1</code></p> <p>La base es que todo lo guarda en repositorios.</p> <p>WebManual de ODI en oracle:  Integrating Oracle Data Integrator 12c (ODI) and Oracle GoldenGate 12c (OGG) </p> <p>Explicaci\u00f3n de la parte izquiera del interfaz de ODI</p> <ul> <li>Pesta\u00f1a <code>Designer</code>   Explicaci\u00f3n de qu\u00e9 es cada cosa.</li> <li> <p>M\u00f3dulos de conocimiento: Hay diferentes tipos de m\u00f3dulos de conocimiento, los mas interesante con los de Check y LKM que son los que cargan.</p> </li> <li> <p>Pesta\u00f1a <code>Modelos</code>: Aqui es donde esta la metadata de las tablas que se importan en ODI. Si se difine una tabla, un fichero o lo que sea, debe esta aqui a nivel l\u00f3gico.</p> </li> <li> <p>Pesta\u00f1a de <code>Objetos Globales</code></p> </li> <li> <p>Pesta\u00f1a de <code>Topology</code>   Tenemos la parte fisica, Conexto y la parte l\u00f3gica de la arquitectura, esto es lo m\u00e1s importante</p> <ul> <li>F\u00edsica: es aqui donde se especifican los elementos f\u00edsicos con los que trabajamos</li> <li>Arquitectura l\u00f3gica: Dado todas las conexiones f\u00edsicas, deben tener un nombre en la parte l\u00f3diga para poder referencialas</li> <li>Contexto: Relaciona las partes l\u00f3gicas con las f\u00edsicas.</li> <li>Repositorios: Es la informaci\u00f3n de c\u00f3mo se conecta, pero esto no lo vamos a tocar</li> </ul> </li> <li> <p>Pesta\u00f1a de <code>Operador</code>   Aqui es donde vamos a ver informaci\u00f3n de si funciona o no </p> </li> </ul>"},{"location":"ApacheNiFi/ODI/#demo-de-uso-con-odi","title":"Demo de uso con <code>ODI</code>","text":"<p>Los puntos 2 y 3 hacen una introducci\u00f3n de lo que se va a hacer. En este caso es una relaci\u00f3n de ventas y grupos de ventas.</p> <p>El punto 4 y 5 ya tienen c\u00f3mo se implementa un ETL en ODI. Realmente se comienza en el punto 4 paso a paso los pasos a seguir</p>"},{"location":"ApacheNiFi/ODI/#maquina-virtual","title":"Maquina virtual","text":"<p>Hay un fichero que indica donde esta todo: <code>Information about this machine</code>. Aqu\u00ed tenemos todo lo que esta instalado as\u00ed como usuarios y contrase\u00f1as de cada unos de los elementos que hay en el sistema.</p> <ul> <li><code>ODI 12c Getting Started Guide.pdf</code>, fichero donde esta la demo que nos explica paso a paso un ejemplo de utilizaci\u00f3n de ODI.</li> <li><code>SQL Developer</code>, el IDE de acceso a la base de datos</li> <li>Se debe probar. Al abrir el IDE, veremos que muestra todas las bases de datos</li> <li>Le damos a crear nuevo conexi\u00f3n, utilizando la plantilla existente y le metemos como nombre y usuario: <code>ODI_DEMO</code>, as\u00ed tenemos una conexi\u00f3n a la base de datos que solo tiene permisos sobre las tablas que a nosotros nos interesan.</li> <li>Ahora tenemos una interfaz sobre la que vamos a trabajar</li> </ul> <p>El propio ODI tiene por ejemplo un registro de errores que se guardar en la propia base de datos, por lo que es interesante poder visualizar y manipular los datos  - <code>ODI studio</code>: Se entra con la password <code>welcome1</code></p>"},{"location":"ApacheNiFi/ODI/#odi-studio","title":"ODI Studio","text":"<p>Cuando le damos a entrar, tenemos el listado en el desplegable de las diferentes demos.</p> <p>Si le damos al l\u00e1piz, podemos editar y podemos ver unos cuantos datos como por ejemplo que el usuario de la base de datos es <code>prod.odi_repo</code> y otros datos como . El usuario y el password de la conexi\u00f3n tambi\u00e9n se ve en el primer grupo de datos.</p> <p>Cuando abrimos ODI, vemos una parte del entorno, en Window podemos ver el resto de ventanas ocultas, como por ejemplo en <code>security</code>, podemos tambi\u00e9n acceder a el <code>log</code></p> <ul> <li><code>Topolog\u00eda</code>: se divide en tres partes</li> <li>Parte f\u00edsica, donde se definen todas las conexiones, por ejemplo en technologies-&gt;files, estan las ficheros de d\u00f3nde estan las cosas</li> <li>Parte l\u00f3gica que corresponde con la f\u00edsica</li> <li>Context: es el que relaciona la parte f\u00edsica con la l\u00f3gica</li> </ul> <p>En la parte superior de la ventana de Topolog\u00eda hay un bot\u00f3n con icono de f\u00e1brica donde podemos ver por ejemplo todas las tecnolog\u00edas (de conexi\u00f3n a diferentes sistemas de datos) </p> <p>En la parte de <code>Projects</code> tenemos  - Paquetes - Mappings: tienen </p>"},{"location":"ApacheNiFi/ODI/#ejercicio-de-demo-de-odi-12c-getting-started-guidepdf","title":"Ejercicio de demo de <code>ODI 12c Getting Started Guide.pdf</code>","text":"<p>Aqui tenemos una demo guiada que nos explica c\u00f3mo hacer un par de mappings y ponerlos en macha, en concreto los puntos a realizar en clase son los siguientes</p>"},{"location":"ApacheNiFi/ODI/#ejercicio-41","title":"Ejercicio 4.1","text":"<p>En la parte f\u00edsica. del elemento final le pone un <code>TRUCATE</code> a <code>true</code>, que al parecer lo que hace es que cada vez que vamos a ejecutar elimina todo lo que hay en el destino para rehacerlo de nuevo. Tambi\u00e9n el <code>FLOW_CONTROL</code> a <code>false</code>. Esto lo que hace es verificar las claves primarias y si esta a true, fallar\u00e1.</p> <p>Tal y como esta hecho en clase, no llegamos a crear el paquete, (LOAD_SALES_ADMINISTRATION), por lo que al ejecutar puede fallar, As\u00ed porque antes de ejecutar el TRG_CUSTOMER se tiene que ejecutar los TRG_COUNTRY, REGION y CITY, por eso en clase pasamos directamente al 6.2 para hacer un paquete a\u00f1adiendo los mapping hasta TRG_CUSTOMER. Bueno en clase hace el CITY, COUNTRY y ya el CUSTOMER.</p> <p>Una vez hecho esto, seleccionamos el paquete y le damos al play para ejecutar.</p>"},{"location":"ApacheNiFi/ODI/#ejercicio-42","title":"Ejercicio 4.2","text":"<p>Aqu\u00ed se hace el mapping TRG_SALES</p>"},{"location":"ApacheNiFi/ODI/#ejercicio-622","title":"Ejercicio 6.2.2","text":"<p>Aqu\u00ed se hace el package que ejecuta todo el ETL completo</p>"},{"location":"ApacheNiFi/ODI/#entregable-1","title":"Entregable 1","text":"<p>Es la continuaci\u00f3n del supuesto para generar un mapping \u201cAGR_VENTAS_PRODUCTOS\u201d </p> <p>Lo \u00fanico rese\u00f1able esta en el punto 2. Importar la nova taula al model de dades de ODI, a la carpeta \u201cSales Administration\u201d.</p> <p>Sobre modelos -&gt; Sales Administration, click con bot\u00f3n derecho y hacemos click en Reverse Engineer. Aparece una nueva pesta\u00f1a y en la parte superior izquierda le damos el bot\u00f3n de Revese-Engeneer y busca tablas nuevas en el esquema de oracle y con esto lo tenemos.</p> <p>El resto ya es lo mismo de siempre.... </p> <p>En el punto 9 tenemos las modificaciones de la parte f\u00edsica del mapping:\u00e7 - IKM : IKM Control Append (POST FLOW = False, TRUNCATE = TRUE). Esto es <code>FLOW CONTROL</code> y <code>TRUNCATE</code></p>"},{"location":"ApacheNiFi/ODI/#caso-de-uso-covid-19","title":"Caso de uso: COVID-19","text":"<p>Se trata de un caso t\u00edpico que se puede pedir en una empresa.</p> <p>El dise\u00f1o este a\u00f1o se da hecho, y el a\u00f1o pasado lo hicier\u00f3n los alumnos, pero no sali\u00f3 bien as\u00ed que se explican los pasos o fases del proyecto, pero este a\u00f1o se ha decidido reducirlo a la parte que se hace en ODI</p>"},{"location":"ApacheNiFi/ODI/#en-la-guia","title":"En la guia:","text":"<p>Analisis de modelo:</p> <p>Se hace sobre un fichero de provincias, una sql que se proporciona para obtenci\u00f3n de la DATA y el fichero con los datos brutos de incidencia del cov\u00edd por d\u00eda y localidad</p> <p>ETL </p> <p>Crearemos unas base de datos y unas tablas source que empezar\u00e1n por <code>SRC_</code></p> <p>Despu\u00e9s tenemos lo cat\u00e1logos: Provinicas, fechas, grupo de edad, fechas.. estas ser\u00e1n tablas lookup LKP</p> <p>La estrega o hecho ser\u00e1n ....</p> <p>Al final hacemos un paquete que lo hace todo y que lanzaremos.</p>"},{"location":"ApacheNiFi/ODI/#sobre-el-enunciado","title":"Sobre el enunciado","text":"<p>En el primer p\u00e1rrafo tenemos el acceso a los datos que vamos a utilizar como fuente.</p> <p>En este enlace vemos 3 pesta\u00f1as y b\u00e1sicamente estan hechas con PowerBI, es el objetivo de lo que vamos a hacer nosotros.</p> <p>Requisitos. Al final esta el PL/SQL para hacer todo los requirimientos de este punto</p> <pre><code>7. Sexe (a partir de les dades rebudes al fitxer font)\n8. Grup edat (a partir de les dades rebudes al fitxer font)\n9. Prov\u00edncia (del fitxer al qual es fa refer\u00e8ncia dalt)\n10. Temporal (Enlla\u00e7 al SQL)\n</code></pre> <p>No vamos a hacer ni las Pruebas ni el dise\u00f1o, nos centramos en el desarrollo de ODI.</p> <p>Al final de todo hay una ampliaci\u00f3n por si alguien llega.</p> <p>En los anexos est\u00e1n:  - Diagrama multidimensional de la extraci\u00f3n: El a\u00f1o pasad lo hicier\u00f3n los alumnos, este se facilita.</p> <p>Est\u00e1n los datos que se extraen del COVID. Se tiene que tener encuenta el documento del primer enlace : <code>metadata_casos_hosp_uci_def_sexo_edad_provres.pdf</code>, porque ah\u00ed esta la interpretaci\u00f3n.</p> <p>Fechas Provincias Estat_Alarma: este fichero viene en l propio pdf de enunciado y se har\u00e1 a mano.</p> <p>Cuando acabemos con el diagrama, pasaremos a la estrella multidimensional</p>"},{"location":"ApacheNiFi/ODI/#comenzamos","title":"Comenzamos:","text":"<ul> <li>Nos leemos el enunciado,</li> <li>Descargamos todos los ficheros y los identificamos</li> <li>SQL Developer : Nos creamos las tablas seg\u00fan los diagramas. Podemos ir directamente a las tablas de la \u00faltimo diagrama de estrella. e ir creando todas las tablas. Nos guardamos los scripts.</li> </ul> <p>A partir de los ficheros facilitados y los datos que queremos obtener, realizamos un an\u00e1lisis funcional y un dise\u00f1o de la soluci\u00f3n. En este caso, estos dos elementos se dan hecho y se facilitan los <code>diagramas multidimensionales</code> de la extracci\u00f3n y de la estrella para agilizar el proceso, ya que es tedioso.</p> <p>A partir de aqu\u00ed, seguimos los pasos indicados en la gu\u00eda para la generaci\u00f3n de las 3 capas del ETL: 1. Generamos los fichero fuente o SOURCE (SRC) que contienen los datos de los ficheros 2. Generamos los cat\u00e1logos (LKP) (lookups) necesarios  3. Generamos la capa de estrella o de hechos (ODS)</p> <p>En primer lugar creamos las tablas de cada una de estas capas en Oracle mediante el SQL Developer y despu\u00e9s dentro de ODI generamos los paquetes para la carga de todas estas capas.</p> <p>As\u00ed pues en ODI tenemos los siguientes pasos: 1. Generar los paquetes SRC, LKP y ODS 2. Validar que no hay errores 3. Validar los datos de las tablas.</p>"},{"location":"ApacheNiFi/ODI/#resumen-jorge","title":"Resumen Jorge:","text":"<ol> <li>Hacer CREATES</li> <li>Importar tablas ODI</li> <li>Carpeta ODI para ficheros</li> <li>Ejecutar el SQL del dimensi\u00f3n FECHA pero en ODI para eu cada uno </li> <li>Ficheros de COVID, provincias, tramos fechas se deben importar a ODI</li> <li></li> </ol>"},{"location":"ApacheNiFi/ODI/#creacion-de-tablas-en-oracle","title":"Creaci\u00f3n de tablas en Oracle","text":"<p>Abrimos el entorno de SQL Developer y creamos todas las tablas que tenemos m\u00e1s abajo</p>"},{"location":"ApacheNiFi/ODI/#creacion-de-paquetes-en-odi","title":"Creaci\u00f3n de paquetes en ODI","text":"<p>Vamos a ODI y en: <code>Topology</code>-&gt;<code>Technologies</code>-&gt;<code>File</code>-&gt;<code>FILE_GENERIC</code> creamos un nuevo \"repositorio de ficheros donde vamos a trabajar\". Nos cogemos el <code>Directory Schema</code> del que ya hay, lo modificamos y copiamos en el nuevo. por ejemplo <code>/u01/Middleware/ODI12c/demo/covid</code>.</p> <p>Despu\u00e9s se debe crear una estructura l\u00f3gica en <code>Logical Architecture</code> y tambi\u00e9n creamos un <code>FILE_COVID_SRC</code></p>"},{"location":"ApacheNiFi/ODI/#sql-creacion-de-tablas","title":"SQL creaci\u00f3n de tablas","text":"<p>Tablas necesarias para almacenar los datos Source (SRC), cat\u00e1logos (LKP) y hechos (ODS)</p> <pre><code>-- SOURCE: COVID  *****************************\n\nDROP TABLE SRC_COVID;\n\nCREATE TABLE SRC_COVID (\n  PROVINCIA_ISO VARCHAR2(150),\n  FECHA VARCHAR2(150),\n  SEXO VARCHAR2(150),\n  GRUP_EDAT VARCHAR2(150),\n  NUM_CASOS  VARCHAR2(150),\n  NUM_HOSP  VARCHAR2(150),\n  NUM_UCI VARCHAR2(150),\n  NUM_DEF VARCHAR2(150),\n  FECHA_ACT TIMESTAMP\n);\n\n\n-- Cat\u00e1logo - SEXO\nDROP TABLE LKP_SEXO;\n\nCREATE TABLE LKP_SEXO (\n  C_SEXO VARCHAR2(150),\n  SEXO VARCHAR2(150),\n  FECHA_ACT TIMESTAMP\n);\n\n-- Cat\u00e1logo - GRUPO EDAD\nDROP TABLE LKP_GRUPO_EDAD;\n\nCREATE TABLE LKP_GRUPO_EDAD (\n  C_GRUPO_EDAD VARCHAR2(150),\n  GRUPO_EDAD VARCHAR2(150),\n  FECHA_ACT TIMESTAMP\n);\n\n\n\n\n-- SOURCE : PROVINCIAS ********************\nDROP TABLE SRC_PROVINCIA;\n\nCREATE TABLE SRC_PROVINCIA (\n  CAT_SUB VARCHAR2(150),\n  CODIGO VARCHAR2(150),\n  NOMBRE_SUBDIVISION VARCHAR2(150),\n  VARIANTE_LOCAL VARCHAR2(150),\n  COD_IDIOMA  VARCHAR2(150),\n  SISTEMA_ROMANIZACION  VARCHAR2(150),\n  SUB_SUPERIOR VARCHAR2(150),\n  FECHA_ACT TIMESTAMP\n);\n\n-- Cat\u00e1logo: PROVINCIAS\nDROP TABLE LKP_PROVINCIA;\n\nCREATE TABLE LKP_PROVINCIA (\n  C_PROVINCIA VARCHAR2(150),\n  PROVINCIA VARCHAR2(150),\n  FECHA_ACT TIMESTAMP\n);\n\n\n\n\n-- SOURCE: ESTAT_ALARMA *************************** \nDROP TABLE SRC_ESTAT_ALARMA;\n\nCREATE TABLE SRC_ESTAT_ALARMA (\n  CODI_ESTAT_ALARMA VARCHAR2(150),\n  ESTAT_ALARMA VARCHAR2(150),\n  FECHA_ACT TIMESTAMP\n);\n\n-- Cat\u00e1logo: Estados de alarma\nDROP TABLE LKP_ESTAT_ALARMA;\n\nCREATE TABLE LKP_ESTAT_ALARMA (\n  C_ESTAT_ALARMA VARCHAR2(150),\n  ESTAT_ALARMA VARCHAR2(150),\n  FECHA_ACT TIMESTAMP\n);\n\n\n-- *************************************\n-- Cat\u00e1logo :  FECHA\nDROP TABLE LKP_FECHA;\n\nCREATE TABLE LKP_FECHA (\n  C_DATA VARCHAR2(150),\n  D_DATA VARCHAR2(150),\n  C_DIA VARCHAR2(150),\n  D_CURTA_DIA VARCHAR2(150),\n  D_LLARGA_DIA VARCHAR2(150),\n  C_DIA_SETMANA VARCHAR2(150),\n  C_MES VARCHAR2(150),\n  C_ANY_MES VARCHAR2(150),\n  D_CURTA_MES VARCHAR2(150),\n  D_LLARGA_MES VARCHAR2(150),\n  C_SETMANA VARCHAR2(150),\n  C_TRIMESTRE VARCHAR2(150),\n  C_SEMESTRE VARCHAR2(150),\n  C_ANY VARCHAR2(150),\n  C_DIA_NATURAL VARCHAR2(150),\n  C_DIA_FESTIU VARCHAR2(150),\n  C_DIA_FEINA VARCHAR2(150),\n  ID_CARREGA  VARCHAR2(150),\n  FECHA_ACT TIMESTAMP\n);\n\n\n\n\n-- ESTRELLA - Hechos: COVID  *********************\nDROP TABLE ODS_COVID;\n\nCREATE TABLE ODS_COVID (\n  C_DATA VARCHAR2(150),\n  C_PROVINCIA VARCHAR2(150),\n  C_SEXO VARCHAR2(150),\n  C_GRUP_EDAT VARCHAR2(150),\n  C_ESTAT_ALARMA VARCHAR2(150),\n  F_NUM_CASOS  VARCHAR2(150),\n  F_NUM_HOSP  VARCHAR2(150),\n  F_NUM_UCI VARCHAR2(150),\n  F_NUM_DEF VARCHAR2(150),\n  FECHA_ACT TIMESTAMP\n);\n</code></pre> <p>Pare rellenar la tabla del cat\u00e1logo de fechas <pre><code>SELECT * FROM(\nwith dimension_fechas as (\n  SELECT\n    to_date(19000101,'YYYYMMDD') + (LEVEL - 1) AS DT\n  FROM DUAL\n  CONNECT BY LEVEL &lt;=\n  /* End Date in here -&gt; */ to_number(add_months(sysdate,36) - 1 -\n  /* Start Date in here -&gt; */ to_date(19000101,'YYYYMMDD')) +1\n)\nSELECT \n  to_number(to_char(DT,'YYYYMMDD')) as C_DATA,\n  DT as D_DATA,\n  TO_CHAR(DT,'DD') as C_DIA,\n  TO_CHAR(DT,'DY','nls_date_language=CATALAN') as D_CURTA_DIA,\n  TO_CHAR(DT,'DAY','nls_date_language=CATALAN') as D_LLARGA_DIA,\n  1 + trunc(DT) - trunc (DT,'IW') as C_DIA_SETMANA,\n  TO_CHAR(DT,'MM') as C_MES, \n  TO_CHAR(DT,'YYYYMM') as C_ANY_MES, \n  to_char(DT,'MON','nls_date_language=CATALAN') as D_CURTA_MES,\n  to_char(DT,'month','nls_date_language=CATALAN') as D_LlARGA_MES,\n  to_char(DT,'WW','nls_date_language=CATALAN') as C_SETMANA,\n  TO_CHAR(DT,'YYYY') || 'T' ||\n  case extract(month from DT)\n    when 1 then 1\n    when 2 then 1\n    when 3 then 1\n    when 4 then 2\n    when 5 then 2\n    when 6 then 2\n    when 7 then 3\n    when 8 then 3\n    when 9 then 3\n    else  4\n  end  as C_TRIMESTRE,\n  TO_CHAR(DT,'YYYY') || 'S' ||\n  case \n    when extract(month from DT) &lt; 7 then 1\n    else  2\n  end  as C_SEMESTRE,\n  TO_CHAR(DT,'YYYY') as C_ANY,\n  1 as C_DIA_NATURAL,\n  DECODE (1 + trunc(DT) - trunc (DT,'IW'), 6, 1, 7, 1, 0) AS C_DIA_FESTIU,\n  DECODE (1 + trunc(DT) - trunc (DT,'IW'), 6, 0, 7,0,1) AS C_DIA_FEINA,\n  1 ID_CARREGA, \n  sysdate as DATA_ACT\nFROM dimension_fechas\nORDER BY 2 DESC)\n</code></pre></p>"},{"location":"ElasticStack/1_elastic_stack/","title":"1. Introducci\u00f3n","text":"<p>Los sistemas NoSQL son una categor\u00eda de bases de datos dise\u00f1adas para manejar grandes vol\u00famenes de datos no estructurados o semiestructurados en entornos distribuidos y escalables. Estos sistemas proporcionan flexibilidad en el modelado de datos y est\u00e1n optimizados para casos de uso que requieren alta disponibilidad, escalabilidad horizontal y baja latencia de acceso a los datos.</p>"},{"location":"ElasticStack/1_elastic_stack/#elastic-stack","title":"Elastic Stack","text":"<p>Elastic Stack puede ser un ejemplo muy representativo de bases de datos NoSQL con caracter\u00edsticas muy beneficiosas para gestionar y analizar datos en entornos donde se necesite trabajar con grandes vol\u00famenes de datos no tradicionales. </p>"},{"location":"ElasticStack/1_elastic_stack/#que-es-elastic-stack","title":"\u00bfQu\u00e9 es Elastic Stack?","text":"<p>Elastic Stack, anteriormente conocido como ELK Stack, es un conjunto de herramientas de c\u00f3digo abierto desarrolladas por Elastic para la b\u00fasqueda, an\u00e1lisis y visualizaci\u00f3n de datos. El nombre Elastic Stack refleja la ampliaci\u00f3n del conjunto de herramientas m\u00e1s all\u00e1 de las tres herramientas originales (Elasticsearch, Logstash y Kibana) para incluir una variedad de productos adicionales que complementan y ampl\u00edan la funcionalidad del conjunto.</p> <p>El Elastic Stack consta de los siguientes componentes principales:</p> <ol> <li> <p>Elasticsearch: Es un motor de b\u00fasqueda y an\u00e1lisis distribuido basado en Lucene. Elasticsearch se utiliza para indexar, buscar y analizar grandes vol\u00famenes de datos en tiempo real. Proporciona capacidades de b\u00fasqueda a escala, incluyendo b\u00fasqueda de texto completo, agregaciones, geolocalizaci\u00f3n y m\u00e1s.</p> </li> <li> <p>Logstash: Es una herramienta de ingesti\u00f3n de datos que se utiliza para recopilar, procesar y enviar datos de log y otros tipos de datos desde m\u00faltiples fuentes a Elasticsearch para su almacenamiento y an\u00e1lisis. Logstash ofrece una amplia gama de filtros y complementos para procesar datos de diferentes formatos y fuentes.</p> </li> <li> <p>Kibana: Es una plataforma de visualizaci\u00f3n de datos y an\u00e1lisis que se utiliza para visualizar y explorar los datos almacenados en Elasticsearch. Kibana ofrece una variedad de herramientas de visualizaci\u00f3n y paneles de control que permiten a los usuarios crear gr\u00e1ficos, tablas y mapas interactivos para analizar y comprender los datos.</p> </li> <li> <p>Beats: Beats es una familia de agentes ligeros que se utilizan para enviar datos a Elasticsearch o Logstash desde una variedad de fuentes, incluidos logs, m\u00e9tricas del sistema, datos de red y m\u00e1s. Beats simplifica la recopilaci\u00f3n y env\u00edo de datos, proporcionando una forma eficiente y escalable de enviar datos a la Elastic Stack.</p> </li> <li> <p>Apm-server: Apm-server es un servidor de recolecci\u00f3n de datos de rendimiento de aplicaciones (APM) que se utiliza para recopilar y enviar datos de rendimiento de aplicaciones a Elasticsearch. Ayuda a monitorear y analizar el rendimiento de las aplicaciones y a identificar problemas de rendimiento en tiempo real.</p> </li> <li> <p>Elasticsearch SQL: Es una capa de SQL que se ejecuta sobre Elasticsearch, lo que permite a los usuarios ejecutar consultas SQL sobre datos indexados en Elasticsearch.</p> </li> </ol> <p>Estos son algunos de los componentes principales del Elastic Stack, pero Elastic tambi\u00e9n ofrece una variedad de otros productos y soluciones que complementan el conjunto, como Elasticsearch Security, Elasticsearch Machine Learning, Elastic Enterprise Search, Elastic Observability, entre otros.</p> <p>Elastic Stack es especialmente popular en entornos de desarrollo de software, operaciones de sistemas (DevOps), an\u00e1lisis de seguridad, monitoreo de infraestructura y an\u00e1lisis de registros de aplicaciones. La combinaci\u00f3n de Elasticsearch, Logstash y Kibana proporciona una soluci\u00f3n completa y escalable para la recopilaci\u00f3n, almacenamiento, an\u00e1lisis y visualizaci\u00f3n de datos.</p> <p>M\u00e1s concretamente Elasticsearch es un motor de b\u00fasqueda dise\u00f1ado para escalar horizontalmente, de forma que se puedan ir a\u00f1adiendo m\u00e1s nodos al cl\u00faster a medida que aumente el volumen de datos requerido.</p> <p>Su base es una librer\u00eda de indexaci\u00f3n y b\u00fasqueda de informaci\u00f3n textual conocida como Apache Lucene, desarrollada inicialmente en Java aunque disponible tambi\u00e9n para otros lenguajes.</p> <p>El surgimiento de Elasticsearch se debi\u00f3 a problemas de escalabilidad detectados en otro motor de b\u00fasqueda que tambi\u00e9n se basa en Apache Lucene, Apache Solr. En el siguiente v\u00eddeo se resumen las diferencias principales entre ambos buscadores, que pueden consultarse en detalle en el siguiente enlace</p>"},{"location":"ElasticStack/1_elastic_stack/#por-que-elastic-stack-en-sistemas-big-data","title":"\u00bf Por qu\u00e9 Elastic Stack en Sistemas Big Data?","text":"<p>Existen varias razones que nos aconsejan dar un vistazo a Elastic Stack (Elasticsearch, Logstash, Kibana) para comprender los Sistemas de Big Data: </p> <ol> <li> <p>Escalabilidad y rendimiento: Elasticsearch, como parte de ELK, est\u00e1 dise\u00f1ado para ser altamente escalable y eficiente en t\u00e9rminos de rendimiento. Estudiar c\u00f3mo Elasticsearch maneja grandes vol\u00famenes de datos y consultas en tiempo real proporciona una comprensi\u00f3n pr\u00e1ctica de c\u00f3mo los sistemas de Big Data pueden escalar y procesar datos de manera eficiente.</p> </li> <li> <p>Ingesti\u00f3n de datos: Logstash, otra parte de ELK, se utiliza para la ingesti\u00f3n de datos, que es un aspecto cr\u00edtico en los sistemas de Big Data. Aprender sobre Logstash y sus capacidades para recopilar datos de diferentes fuentes, procesarlos y enviarlos a Elasticsearch* para su almacenamiento y an\u00e1lisis proporciona una comprensi\u00f3n s\u00f3lida de c\u00f3mo se manejan los datos en sistemas de Big Data.</p> </li> <li> <p>An\u00e1lisis y visualizaci\u00f3n de datos: Kibana, la tercera parte de ELK, es una herramienta poderosa para la visualizaci\u00f3n y el an\u00e1lisis de datos. Estudiar Kibana te permite aprender c\u00f3mo crear visualizaciones interactivas, tablas de datos y paneles de control para analizar y comprender datos almacenados en Elasticsearch. Esto es fundamental en los sistemas de Big Data para extraer informaci\u00f3n significativa de conjuntos de datos masivos.</p> </li> <li> <p>Casos de uso pr\u00e1cticos: ELK se utiliza en una amplia variedad de casos de uso, incluyendo monitoreo de infraestructura, an\u00e1lisis de logs, observabilidad de aplicaciones, an\u00e1lisis de seguridad, an\u00e1lisis de registros de aplicaciones y m\u00e1s. Estudiar c\u00f3mo se implementa ELK en estos casos de uso proporciona una comprensi\u00f3n pr\u00e1ctica de c\u00f3mo se utilizan los sistemas de Big Data en entornos del mundo real.</p> <p>Info</p> <p>Elasticsearch ha sido adoptado por algunas marcas importantes, como: Tesco, Linkedin, Foursquare, Facebook, Netflix, Dell, Ebay, Wikipedia, The Guardian, New York Times, Salesforce, Docker, Orange, Groupon, Eventbrite y muchos otros.* Usar Elasticsearch: ventajas, casos pr\u00e1cticos y libros</p> </li> <li> <p>Herramientas de c\u00f3digo abierto: ELK es una suite de herramientas de c\u00f3digo abierto ampliamente utilizada en la industria. Estudiar ELK te expone a tecnolog\u00edas de c\u00f3digo abierto populares y te proporciona habilidades que son altamente valoradas en el mercado laboral de la tecnolog\u00eda.</p> </li> </ol> <p>En resumen, estudiar ELK te proporciona una comprensi\u00f3n pr\u00e1ctica de c\u00f3mo funcionan y se utilizan los sistemas de Big Data en entornos del mundo real, desde la ingesti\u00f3n de datos hasta el almacenamiento, an\u00e1lisis y visualizaci\u00f3n de datos. Esto puede ser valioso para aquellos que desean desarrollar habilidades en el campo del an\u00e1lisis de datos y Big Data.</p>"},{"location":"ElasticStack/1_elastic_stack/#elasticsearch","title":"Elasticsearch","text":"<p>Elasticsearch es un potente motor de b\u00fasqueda y an\u00e1lisis distribuido que se utiliza para buscar, analizar y visualizar grandes vol\u00famenes de datos en tiempo real. En su n\u00facleo, Elasticsearch organiza y almacena la informaci\u00f3n utilizando un enfoque de b\u00fasqueda y recuperaci\u00f3n de informaci\u00f3n, similar a un motor de b\u00fasqueda web, pero optimizado para una amplia gama de casos de uso y tipos de datos.</p>"},{"location":"ElasticStack/1_elastic_stack/#caracteristicas","title":"Caracter\u00edsticas","text":"<p>Es una colecci\u00f3n de informaci\u00f3n organizada para un posterior aprovechamiento de la misma.</p> <p>Esta informaci\u00f3n debe:</p> <ul> <li>Ser accesible</li> <li>Se pueda gestionar</li> <li>Se pueda actualizar</li> </ul> <p>Las caracter\u00edsticas principales son: </p> <ul> <li>Distribuido y escalable. Permite crecer conforme lo hagan las necesidades de forma horizontal.</li> <li>Datos en tiempo real. Los datos est\u00e1n disponibles para su an\u00e1lisis segundos despu\u00e9s de haber sido indexados (real time).</li> <li>Alta disponibilidad. Datos replicados a lo largo de distintos nodos permite el fallo de alguno de estos dentro del cl\u00faster sin que se vea afectado el funcionamiento.</li> <li>Multi-tenancy. Los \u00edndices donde se almacenan la informaci\u00f3n pueden ser consultados de manera independiente.</li> <li>B\u00fasquedas full-text. Usa Apache Lucene aprovechando sus capacidades de b\u00fasqueda de texto, soportando geolocalizaci\u00f3n, autocompletado, expresiones regulares\u2026</li> <li>API Restful. Proporciona una API sobre JSON para realizar consultas e interactuar con Elasticsearch.</li> </ul>"},{"location":"ElasticStack/1_elastic_stack/#almacenamiento-de-la-informacion","title":"Almacenamiento de la informaci\u00f3n","text":"<p>La informaci\u00f3n se organiza dentro de elasticsearch en: </p> <ul> <li> <p>\u00cdndice: Colecci\u00f3n de documentos con caracter\u00edsticas similares (cat\u00e1logo de productos, clientes, pedidos\u2026). Identificado por un nombre (min\u00fasculas) usado para realizar operaciones como indexado, b\u00fasqueda, actualizaci\u00f3n y borrado de los documentos que contiene.</p> </li> <li> <p>Documento: Unidad b\u00e1sica de informaci\u00f3n que puede ser indexada (documento por producto, cliente o pedido) en formato JSON.  En un \u00edndice se pueden almacenar todos los documentos que se requiera.</p> </li> <li> <p>Shards: Cuando se almacena gran cantidad de informaci\u00f3n puede exceder los l\u00edmites hardware de un nodo. Elasticsearch proporciona la posibilidad de subdividir un \u00edndice en distintas partes denominadas shards.</p> </li> <li>Permite subdividir/escalar la informaci\u00f3n almacenada.</li> <li> <p>Permite distribuir y paralelizar operaciones.</p> </li> <li> <p>R\u00e9plica: Elasticsearch permite realizar una o m\u00e1s copias de los shards. Su importancia recae en:</p> </li> <li>Proporciona alta disponibilidad en el caso de que un shard/nodo falle. Importante que las r\u00e9plicas no se encuentren en el mismo nodo.</li> <li>Permite el escalado del volumen de b\u00fasquedas ya que estas pueden ejecutarse en paralelo en las r\u00e9plicas.</li> </ul> <p>Una vez replicado, cada \u00edndice tendr\u00e1 un shard primario y las r\u00e9plicas (copias exactas del primario). Los shards y r\u00e9plicas se pueden definir por \u00edndice a la hora de creaci\u00f3n de los mismos. Las r\u00e9plicas podr\u00e1n cambiarse en caliente al contrario que los shards.</p> <p>La informaci\u00f3n se almacena en diferentes sitios de la red (f\u00edsicamente) pero l\u00f3gicamente es una \u00fanica base de datos.</p> <ul> <li>Transparente para un usuario final.</li> <li>Independencia respecto al sistema operativo.</li> <li>Procesamiento distribuido de consultas.</li> <li>Informaci\u00f3n fragmentada.</li> <li>R\u00e9plicas = Alta disponibilidad.</li> </ul>"},{"location":"ElasticStack/1_elastic_stack/#tipos-de-nodos","title":"Tipos de nodos","text":"<p>En Elasticsearch, los nodos son las instancias individuales que forman parte de un cl\u00faster y que almacenan, indexan y procesan los datos. Existen varios tipos de nodos en Elasticsearch, cada uno con un prop\u00f3sito espec\u00edfico. A continuaci\u00f3n, se describen los tipos principales de nodos:</p> <ol> <li>Nodos de datos (Data Nodes):</li> <li>Los nodos de datos son responsables de almacenar y gestionar los datos indexados en Elasticsearch.</li> <li>Estos nodos almacenan los shards (fragmentos) de los \u00edndices en el cl\u00faster.</li> <li>Cada nodo de datos contiene una parte del conjunto completo de datos en el cl\u00faster.</li> <li> <p>Los nodos de datos participan en la indexaci\u00f3n y b\u00fasqueda de datos.</p> </li> <li> <p>Nodos de maestro (Master Nodes):</p> </li> <li>Los nodos maestro son responsables de coordinar las actividades en el cl\u00faster y de gestionar los metadatos.</li> <li>Controlan las operaciones de indexaci\u00f3n, b\u00fasqueda, asignaci\u00f3n de shards y recuperaci\u00f3n en el cl\u00faster.</li> <li>Los nodos maestro no almacenan datos de \u00edndices, pero mantienen un seguimiento del estado del cl\u00faster y de la topolog\u00eda.</li> <li> <p>Los cl\u00fasteres de Elasticsearch deben tener al menos un nodo maestro para funcionar correctamente.</p> </li> <li> <p>Nodos de coordinaci\u00f3n (Coordinating Nodes):</p> </li> <li>Los nodos de coordinaci\u00f3n act\u00faan como puntos de entrada para las solicitudes de clientes y distribuyen las solicitudes a los nodos adecuados.</li> <li>Ayudan a equilibrar la carga de trabajo y a distribuir las consultas de b\u00fasqueda y recuperaci\u00f3n en el cl\u00faster.</li> <li> <p>Estos nodos no almacenan datos de \u00edndices y no participan en la indexaci\u00f3n o b\u00fasqueda directa de datos.</p> </li> <li> <p>Nodos de ingesti\u00f3n (Ingest Nodes):</p> </li> <li>Los nodos de ingesti\u00f3n se utilizan para procesar y transformar datos antes de que se almacenen en Elasticsearch.</li> <li>Pueden aplicar transformaciones, an\u00e1lisis de texto y enriquecimiento de datos antes de indexarlos en el cl\u00faster.</li> <li> <p>Estos nodos son \u00fatiles para la preparaci\u00f3n y limpieza de datos antes de su almacenamiento en el \u00edndice.</p> </li> <li> <p>Nodos de cliente (Client Nodes):</p> </li> <li>Los nodos de cliente son nodos dedicados exclusivamente a recibir solicitudes de clientes y reenviarlas al nodo adecuado para su procesamiento.</li> <li>Ayudan a distribuir la carga de trabajo al actuar como intermediarios entre los clientes y los nodos de datos, coordinaci\u00f3n o ingesti\u00f3n.</li> </ol>"},{"location":"ElasticStack/1_elastic_stack/#arquitectura-logica-de-funcionamiento","title":"Arquitectura: L\u00f3gica de funcionamiento:","text":"<p>La arquitectura de funcionamiento de un sistema elastic stack ser\u00eda el representado por la siguiente imagen.</p> <p>Y en cuanto a las comunicaciones entre los diferentes elementos, debemos tener en cuenta : </p> <p>Elasticsearch incluye 2 protocolos de comunicaci\u00f3n: 1. Protocolo nativo en binario llamado Transport Client. Puerto 9300 por defecto    - S\u00f3lo pensado para el lenguaje Java.     - Se utiliza para comunicar Elastic con el resto de aplicaciones del Stack 2. Protocolo HTTP: API REST. Puerto 9200 por defecto    - Forma m\u00e1s com\u00fan de conexi\u00f3n    - REST es un protocolo de uso general de operaciones y herramientas de test conocidas    - Posibilidad de implementar clientes en cualquier lenguaje de programaci\u00f3n: java, python, javascript ...</p>"},{"location":"ElasticStack/1_elastic_stack/#instalacion-de-elastic","title":"Instalaci\u00f3n de elastic","text":"<p>Tenemos diferentes opciones para la instalaci\u00f3n de elastic stack, de hecho no es necesario instalar todo el sistema completo, si no que podemos instalar elementos separados de ellos, por ejemplo Elasticsearch y Kibana por separado.</p> <p>Adem\u00e1s podemos realizar instalaci\u00f3n de diferentes nodos en diferentes equipos para que funcionen de forma connjunta o podemos instalarlos mediante docker en mismo sistema.</p>"},{"location":"ElasticStack/1_elastic_stack/#preparacion-para-la-instalacion","title":"Preparaci\u00f3n para la instalaci\u00f3n.","text":"<p>Nosotros vamos a utilizar un equipo Ubuntu Desktop, aunque realmente ser\u00eda m\u00e1s eficiente un Ubuntu Server o similar. Si embargo, para que sea m\u00e1s sencillo y poder acceder posteriormente desde mismo equipo, instalaremos un m\u00e1quina con experiencia de usuario.</p> <p>Una configuraci\u00f3n de esta m\u00e1quina en Oracle VM VirtualBox podr\u00eda ser la siguiente:</p> <p>Observar que se ha incluido una cantidad elevada de memoria: 10GB (en un equipo de 16GB), se ha asignado m\u00e1s de un procesador y un m\u00ednimo de 50GB de disco duro.</p> <p>Una vez instalado el sistema, es aconsejable instalar los VirtualBox Guest Additions.</p> <p>Seleccionamos su instalaci\u00f3n y abrimos un terminal en el medio que lo contiene: </p> <p><pre><code>sudo apt update\nsudo apt upgrade                # opcional. Puede tardar mucho\nsudo apt install gcc make perl  # opcional y aconsejable, para poder recompilar el n\u00facleo \nsudo ./VBoxLinuxAdditions.run   # instalaci\u00f3n\n</code></pre> Antes de seguir, tambi\u00e9n es aconsejable hacer una copia de seguridad o una instant\u00e1nea, por si algo sale mal.</p> <p>Tambi\u00e9n es interesante habilitar el usuario root y trabajar con este usuario </p> <p>Una vez preparado nuestro sistema, vamos a ver diferentes instalaciones. Realizaremos dos: en primer lugar instalaremos Elasticsearch sobre un docker sobre la m\u00e1quina, y posteriormente instalaremos un cluster aprovechando la tecnolog\u00eda docker-compose. Por supuesto tambi\u00e9n podemos realizar una instalaci\u00f3n directa sobre nuestra m\u00e1quina, pero en este caso, como la finalidad es montar un cluster para ver su comportamiento, optaremos por obviar esta variante. </p>"},{"location":"ElasticStack/1_elastic_stack/#instalacion-de-docker-docker-compose-y-curl","title":"Instalaci\u00f3n de <code>docker</code>, <code>docker-compose</code> y <code>curl</code>","text":"<p>Docker es una plataforma de c\u00f3digo abierto dise\u00f1ada para facilitar la creaci\u00f3n, implementaci\u00f3n y ejecuci\u00f3n de aplicaciones en contenedores. Un contenedor es una unidad ligera de software que contiene todo lo necesario para ejecutar una aplicaci\u00f3n, incluidas las bibliotecas, las herramientas del sistema, el c\u00f3digo y las dependencias. Docker proporciona una forma estandarizada de empaquetar y distribuir aplicaciones, lo que facilita la creaci\u00f3n de entornos de desarrollo consistentes y port\u00e1tiles.</p> <p>Docker Compose, por otro lado, es una herramienta que permite definir y gestionar aplicaciones multi-contenedor. Con Docker Compose, se puede definir la configuraci\u00f3n de una aplicaci\u00f3n en un archivo YAML, especificando los servicios, im\u00e1genes de contenedor, vol\u00famenes, redes y otros detalles necesarios para ejecutar tu aplicaci\u00f3n. Docker Compose simplifica la gesti\u00f3n de aplicaciones complejas al permitirte definir y controlar todos los componentes de tu aplicaci\u00f3n desde un solo lugar.</p> <p>Para instalar Docker y Docker Compose en Ubuntu 22.04, puedes seguir estos pasos:</p> <p>1.- Actualiza el \u00edndice de paquetes de Ubuntu:</p> <pre><code>sudo apt update\n</code></pre> <p>2.- Instala los paquetes necesarios para permitir que apt utilice un repositorio sobre HTTPS:</p> <pre><code>sudo apt install apt-transport-https ca-certificates curl software-properties-common\n</code></pre> <p>3.- Descarga e importa la clave GPG oficial de Docker:</p> <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n</code></pre> <p>4.- Agrega el repositorio de Docker al sistema:</p> <pre><code>echo \\\n  \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> <p>5.- Actualiza el \u00edndice de paquetes nuevamente:</p> <pre><code>sudo apt update\n</code></pre> <p>6.- Instala la versi\u00f3n de Docker que prefieras. Puedes elegir entre la versi\u00f3n comunitaria (Docker CE) o la versi\u00f3n empresarial (Docker EE). Para instalar la versi\u00f3n comunitaria, ejecuta:</p> <pre><code>sudo apt install docker-ce docker-ce-cli containerd.io\n</code></pre> <p>7.- Instala docker-compose, ejecuta:</p> <pre><code>sudo apt install docker-compose\n</code></pre> <p>8.- Verifica que Docker se haya instalado correctamente ejecutando el siguiente comando para verificar la versi\u00f3n:</p> <pre><code>docker --version\ndocker-compose --version\n</code></pre> <p>9.- Para permitir que tu usuario actual ejecute comandos de Docker sin necesidad de usar <code>sudo</code>, agrega tu usuario al grupo <code>docker</code>:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>10.- Cierra la sesi\u00f3n actual y vuelve a iniciarla para que los cambios surtan efecto.</p> <p>Ya que nos estamos preparando, podemos instalar tambi\u00e9n <code>curl</code>, que nos permitir\u00e1 contactar desde la l\u00ednea de comandos con el servidor Elasticsearh.</p> <p>Para ello :</p> <pre><code>sudo apt install curl\n</code></pre> <p>M\u00e1s informaci\u00f3n sobre <code>curl</code> en la Wikipedia o en la red</p>"},{"location":"ElasticStack/1_elastic_stack/#un-poco-de-docker","title":"Un poco de <code>docker</code>","text":"<p>Aqu\u00ed tienes una tabla que muestra algunas opciones t\u00edpicas del comando <code>docker</code>:</p> Comando y Opci\u00f3n Descripci\u00f3n <code>docker run &lt;imagen&gt;</code> Crea y ejecuta un nuevo contenedor basado en la imagen especificada. <code>docker ps</code> Muestra una lista de los contenedores en ejecuci\u00f3n. <code>docker images</code> Lista las im\u00e1genes Docker descargadas en el sistema. <code>docker pull &lt;imagen&gt;</code> Descarga una imagen de Docker del registro p\u00fablico o privado. <code>docker build &lt;ruta&gt;</code> Construye una imagen Docker desde un Dockerfile ubicado en la ruta especificada. <code>docker push &lt;imagen&gt;</code> Sube una imagen de Docker al registro remoto. <code>docker stop &lt;contenedor&gt;</code> Detiene un contenedor en ejecuci\u00f3n. <code>docker start &lt;contenedor&gt;</code> Inicia un contenedor detenido. <code>docker rm &lt;contenedor&gt;</code> Elimina uno o varios contenedores. <code>docker rmi &lt;imagen&gt;</code> Elimina una o varias im\u00e1genes de Docker del sistema. <code>docker exec -it &lt;contenedor&gt; &lt;comando&gt;</code> Ejecuta un comando dentro de un contenedor en ejecuci\u00f3n. Ej. <code>docker exec --user='root' -it es01 /bin/bash</code> <code>docker logs &lt;contenedor&gt;</code> Muestra los registros de salida de un contenedor en ejecuci\u00f3n. <code>docker inspect &lt;objeto&gt;</code> Muestra informaci\u00f3n detallada sobre un contenedor, imagen o red de Docker. <p>Ejemplos de uso de comando <code>docker</code></p> <ul> <li> <p>listar contenedores activos <pre><code>docker ps\n</code></pre></p> </li> <li> <p>operaciones con contenedores <pre><code>docker start es01       # inicia el contenedor es01\ndocker stop es01        # lo para\ndocker restart es01     # lo reinicia   \ndocker rm es01          # lo elimina\ndocker kill es01        # lo .... \n</code></pre></p> </li> <li> <p>entrar en el terminal de un contenedor  <pre><code>docker exec -it es01 /bin/bash\n</code></pre></p> </li> <li> <p>copiar desde y hacia un contenedor. <pre><code>docker cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .\ndocker cp elasticsearch.yml es01:/usr/share/elasticsearch/config/elasticsearch.yml \n</code></pre></p> </li> <li> <p>eliminar y crear interfaces de red <pre><code>docker network create elastic\ndocker network rm elastic\n</code></pre></p> </li> </ul>"},{"location":"ElasticStack/1_elastic_stack/#y-un-poco-de-docker-compose","title":"y un poco de <code>docker-compose</code>","text":"<p>Aqu\u00ed tienes algunos usos t\u00edpicos de comandos docker-compose:</p> Comando Descripci\u00f3n <code>docker-compose up</code> Construye, crea y arranca todos los servicios definidos en el archivo <code>docker-compose.yml</code>. Si no existe, Docker Compose construir\u00e1 las im\u00e1genes necesarias. <code>docker-compose up -d</code> Similar a <code>docker-compose up</code>, pero ejecuta los servicios en segundo plano (modo detached). \u00datil para ejecutar servicios en el fondo sin bloquear la terminal. <code>docker-compose down</code> Detiene y elimina todos los contenedores, redes y vol\u00famenes creados por <code>docker-compose up</code>. <code>docker-compose ps</code> Muestra el estado de los servicios definidos en el archivo <code>docker-compose.yml</code>. Proporciona informaci\u00f3n sobre los contenedores en ejecuci\u00f3n, sus puertos mapeados y el estado. <code>docker-compose logs</code> Muestra los logs de todos los servicios o de un servicio espec\u00edfico definido en <code>docker-compose.yml</code>. Puedes usar opciones adicionales para controlar el formato y la salida de los logs. <code>docker-compose exec &lt;servicio&gt; &lt;comando&gt;</code> Ejecuta un comando dentro de un contenedor de un servicio espec\u00edfico. \u00datil para ejecutar comandos en el contexto de un servicio en ejecuci\u00f3n. <code>docker-compose build</code> Construye o reconstruye los servicios definidos en <code>docker-compose.yml</code>. \u00datil cuando se realizan cambios en Dockerfiles o archivos de configuraci\u00f3n y se necesita actualizar las im\u00e1genes. <code>docker-compose restart &lt;servicio&gt;</code> Reinicia un servicio espec\u00edfico definido en <code>docker-compose.yml</code>. Esto detiene y vuelve a arrancar el contenedor del servicio. <code>docker-compose stop</code> Detiene los contenedores de los servicios definidos en <code>docker-compose.yml</code> sin eliminarlos. \u00datil para detener servicios sin eliminar los recursos asociados. <code>docker-compose start</code> Arranca todos los servicios previamente creados. \u00datil para volver a arrancar los contenedores del servicio."},{"location":"ElasticStack/1_elastic_stack/#instalacion-de-elasticsearch-en-ubuntu-mediante-docker","title":"Instalaci\u00f3n de <code>Elasticsearch</code> en Ubuntu mediante <code>Docker</code>","text":"<p>Antes de nada, para evitar problemas con los contenedores, es recomendable cambiar los siguientes par\u00e1metros del sistema:</p> <p><pre><code># Deshabilitar swapping\nsudo swapoff -a\n# Mem\u00f3ria virtual\nsudo sysctl -w vm.max_map_count=262144\n# Number or threads\nsudo sysctl -w fs.file-max=65536\n</code></pre> podemos incluir todo esto en un fichero de configuraci\u00f3n y ejecutar directamente. </p>"},{"location":"ElasticStack/1_elastic_stack/#instalando-elasticsearch-en-docker","title":"Instalando <code>ElasticSearch</code> en <code>docker</code>","text":"<p>Para instalar elastic en docker seguimos: </p> <pre><code># instalamos elacticseach sobre docker\ndocker network create elastic\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:8.12.0\ndocker run --name elasticsearch --net elastic -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -it docker.elastic.co/elasticsearch/elasticsearch:8.12.0\n\n# Para la ejecuci\u00f3n es recomendable asignar m\u00e1s de 1GB, para evitar problemas\ndocker run --name elasticsearch --net elastic -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -it -m 2GB docker.elastic.co/elasticsearch/elasticsearch:8.12.0\n</code></pre> <p>Al terminal la instalaci\u00f3n y puesta en marcha del contenedor nos aparece la siguiente informaci\u00f3n donde tenemos contrase\u00f1a de acceso y tokens (claves) para poder unir <code>Kibana</code> u otros nodos de Elasticsearch. Esta informaci\u00f3n es importante retenerla.</p>"},{"location":"ElasticStack/1_elastic_stack/#comprobando-la-instalacion","title":"Comprobando la instalaci\u00f3n","text":"<p>Una vez llegados a este punto, dejamos el contenedor funcionando y nos abrimos un nuevo terminal. </p> <p>Nos preparamos para trabajar de creando una variable para guardar la contrase\u00f1a anterior y copiando el certificado de autentificaci\u00f3n en nuestro equipo.</p> <pre><code>export ELASTIC_PASSWORD=\"ar+cS5jc7JzL_AzppMrt\"\n\n# Copy the http_ca.crt SSL certificate from the container to your local machine.\ndocker cp elasticsearch:/usr/share/elasticsearch/config/certs/http_ca.crt .\n\n# si no estamos en el root, debemos hacer que nuestro usuario tenga acceso al certificado\nsudo chmod o+r http_ca.crt\n</code></pre> <p>y realizamos la comprobaci\u00f3n de que Elasticsearch esta funcionando.</p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200\n</code></pre> <p>Como podemos ver, obtenemos la respuesta mediante un JSON que nos indica que el servidor esta en marcha y nos muestra datos como el nombre de nodo y las versiones de elastic as\u00ed como de <code>Lucene</code>.</p> <p>Note</p> <p>Cuando hacemos <code>export ELASTIC_PASSWORD=</code>, datos el valor a esta variable hasta el momento en que se reinicia el equipo. Para mantener esta variable siempre en el sistema, incluso despu\u00e9s de reinicios, se deber\u00eda incluir en el fichero <code>/etc/environment</code> </p> <p>Tambi\u00e9n podemos consulta el estado del servidor o cluster de servidores: </p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200/_cluster/health?pretty \n</code></pre> <p>Como vemos en la siguiente imagen, realmente hemos creado un cluster con un solo nodo, que ser\u00e1 el master.</p> <p>Adem\u00e1s podemos ver el status del cluster: green. los posibles estados de un cluster son: </p> Estado del Cluster Descripci\u00f3n Green Todos los nodos en el cl\u00faster est\u00e1n activos y operativos. Todas las r\u00e9plicas y shards primarios est\u00e1n presentes y asignados. El cl\u00faster est\u00e1 completamente funcional y no hay p\u00e9rdida de datos incluso en el caso de la falla de un nodo. Yellow Todos los nodos en el cl\u00faster est\u00e1n activos y operativos, pero algunas r\u00e9plicas de shards no est\u00e1n asignadas. Esto puede ocurrir cuando se agrega un nuevo nodo al cl\u00faster y las r\u00e9plicas a\u00fan no se han asignado completamente, o si un nodo se ha ca\u00eddo y las r\u00e9plicas no se han restaurado. A pesar de que algunas r\u00e9plicas no est\u00e1n asignadas, la funcionalidad de b\u00fasqueda y recuperaci\u00f3n no se ve afectada. Red Al menos un shard primario o una r\u00e9plica est\u00e1 ausente o no asignada. Esto puede deberse a la p\u00e9rdida de datos debido a la falta de r\u00e9plicas suficientes o a la falta de asignaci\u00f3n de un shard primario. El cl\u00faster puede seguir funcionando, pero la p\u00e9rdida de datos es posible y se requiere intervenci\u00f3n para restaurar la integridad del cl\u00faster. Gray Este estado puede indicar un error en el proceso de monitoreo del cl\u00faster o una incapacidad para determinar el estado real del cl\u00faster. Esto puede ocurrir cuando los nodos no pueden comunicarse entre s\u00ed o si hay problemas con el monitoreo y la recopilaci\u00f3n de datos del estado del cl\u00faster. Se requiere una revisi\u00f3n para diagnosticar y resolver la causa del estado de cl\u00faster en gris. <p>Estos son algunos de los estados comunes del cl\u00faster en Elasticsearch y sus descripciones asociadas. Es importante monitorear el estado del cl\u00faster regularmente para garantizar la disponibilidad y la integridad de los datos en Elasticsearch.</p> <p>Otras consultas que podemos hacer sobre el cluster son : </p> <ul> <li> <p>Informaci\u00f3n detallada del cluster <pre><code>sudo curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XGET 'https://localhost:9200/_cluster/stats?human&amp;pretty'\n</code></pre></p> </li> <li> <p>Listado de nodos del cluster <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200/_cat/nodes?v\n</code></pre></p> </li> <li> <p>Informaci\u00f3n sobre el listado de nodos <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XGET 'https://localhost:9200/_nodes?pretty'\n</code></pre></p> </li> </ul> <p>Tambi\u00e9n podemos introducir nuestro primer \u00edndice con algunos datos (o documentos).</p> <p>Veamos c\u00f3mo se insertar\u00eda un \u00edndice y c\u00f3mo se consulta: </p> <ul> <li>A\u00f1adir al indice test</li> </ul> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XPOST https://localhost:9200/test/_bulk?pretty -d'\n{ \"index\" : {} }\n{ \"num\": \"2015/56\", \"cliente\": 13, \"importe\":189 }\n{ \"index\" : {} }\n{ \"num\": \"2015/57\", \"cliente\": 45, \"importe\":190 }\n' -H 'Content-Type: application/json'\n</code></pre> <ul> <li>obtener todo lo que hay en indice test</li> </ul> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XGET https://localhost:9200/test/_search?pretty\n</code></pre>"},{"location":"ElasticStack/1_elastic_stack/#y-ahora-que-mas-nodos-anadimos-kibana","title":"y ahora qu\u00e9, \u00bfm\u00e1s nodos?, \u00bfa\u00f1adimos <code>kibana</code>?","text":"<p>Podr\u00edamos a\u00f1adir m\u00e1s nodos, como se indica el manual con la siguiente instrucciones, pero nosotros de momento no lo vamos a hacer: </p> <pre><code>docker run -e ENROLLMENT_TOKEN=\"&lt;token&gt;\" --name es02 --net elastic -it -m 1GB docker.elastic.co/elasticsearch/elasticsearch:8.12.0\n</code></pre> <p>y pod\u00edamos a\u00f1adir <code>kibana</code>, tal y como se especifica en las web de elastic.</p> <pre><code># pull the Kibana Docker image.\ndocker pull docker.elastic.co/kibana/kibana:8.12.0\n\n# Start a Kibana container.\ndocker run --name kibana --net elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.12.0\n</code></pre> <p>Con esto, iniciamos el contenedor, si todo funciona bien, nos da un enlace desde el que acceder a kibana: </p> <p>Si abrimos el enlace accedemos al punto de uni\u00f3n de Kibana con Elastic, donde nos solicita que introduzcamos el token para unirnos con elastic</p> <p>y ya tenemos funcionando elastic y kibana. Nos solicita la contrase\u00f1a, que es la misma de elastic.</p> <p>Si en un momento determinado perdemos la contrase\u00f1a, o pasan mas de 30 minutos y tenemos que regenerar el token de uni\u00f3n de kibana, entonces ejecutamos: </p> <pre><code>docker exec -it elasticsearch /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic\ndocker exec -it elasticsearch /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana\n</code></pre> <p>Sin embargo, pero las limitaciones de ejecuci\u00f3n con muchas, por lo que vamos a continuar con una soluci\u00f3n m\u00e1s completa</p>"},{"location":"ElasticStack/1_elastic_stack/#instalacion-de-un-cluster-elasticsearch-y-kibana-utilizando-docker-compose","title":"Instalaci\u00f3n de un cluster <code>elasticsearch</code> y <code>kibana</code> utilizando <code>docker-compose</code>","text":"<p>Vamos a realizar una instalaci\u00f3n de un cluster de 3 nodos elasticsearch mas un nodo kibana utilizando <code>docker-compose</code></p> <p>Par ello, seguimos con las indicaciones que nos ofrece la web de elastic, con alguna m\u00ednima modificaci\u00f3n.</p> <p>Los pasos ser\u00e1n: </p> <ul> <li>Creamos una carpeta donde vamos a alojar la \"composici\u00f3n\"</li> <li>Descargamos los ficheros </li> <li>.env</li> <li>.docker-compose.yml</li> <li>Editamos <code>.env</code> y realizamos los cambios indicados en la web de elastic</li> </ul> <p>Adem\u00e1s se han realizado otros cambios, como asignar m\u00e1s memoria. En concreto en la imagen se ve una configuraci\u00f3n con 1,5 GB por nodo. Es posible que funcione con 1 GB  o tal vez necesites 2GB, seg\u00fan versiones y equipos.</p> <p>Ahora ya ejecutamos el comando para iniciar el sistema:</p> <pre><code>sudo docker-compose up -d # iniciamos el sistema. La opci\u00f3n -d permite una visualizaci\u00f3n sencilla \nsudo docker-compose ps    # mediante este comando, podemos ver si los diferentes contenedores est\u00e1n activos.\n</code></pre> <p>Con el comando <code>docker-compose ps</code> comprobamos que todos los nodos est\u00e1n activos y con esto, ya podemos intentar entrar en kibana de nuevo (http://127.0.0.1:5601/), pero ahora con la nueva contrase\u00f1a: changeme</p> <p>Por \u00faltimo, si queremos detener o eliminar el sistema ejecutaremos</p> <pre><code>sudo docker-compose stop    # detiene los servicios \nsudo docker-compose down    # esta opci\u00f3n elimina los contenedores\nsudo docker-compose down -v # adem\u00e1s de eliminar los contenedores y los vol\u00famenes de datos\n\nsudo docker-compose start   # si hemos parado, si hemos eliminado: up\n</code></pre> <p>Correspondencia de Vol\u00famenes de los contenedores y datos en local</p> <p>Mediante la definici\u00f3n del <code>volume</code>, podemos almacenar los datos de los nodos en local, con la finalidad de preservar los datos en caso de que los nodos tengan cualquier problema. Esto se aplica a este ejemplo tal como se puede ver en el fichero de configuraci\u00f3n .docker-compose.yml. </p> <p>Podemos comprobar d\u00f3nde se encuentran mediante el comando: </p> <pre><code>docker volume ls                        # listamos todos los vol\u00famenes\ndocker volume inspect elastic_esdata01  # inspeccionamos un volumen en concreto\n</code></pre>"},{"location":"ElasticStack/1_elastic_stack/#accediendo-desde-el-terminal-para-verificar-el-estado-del-cluster","title":"Accediendo desde el terminal para verificar el estado del cluster","text":"<p>Si queremos acceder al elastic desde la l\u00ednea de comando, ejecutamos una secuencia similar a la anterior, pero ojo, ahora la ubicaci\u00f3n del certificado ha cambiado, tal y como se puede observar en el fichero de configuraci\u00f3n <code>docker-compose.yml</code>.</p> <p>As\u00ed pues ejecutamos:</p> <pre><code>export ELASTIC_PASSWORD=\"changeme\"\n\n# Copy the http_ca.crt SSL certificate from the container to your local machine.\nsudo docker cp elastic_es01_1:/usr/share/elasticsearch/config/certs/ca/ca.crt ./http_ca.crt\n\n# si no estamos en el root, debemos hacer que nuestro usuario tenga acceso al certificado\nsudo chmod o+r http_ca.crt\n</code></pre> <p>Ahora, podemos comprobar de nuevo el estado del cluster: </p> <p><pre><code>sudo curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200/_cat/health?pretty \nsudo curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200/_cluster/health?pretty \n</code></pre> y obtenemos la salida del estado:</p> <p>Observar que si en alg\u00fan momento cae alguno de los elementos del cluster, el estado se ver\u00e1 comprometido, y podr\u00e1 incluso estar en riesgo de perdida de datos, tal y como se puede ver en la siguiente captura:</p> <p>Tambi\u00e9n podemos pregunta sobre el cluster de nodos: </p> <p><pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200/_cat/nodes?v\n</code></pre> En esta captura podemos apreciar tanto los nodos que componen el cluster, las direcciones, ip, como porcentajes de memoria, cpu, de carga, los roles que tiene cada nodo y qui\u00e9n esta actuando de nodo master</p> <p>Otra forma para comprobar el estado de salud del cluster ser\u00eda mediante </p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200/_cat/health?v\n</code></pre> <p>Aqu\u00ed obtenemos el estado, as\u00ed como la cantidad de nodos destinados a datos, shards, etc... </p>"},{"location":"ElasticStack/1_elastic_stack/#accediendo-desde-la-interface-de-kibana","title":"Accediendo desde la interface de <code>Kibana</code>","text":"<p>Trabajar desde el terminal puede ser engorroso, por ello, podemos aprovechar el interface que nos ofrece <code>kibana</code> para poder realizar las mismas operaciones que estamos realizando desde la l\u00ednea de comando, y por supuesto, en un futuro, nos facilitar\u00e1 la interacci\u00f3n con <code>Elasticsearch</code> gracias a su terminal.</p> <p>As\u00ed pues entramos en <code>Kibana</code>, que nos pedir\u00e1 usuario (elastic) y contrase\u00f1a (changeme), y buscamos la opci\u00f3n de Dev Tools dentro de la secci\u00f3n  management</p> <p>Aqu\u00ed podemos lanzar nuestras peticiones sin necesidad de especificar todas las opciones de seguridad necesarias desde la l\u00ednea de comandos, y adem\u00e1s nos permite mantener un historial sencillo de utilizar.</p>"},{"location":"ElasticStack/1_elastic_stack/#instalacion-de-elasticsearch-en-un-servidor-linux","title":"Instalaci\u00f3n de <code>Elasticsearch</code> en un servidor l\u00ednux","text":"<p>Por \u00faltimo, vamos a presentar otra opci\u00f3n para la instalaci\u00f3n del sistema <code>Elastic stack</code> en un equipo tipo Ubuntu Server.</p> <p>Para ello, como en las secciones anteriores, seguimos los pasos indicados por el propio fabricante: Install Elasticsearch with Debian Package</p> <p>En primer lugar importamos claves y preparamos el sistema para instalar</p> <pre><code>wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg\nsudo apt-get install apt-transport-https\necho \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list\n</code></pre>"},{"location":"ElasticStack/1_elastic_stack/#instalacion-de-elastic_1","title":"Instalaci\u00f3n de <code>Elastic</code>","text":"<p>Una vez realizados los pasos anteriores, para la instalaci\u00f3n de elastic</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install elasticsearch \n</code></pre> <p>Nota: para instalar elastic, kibana y logstash de una vez: <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install elasticsearch logstash kibana\n</code></pre></p> <p>Todo esto generar\u00e1 una salida donde tendremos una imagen similar a la obtenida en la instalaci\u00f3n de ElasticSearch con Docker</p> <p>Posteriormente, realizamos otros cambios, como por ejemplo el fichero <code>/etc/default/elasticsearch</code> que tiene variables internas de elastic</p> <p>Es recomendable activar autoindices, editamos fichero de configuraci\u00f3n</p> <p><pre><code>sudo nano /etc/elasticsearch/elasticsearch.yml\n</code></pre> e incluyendo las l\u00ednea: </p> <pre><code>action.auto_create_index: .monitoring*,.watches,.triggered_watches,.watcher-history*,.ml*\nnetwork.host: 0.0.0.0\n</code></pre> <p>Se debe habilitar el servicio: </p> <p><pre><code>sudo systemctl daemon-reload\nsudo systemctl enable elasticsearch.service\nsudo systemctl start elasticsearch.service\n</code></pre> y ya tenemos el sistema listo para comprobar que esta funcionando</p> <pre><code>curl --cacert /etc/elasticsearch/certs/http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200 \n</code></pre> <p>Las principales rutas del servicio Elasticse`arch son:</p> Carpeta Descripci\u00f3n /etc/elasticsearch Contiene los archivos de configuraci\u00f3n de Elasticsearch /var/lib/elasticsearch Directorio principal de datos de Elasticsearch /var/log/elasticsearch Archivos de registro de Elasticsearch /usr/share/elasticsearch Directorio de instalaci\u00f3n de Elasticsearch /usr/share/elasticsearch/bin Contiene los binarios y scripts de Elasticsearch /usr/share/elasticsearch/config Contiene archivos de configuraci\u00f3n predeterminados de Elasticsearch /usr/share/elasticsearch/data Directorio de datos de Elasticsearch /usr/share/elasticsearch/logs Directorio de registros de Elasticsearch /usr/share/elasticsearch/plugins Directorio de plugins de Elasticsearch"},{"location":"ElasticStack/1_elastic_stack/#instalacion-de-kibana","title":"Instalaci\u00f3n de <code>Kibana</code>","text":"<pre><code>sudo apt-get update &amp;&amp; sudo apt-get install kibana\n</code></pre> <p>Para la configuraci\u00f3n de <code>Kibana</code>, seg\u00fan la misma web del fabricante, seguimos los pasos y despu\u00e9s tambi\u00e9n tenemos que habilitar el acceso desde cualquier equipo. Esto se hace mediante el fichero: </p> <p><pre><code>sudo nano /etc/kibana/kibana.yml\n</code></pre> y de nuevo </p> <p><pre><code>server.host: \"0.0.0.0\"\n</code></pre> y despu\u00e9s habilitamos e iniciamos el servicio (o lo reiniciamos si ya esta activo)</p> <pre><code>sudo systemctl enable kibana.service \nsudo systemctl start kibana.service \n</code></pre> <p>Una vez realizado esto ya podemos acceder al servicio mediante el puerto 5601</p> <pre><code>http://localhost:5601\n</code></pre> <p>La primera vez que lo ejecutemos, nos pide que enlacemos ELK, por lo tanto primero nos pide que introduzcamos el <code>token</code> que generamos mediante el comando: </p> <p><pre><code>sudo /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana\n</code></pre> Posteriormente nos solicita un c\u00f3digo de identificaci\u00f3n de Kibana, que se genera al ejecutar: </p> <pre><code>sudo /usr/share/kibana/bin/kibana-verification-code \n</code></pre> <p>Al introducir estos datos, ya nos permite entrar en el sistema, para lo cual nos pide el usuario: <code>elastic</code> y la contrase\u00f1a, la que hemos obtenido tras las instalaci\u00f3n de <code>Elastisearch</code></p>"},{"location":"ElasticStack/1_elastic_stack/#instalacion-de-logstash","title":"instalaci\u00f3n de <code>Logstash</code>","text":"<p>Siguiendo lo realzado en los puntos anteriores: </p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install logstash\n</code></pre> <p>para probar logstash, podemos hacerlo de la siguiente forma: b\u00e1sicamente, la entrada se replica en la salida:</p> <pre><code>sudo /usr/share/logstash/bin/logstash -e 'input { stdin { } } output { stdout {} }'\n</code></pre> <p>Con esto, nos sale un terminal que replica las teclas que pulsemos.</p>"},{"location":"ElasticStack/1_elastic_stack/#instalacion-de-filebeat","title":"Instalaci\u00f3n de <code>Filebeat</code>","text":"<p>Filebeat se encuenta dento de lo que ser\u00eda <code>Beats</code>, y para enviar registros y datos a Elasticsearch o Logstash para su posterior indexaci\u00f3n, an\u00e1lisis y visualizaci\u00f3n</p> <p>Quick start and installation</p> <p>Para su instalaci\u00f3n: </p> <pre><code>curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.12.0-amd64.deb\nsudo dpkg -i filebeat-8.12.0-amd64.deb\n</code></pre> <p>Puede ser interesante su instalaci\u00f3n para cierto tipo de ingesti\u00f3n de datos.</p>"},{"location":"ElasticStack/1_elastic_stack/#comprendiendo-los-conceptos-de-cluster-indices-y-shards","title":"Comprendiendo los conceptos de cluster, indices y shards","text":"<p>En Elasticsearch, existen varios conceptos clave que son fundamentales para comprender su funcionamiento. Estos incluyen:</p> <ol> <li>Cluster:</li> <li>Un cluster de Elasticsearch consiste en un conjunto de uno o m\u00e1s nodos que trabajan juntos para almacenar y procesar datos.</li> <li>Los nodos en un cluster de Elasticsearch se comunican entre s\u00ed para compartir informaci\u00f3n sobre el estado del cluster y coordinar las operaciones, como la indexaci\u00f3n y la b\u00fasqueda de datos.</li> <li> <p>Los clusters est\u00e1n dise\u00f1ados para ser altamente disponibles y escalables, lo que significa que pueden manejar grandes vol\u00famenes de datos y continuar funcionando incluso si algunos nodos fallan.</p> </li> <li> <p>\u00cdndices:</p> </li> <li>En Elasticsearch, un \u00edndice es una colecci\u00f3n l\u00f3gica de documentos que comparten caracter\u00edsticas similares.</li> <li>Cada documento en un \u00edndice es un objeto JSON que contiene datos.</li> <li>Los \u00edndices se utilizan para almacenar y organizar datos de manera eficiente para su posterior b\u00fasqueda y an\u00e1lisis.</li> <li> <p>Los nombres de \u00edndice son \u00fanicos dentro de un cluster de Elasticsearch y se utilizan para identificar y acceder a los datos almacenados en ellos.</p> </li> <li> <p>Shards:</p> </li> <li>Un shard es una parte de un \u00edndice de Elasticsearch que contiene una porci\u00f3n del conjunto total de datos.</li> <li>Elasticsearch divide los datos de un \u00edndice en m\u00faltiples shards para distribuirlos y paralelizar las operaciones de indexaci\u00f3n y b\u00fasqueda.</li> <li>Los shards pueden ser primarios o r\u00e9plicas. Cada \u00edndice tiene un conjunto de shards primarios y opcionalmente un conjunto de shards r\u00e9plicas que proporcionan redundancia y alta disponibilidad.</li> <li>La distribuci\u00f3n de shards en un cluster de Elasticsearch permite escalar horizontalmente la capacidad de almacenamiento y la capacidad de procesamiento de datos.</li> </ol> <p>Podemos comprobar los indices que tenemos en el cluster </p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XGET 'https://localhost:9200/_cat/indices?v'\n</code></pre> <p>Podemos comprobar los diferentes shards de cada uno de los \u00edndices que hay en el sistema </p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XGET 'https://localhost:9200/_cat/shards?v'\n</code></pre> <p>Ahora vamos a crear un \u00edndice llamado test, especificando que queremos almacenar los datos en dos shards primarios y tener una replica por shard:</p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XPUT https://localhost:9200/test -d' { \"settings\": { \"number_of_shards\": 2, \"number_of_replicas\":1 }}' -H 'Content-Type: application/json'\n</code></pre> <p>Podemos comprobar la distribuci\u00f3n del \u00edndice test dentro de los nodos con el comando:</p> <p><pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XGET https://localhost:9200/_cat/shards/test?v\n</code></pre> El resultado puede ser similar a :</p> <p>En esta imagen podemos ver que hemos utilizado tenemos dos shards primarios en dos de los nodos y que adem\u00e1s en cada uno de estos nodos hay una replica. En este caso, como el cluster esta formado por los tres nodos, los datos de este \u00edndice distribuidos entre los 3 nodos de forma que tenemos shards primarios en los nodos 1 y 2 y tenemos replicas en los nodos 3 y 1, puesto que se indica que haya una replica de cada shard</p> <p>As\u00ed pues  - <code>number_of_shards</code> especifica el n\u00famero de fragmentos en los que se dividir\u00e1 el \u00edndice. Un fragmento es una parte de los datos de un \u00edndice y cada fragmento es un \u00edndice de Lucene independiente que puede estar alojado en un nodo diferente del cl\u00faster de Elasticsearch.    - Elasticsearch distribuye los datos entre los fragmentos seg\u00fan una funci\u00f3n hash del identificador del documento. Cuantos m\u00e1s fragmentos haya, mayor ser\u00e1 la distribuci\u00f3n de los datos y m\u00e1s capacidad de escalabilidad tendr\u00e1 el \u00edndice.    - Sin embargo, el exceso de fragmentos puede tener un impacto negativo en el rendimiento y en los recursos del cl\u00faster, por lo que es importante encontrar un equilibrio.</p> <ul> <li><code>number_of_replicas</code> especifica el n\u00famero de r\u00e9plicas de cada fragmento que se crear\u00e1n en el cl\u00faster. Las r\u00e9plicas son copias id\u00e9nticas de los fragmentos primarios que se utilizan para garantizar la disponibilidad y la tolerancia a fallos.</li> <li>Las r\u00e9plicas se distribuyen en los nodos del cl\u00faster de Elasticsearch de manera que est\u00e9n balanceadas y garantizan que cada fragmento tenga sus r\u00e9plicas en diferentes nodos.</li> <li>Al tener r\u00e9plicas, si un nodo falla, Elasticsearch puede continuar sirviendo las consultas utilizando las r\u00e9plicas disponibles, lo que mejora la disponibilidad del sistema.</li> <li>Sin embargo, el aumento del n\u00famero de r\u00e9plicas tambi\u00e9n aumenta el consumo de recursos, como almacenamiento y uso de CPU, por lo que es importante considerar estos aspectos al configurar las r\u00e9plicas.</li> </ul> <p>Para que quede m\u00e1s claro, vamos a generar un nuevo \u00edndice llamado test_shards donde vamos a repartir los datos entre 3 shards y adem\u00e1s le pedimos dos replicas en cada uno de los shards.</p> <p>El comando para generar el \u00edndice ser\u00eda:</p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XPUT https://localhost:9200/test_shards -d' { \"settings\": { \"number_of_shards\": 3, \"number_of_replicas\":2 }}' -H 'Content-Type: application/json'\n</code></pre> <p>y el resultado obtenido ser\u00eda como sigue:</p> <p>Se puede observar que estamos utilizando los 3 nodos para almacenar el \u00edndice, de forma que en todos ellos tenemos un shard principal (<code>p</code>) y otos dos de replica (<code>r</code>)</p> <p>Ahora, si a\u00f1adimos datos al \u00edndice, estos deben almacenarse en los 3 shards. Vamos a insertar datos y veremos que es as\u00ed: </p> <p>Ejecutamos la orden siguiente que introduce dos documentos en el \u00edndice test_shards:</p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XPOST https://localhost:9200/test_shards/_bulk?pretty -d'\n{ \"index\" : {} }\n{ \"num\": \"2015/56\", \"cliente\": 13, \"importe\":189 }\n{ \"index\" : {} }\n{ \"num\": \"2015/57\", \"cliente\": 45, \"importe\":190 }\n' -H 'Content-Type: application/json'\n</code></pre> <p>y el resultado que tenemos de la inserci\u00f3n nos indica que se ha incluido en 3 shards: </p> <p>En la imagen anterior:  1. Vemos que hay una secci\u00f3n de detalla dos datos de los shards de \u00edndice. 2. En nuestro caso nos indica que se han insertado de forma exitosa en 3 shards 3. Pero en esta imagen tambi\u00e9n podemos ver que tenemos una penalizaci\u00f3n en cuento a tiempo, puesto que ha tardado 31 ms en realizar la inserci\u00f3n. En teor\u00eda, el hecho de insertar en mas de un nodo beneficia la velocidad, sobre todo si se trata de una cantidad importante de datos puesto que se dividen entre todos ellos, pero en este caso real, recordemos que tenemos 3 nodos trabajando sobre la misma m\u00e1quina y los datos son m\u00ednimos por lo tanto en este caso se puede ver perjudicado.</p> <p>El t\u00e9rmino \"took\" en las respuestas de Elasticsearch indica el tiempo total en milisegundos que tom\u00f3 ejecutar la consulta. Esta m\u00e9trica incluye el tiempo de procesamiento de la consulta en Elasticsearch, as\u00ed como cualquier tiempo adicional dedicado a la comunicaci\u00f3n de red, procesamiento en el cliente, etc.</p> <p>Por \u00faltimo, si realizamos un b\u00fasqueda de los datos, tambi\u00e9n nos dir\u00e1 que se encuentran en los 3 shards</p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XGET https://localhost:9200/test_shards/_search?pretty\n</code></pre>"},{"location":"ElasticStack/2_elastic_operaciones/","title":"2. Elasticsearch. Operaciones","text":""},{"location":"ElasticStack/2_elastic_operaciones/#primeros-pasos","title":"Primeros pasos","text":"<p>Como hemos visto anteriormente, un \u00edndice esta compuesto por diversos documentos. Cada elemento o registro que se inserta en un \u00edndice es considerado un documento, y en ellos se representan los datos almacenados sobre los que m\u00e1s tarde realizaremos operaciones.</p> <p>Nosotros para comenzar a conocer las operaciones o posibilidades de elasticserach, tendremos en cuenta los siguientes conceptos.:</p> <ul> <li>Los datos introducidos en un \u00edndice tienen el mismo formato que un fichero JSON.</li> <li>Este fichero es lo que se llama un documento, que se indexa dentro de un \u00edndice</li> <li>Los indices son independiente uno de otros, no se pueden hace joins entre diferentes \u00edndices.</li> <li>Para indexar un documento, se hace mediante una API de \u00edndice, mediante un <code>PUT</code>o un <code>POST</code></li> <li>Cuando hacemos la primera ingesta en un indice, elastic define autom\u00e1ticamente si no la hemos definido previamente.</li> </ul>"},{"location":"ElasticStack/2_elastic_operaciones/#operaciones-basicas","title":"Operaciones b\u00e1sicas","text":"<ul> <li>Creaci\u00f3n de un \u00edndice con <code>PUT</code></li> </ul> <pre><code>PUT mi_indice\n</code></pre> <p>o especificando shards y r\u00e9plicas <pre><code>PUT mi_indice \n{ \n  \"settings\": { \n    \"number_of_shards\": 2, \n    \"number_of_replicas\":2 \n  }\n}\n</code></pre></p> <ul> <li>Eliminar un \u00edndice con <code>DELETE</code></li> </ul> <pre><code>DELETE mi_indice\n</code></pre> <ul> <li>Creaci\u00f3n de un \u00edndice con <code>POST</code></li> </ul> <p><pre><code>POST mi_indice/_doc\n{\n  \"Nombre\": \"Julian\",\n  \"Apellido\": \"Juli Joli\",\n  \"Edad\": 25\n}\n</code></pre> La creaci\u00f3n de un indice as\u00ed como la inserci\u00f3n o actualizaci\u00f3n de documentos en el mismo se puede realizar mediante <code>PUT</code> o mediante <code>POST</code>. En los ejemplos anteriores, con <code>PUT</code> simplemente se crea un \u00edndice y con <code>POST</code> se introduce in documento en un \u00edndice, y en el caso de que el \u00edndice no exista, se crea.</p> <p>La principal diferencia entre <code>PUT</code> y <code>POST</code> en Elasticsearch radica en c\u00f3mo manejan la identificaci\u00f3n de documentos y la indexaci\u00f3n. <code>PUT</code> se utiliza para operaciones de indexaci\u00f3n o reindexaci\u00f3n espec\u00edficas con una ID expl\u00edcita, mientras que <code>POST</code> se utiliza para operaciones de indexaci\u00f3n m\u00e1s flexibles donde Elasticsearch puede generar autom\u00e1ticamente la ID del documento si no se especifica. Esto se traduce en que <code>PUT</code> es idempotente, mientras que <code>POST</code> no lo es.</p> <p>En todo caso, <code>PUT</code> se suele utilizar normalmente para la creaci\u00f3n de \u00edndices y <code>POST</code> para la inserci\u00f3n y actualizaci\u00f3n de documentos.</p> <ul> <li>Consulta de los datos de un \u00edndice</li> </ul> <p><pre><code>GET mi_indice/_search\n</code></pre> Obtenemos todos los hits (registros) del \u00edndice. Posteriormente profundizaremos sobre esta opci\u00f3n que es la m\u00e1s potente de elastic</p> <ul> <li>Introducimos un documento con un \u00edndice concreto</li> </ul> <p><pre><code>POST mi_indice/_doc/1\n{\n  \"Nombre\": \"Pedro\",\n  \"Apellido\": \"Pedrito Pedrete\",\n  \"Edad\": 24\n}\n</code></pre> Asignamos un <code>id</code> espec\u00edfico a este documento</p> <ul> <li>Modificaci\u00f3n de un documento por \u00edndice</li> </ul> <p><pre><code>POST mi_indice/_doc/1\n{\n  \"Nombre\": \"Juan\",\n  \"Apellido\": \"Juanito Juante\",\n  \"Edad\": 26\n}\n</code></pre> En este caso, machacamos el registro anterior con esta nuevo.</p> <ul> <li>Actualizaci\u00f3n mediante <code>_update</code></li> </ul> <p><pre><code>POST mi_indice/_update/1\n{\n  \"doc\" : \n  {\n    \"edad\": \"62\"\n  }\n}\n</code></pre> Utilizando <code>_update</code> podemos actualizar un campo del documento, en este caso referenciado por un <code>id</code></p> <p><pre><code>POST mi_indice/_update/1\n{\n  \"doc\" : \n  {\n    \"Localidad\": \"Xativa\"\n  }\n}\n</code></pre> Observar el resultado, a\u00f1adimos un nuevo campo sobre el documento existente.</p> <ul> <li>Eliminar un documento concreto por <code>id</code> <pre><code>DELETE mi_indice/_doc/1\n</code></pre> Eliminamos selectivamente el documento. Observar el <code>_doc</code></li> </ul>"},{"location":"ElasticStack/2_elastic_operaciones/#el-metodo-_bulk","title":"El m\u00e9todo <code>_bulk</code>","text":""},{"location":"ElasticStack/2_elastic_operaciones/#ingestion-de-documentos-masiva","title":"Ingesti\u00f3n de documentos masiva","text":"<p>Este m\u00e9todo nos permite ingestar mas de un documento de una \u00fanica vez</p> <p><pre><code>POST mi_indice/_bulk\n{\"index\":{\"_id\":3}}\n{\"Nombre\":\"Evaristo\",\"Apellido\":\"Evarist Evaristate\",\"Edad\":16}\n{\"index\":{\"_id\":4}}\n{\"Nombre\":\"Dorotea\",\"Apellido\":\"Dori Dorita\",\"Edad\":19}\n{\"index\":{\"_id\":5}}\n{\"Nombre\":\"Hortensia\",\"Apellido\":\"Hori Hortensi\",\"Edad\":26}\n</code></pre> Introduce los tres registros simult\u00e1neamente. Observar que se especifica el <code>id</code> y que se omite el <code>_doc</code></p> <p><pre><code>POST mi_indice/_bulk\n{\"index\":{}}\n{\"Nombre\":\"Lola\",\"Apellido\":\"Lolita Loleira\",\"Edad\":20}\n{\"index\":{}}\n{\"Nombre\":\"Silvino\",\"Apellido\":\"Silva Silvando\",\"Edad\":21}\n{\"index\":{}}\n{\"Nombre\":\"Torcuato\",\"Apellido\":\"Torcu Torcu\",\"Edad\":22}\n</code></pre> En este caso, se inserta, pero ahora el <code>id</code> se genera autom\u00e1ticamente.</p> <p>y esto que viene a continuaci\u00f3n, tambi\u00e9n funciona:</p> <p><pre><code>POST mi_indice/_bulk\n{ \"index\":{\"_id\":10} }\n{ \"name\":\"john doe\",\"age\":25 }\n{ \"index\":{\"_id\":11} }\n{ \"name\":\"mary smith\",\"age\":32 }\n</code></pre> Otros campos, y sin problemas</p>"},{"location":"ElasticStack/2_elastic_operaciones/#y-mas-cosas-anadir-actualizar-y-borrar","title":"y mas cosas: a\u00f1adir, actualizar y borrar","text":"<p>Veamos el siguiente ejemplo, donde <code>_bulk</code> realiza varias operaciones de una \u00fanica vez:</p> <pre><code>POST mi_indice/_bulk\n{ \"index\":{\"_id\":3}}\n{ \"Apellidos\":\"Evar Eviris\",\"age\":26}\n{ \"delete\":{\"_id\":4}}\n{ \"update\":{\"_id\":10}}\n{ \"doc\": {\"name\":\"John Doe\",\"age\":26}}\n{ \"create\":{\"_id\":8}}\n{ \"Nombre\":\"Gervasio\",\"Apellidos\":\"Ger Gervi\",\"age\":22}\n{ \"create\":{}}\n{ \"Nombre\":\"Tomasa\",\"Apellidos\":\"Tomi Toma\",\"age\":23}\n</code></pre> <p>Como se puede ver, en una misma ejecuci\u00f3n, hemos realizado - Una actualizaci\u00f3n del <code>id</code> 3 (porque ya exist\u00eda) - Un borrado del <code>id</code> 4 - Una actualizaci\u00f3n del <code>id</code> 10 - Una inserci\u00f3n del <code>id</code> 8 - Una inserci\u00f3n con un <code>id</code> autom\u00e1tico Observar que cada uno de estos elementos tiene una respuesta en su ejecuci\u00f3n.</p>"},{"location":"ElasticStack/2_elastic_operaciones/#usando-_bulk-para-operaciones-masiva","title":"Usando <code>_bulk</code> para operaciones masiva","text":"<p>Podemos tener fichero tipo <code>json</code> con un listado de operaciones como las anteriores y lanzarlos directamente sobre elastic, de forma que realizar\u00e1 todas las operaciones simult\u00e1neamente:</p> <p>Dado un fichero llamado vehiculos_aux.json, su ingesta se realizar\u00eda desde la l\u00ednea de comandos, ya que no hay forma de hacerlo desde la interface de <code>kibana</code>.</p> <p>El comando para tal efecto ser\u00eda: </p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200/_bulk -H \"Content-type: application/json\" --data-binary @vehiculos_aux.json \n</code></pre> <p>Volvemos a la interfaz y si tecleamos </p> <p><pre><code>GET vehiculos_test/_search\n</code></pre> Obtendremos un resultado similar al siguiente, donde podemos ver que se ha creado un \u00edndice con m\u00e1s de 10000 registros, llamado *vehiculos_test\", tal y como se especificaba en el fichero</p> <p>Para mas informaci\u00f3n en documentaci\u00f3n oficial de elastic del API de Bulk</p> <p>Tambi\u00e9n existen implementaciones en python, perl, javascript y otro lenguajes para pasar un fichero JSON a un fichero preparado para ser tratado con <code>_bulk</code>\u00e7</p>"},{"location":"ElasticStack/2_elastic_operaciones/#el-metodo-get","title":"El m\u00e9todo <code>GET</code>","text":"<p>El m\u00e9todo <code>GET</code> en Elasticsearch se utiliza principalmente para realizar operaciones de lectura y recuperaci\u00f3n de datos. </p> <p>En apartados anteriores hemos visto como la API <code>GET</code> tambi\u00e9n se puede utilizar para obtener informaci\u00f3n sobre el estado del cluster, ahora veremos c\u00f3mo recuperar documentos individuales, realizar b\u00fasquedas, obtener informaci\u00f3n sobre los mappings de \u00edndice y otros fines de lectura.</p> <p>A continuaci\u00f3n, se presentan algunos ejemplos de c\u00f3mo se puede utilizar el m\u00e9todo GET en Elasticsearch:</p> <ul> <li> <p>Listado de los \u00edndices del cluster <pre><code>GET _cat/indices/?v\n</code></pre></p> </li> <li> <p>Recuperar un documento por su ID: <pre><code>GET mi_indice/_doc/1\n</code></pre> Esta solicitud recupera un hit (documento) espec\u00edfico de un \u00edndice por su ID.</p> </li> <li> <p>Obtener informaci\u00f3n sobre un \u00edndice: <pre><code>GET mi_indice\n</code></pre> Esta solicitud devuelve informaci\u00f3n sobre un \u00edndice espec\u00edfico, incluidos los mappings, la configuraci\u00f3n y la cantidad de documentos.</p> </li> <li> <p>Realizar una b\u00fasqueda: <pre><code>GET mi_indice/_search\n</code></pre> Esta solicitud realiza una b\u00fasqueda en un \u00edndice espec\u00edfico y devuelve los hits (documentos) que coinciden con los criterios de b\u00fasqueda especificados.</p> </li> </ul> <p>M\u00e1s informaci\u00f3n sobre el API <code>_search</code> en la documentaci\u00f3n oficial de elastic</p> <p>Observar el encabezado donde se muestran por ejemplo la cantidad de registro con <code>value</code> y la exactitud de este valor mediante <code>relation</code>, as\u00ed un <code>eq</code> indica un valor exactos, <code>gte</code> m\u00e1s de las indicadas... </p> <p>As\u00ed, en el ejemplo del apartado anterior, al ejecutar la consulta del \u00edndice importado</p> <p><pre><code>GET vehiculos_test/_search\n</code></pre> el resultado es el siguiente, donde indica que hay m\u00e1s de 10.000 hits, y c\u00f3mo podemos ver en el resultado si lo ejecutamos, s\u00f3lo nos muestra unos 10 hits, o sea, no los muestra todos. Esto se hace por cuestiones de optimizaci\u00f3n.</p> <p>Si dentro de la b\u00fasqueda incluimos el siguiente JSON indicando <code>track_total_hits</code>, entonces nos indicar\u00e1 exactamente la cantidad de registros. Ahora Elastic se tomar\u00e1 su tiempo, para calcular la cantidad exacta de hits, cosa que antes no ha hecho </p> <pre><code>GET vehiculos_test/_search\n{\n  \"track_total_hits\": true,\n  \"size\": 5000\n}\n</code></pre> <p>En el caso anterior, hemos a\u00f1adido que nos muestre 5000 hits.</p> <p>En este caso, se obtienen 5000 hits, pero por contra se han utilizado 25ms para obtener estos datos. Por este motivo, elastic esta preparado para ofrecer como m\u00e1ximo 10.000 documentos. Existe otra API llamada <code>scroll</code> que permite recuperar todos los documentos, pero en bloques limitados, de forma que cada petici\u00f3n nos ofrece una cantidad de todos ellos.</p> <ul> <li> <p>Obtener informaci\u00f3n sobre el cluster: <pre><code>GET /_cluster/state\n</code></pre> Esta solicitud devuelve informaci\u00f3n sobre el estado actual del cluster, incluidos los nodos, los shards y la configuraci\u00f3n.</p> </li> <li> <p>Obtener informaci\u00f3n sobre nodos individuales: <pre><code>GET /_nodes\n</code></pre> Esta solicitud devuelve informaci\u00f3n sobre todos los nodos en el cluster, incluyendo estad\u00edsticas de uso de recursos y configuraci\u00f3n.</p> </li> <li> <p>Obtener informaci\u00f3n sobre mappings de \u00edndice: <pre><code>GET mi_indice/_mapping\n</code></pre> Esta solicitud devuelve el mapping de un \u00edndice espec\u00edfico, que describe la estructura de los documentos en el \u00edndice.</p> </li> </ul> <p>O sea, vemos los campos y la tipificaci\u00f3n de datos establecida.</p> <p>Como se puede ver en la captura anterior aparecen todos los campos introducidos en los diferentes documentos de introducidos en el \u00edndice, y su tipificaci\u00f3n b\u00e1sica.</p>"},{"location":"ElasticStack/2_elastic_operaciones/#consulta-de-datos","title":"Consulta de datos.","text":""},{"location":"ElasticStack/2_elastic_operaciones/#un-poco-de-teoria","title":"Un poco de teor\u00eda","text":"<p>Elastic hace una b\u00fasqueda indexada de los elementos, por lo que a la hora de responde puede devolver respuesta ordenadas de forma m\u00e1s o menos acertada</p> <p>En Elasticsearch, al igual que en otros motores de b\u00fasqueda, se utilizan varios conceptos importantes para evaluar y medir el rendimiento de las consultas. Estos conceptos incluyen la relevancia, la precisi\u00f3n y el recall. Aqu\u00ed tienes una descripci\u00f3n de cada uno de ellos:</p> <ol> <li>Relevancia:</li> <li>La relevancia se refiere a la medida en que los resultados devueltos por una consulta son pertinentes o adecuados para la intenci\u00f3n del usuario.</li> <li>En el contexto de Elasticsearch, la relevancia se determina mediante un algoritmo de puntuaci\u00f3n que asigna una puntuaci\u00f3n de relevancia a cada documento en funci\u00f3n de su similitud con los t\u00e9rminos de b\u00fasqueda y otros factores como la frecuencia de los t\u00e9rminos, la longitud del documento, etc.</li> <li> <p>Elasticsearch utiliza el modelo de puntuaci\u00f3n TF-IDF (Term Frequency-Inverse Document Frequency) como uno de los factores para calcular la relevancia de los documentos.</p> </li> <li> <p>Precisi\u00f3n:</p> </li> <li>La precisi\u00f3n se refiere a la proporci\u00f3n de documentos relevantes entre todos los documentos devueltos por una consulta.</li> <li>En otras palabras, la precisi\u00f3n mide qu\u00e9 tan \u00fatiles son los resultados devueltos por una consulta en relaci\u00f3n con la cantidad total de resultados.</li> <li> <p>Una alta precisi\u00f3n significa que la mayor\u00eda de los documentos devueltos son relevantes para la consulta, mientras que una baja precisi\u00f3n indica que muchos de los documentos devueltos son irrelevantes.</p> </li> <li> <p>Recall:</p> </li> <li>El recall, tambi\u00e9n conocido como exhaustividad, se refiere a la proporci\u00f3n de documentos relevantes recuperados por una consulta en relaci\u00f3n con todos los documentos relevantes en la colecci\u00f3n.</li> <li>En otras palabras, el recall mide qu\u00e9 tan bien una consulta recupera todos los documentos relevantes en el conjunto de datos.</li> <li>Un alto recall significa que la mayor\u00eda de los documentos relevantes se recuperan en la consulta, mientras que un bajo recall indica que muchos documentos relevantes se pierden.</li> </ol> <p>Elasticsearch utiliza una puntuaci\u00f3n (score) para determinar la clasificaci\u00f3n de documentos coincidentes</p>"},{"location":"ElasticStack/2_elastic_operaciones/#consultas-dsl","title":"Consultas DSL","text":"<p>Elasticsearch posibilita usar JSON para definir consulta DSL (Lenguaje de Dominio Espec\u00edfico).</p> <p>Vamos a ver c\u00f3mo podemos lanzar diferentes tipos de Query para obtener los hits que mas relevancia tienen para nosotros, para ello vamos a utilizar el \u00edndice que obtendremos de la ingesta del fichero restaurantes_es.json </p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200/_bulk -H \"Content-type: application/json\" --data-binary @restaurantes_es.json \n</code></pre> <p>Una vez introducidos los datos, verificamos y revisamos su estructura:</p> <pre><code>GET restaurantes/_search\n</code></pre>"},{"location":"ElasticStack/2_elastic_operaciones/#match","title":"<code>match</code>","text":"<p>El primer tipo de consulta que vamos a utilizar es el <code>match query</code></p> <p>As\u00ed pues, al ejecutar la siguiente consulta para buscar \"Cocina Tradicional\", obtendremos los siguientes registros:</p> <p><pre><code>GET restaurantes/_search\n{\n  \"size\":50,\n  \"query\":{\n    \"match\": {\n      \"DESCRIPCION\": \"cocina tradicional\"\n    }\n  }\n}\n</code></pre> Como se puede observar, la consulta ha tenido \u00e9xito, ha obtenido un total de 17 registros y podemos observar en los registros del resultado que contiene en la descripci\u00f3n el literal buscado.</p> <p>Nota: se ha especificado un <code>\"size\":50</code> porque como por defecto solo muestra los 10 primeros documentos, para que los muestre todo, en este caso ser\u00e1n 17</p> <p>En los \u00faltimos registros podremos ver que exactamente no obtenemos el literal \"Cocina Tradicional\"</p> <p>Esto es porque realmente ha hecho un or l\u00f3gico entre cocina y tradicional.</p> <p>Si queremos afinar m\u00e1s la b\u00fasqueda y queremos exigir que existan los dos terminos, entonces necesitamos el operador l\u00f3gico and de la siguiente manera</p> <pre><code>GET restaurantes/_search\n{\n  \"size\":50,\n  \"query\":{\n    \"match\": {\n      \"DESCRIPCION\": {\n        \"query\": \"cocina tradicional\",\n        \"operator\": \"and\"\n      }\n    }\n  }\n}\n</code></pre> <p>Ahora nos responde 8 registros y veremos que en todos exige las dos palabras, aunque puede que no se encuentren de forma consecutiva.</p> <p>Otra forma de obligar a la consulta a un m\u00ednimo de coincidencia es mediante el operador <code>minimum_should_match</code> donde es especifica que como m\u00ednimo de todas las palabras a buscar, se deben encontrar una cantidad m\u00ednima de ellas. </p> <p>Veamos el ejemplo buscando \"cocina tradicional canaria\"</p> <pre><code>GET restaurantes/_search\n{\n  \"query\":{\n    \"match\": {\n      \"DESCRIPCION\": {\n        \"query\": \"cocina tradicional canaria\",\n        \"minimum_should_match\": 2\n      }\n    }\n  }\n}\n</code></pre> <p>En este caso, nos devuelve 8 registros. </p> <p>Observar que tenemos una puntuaci\u00f3n para cada uno de los documentos obtenidos en la b\u00fasqueda identificada por el campo <code>max_score</code></p> <p>As\u00ed pues, <code>max_score</code> es un campo que se devuelve en los resultados de una consulta de b\u00fasqueda y que indica la puntuaci\u00f3n m\u00e1xima de relevancia entre todos los documentos recuperados por esa consulta. Este campo se utiliza para proporcionar una medida relativa de la relevancia de los documentos devueltos en comparaci\u00f3n con otros documentos en el conjunto de resultados.</p> <p>El valor de <code>max_score</code> es \u00fatil para los usuarios y los desarrolladores de aplicaciones para comprender la distribuci\u00f3n de relevancia en los resultados de la b\u00fasqueda y para determinar la importancia relativa de los documentos devueltos. Tambi\u00e9n puede ser utilizado para normalizar las puntuaciones de relevancia de otros documentos en el conjunto de resultados, si es necesario.</p> <p>B\u00e1sicamente depende de 3 factores:</p> <ul> <li>Frecuencia del termino: Cuanto m\u00e1s aparece un termino en un campo, m\u00e1s punt\u00faa.</li> <li>Frecuencia inversa del documento: Cuanto m\u00e1s documentos contienen el termino, menos importante es.</li> <li>Longitud del campo: Los campo m\u00e1s cortos son m\u00e1s relevantes que los largos.</li> </ul>"},{"location":"ElasticStack/2_elastic_operaciones/#match_phrase","title":"<code>match_phrase</code>","text":"<p>Para buscar frase y no t\u00e9rminos separados utilizamos el operados <code>match_phrase</code></p> <p>Entonces, si buscamos el literal exacto</p> <pre><code>GET restaurantes/_search\n{\n  \"query\":{\n    \"match_phrase\": {\n      \"DESCRIPCION\": {\n        \"query\": \"cocina tradicional\"\n      }\n    }\n  }\n}\n</code></pre> <p>En este caso nos devuelve solo 7 registros. Si buscamos la canaria, nos reduce la cantidad de elementos encontrados.</p> <p>De las misma forma que el <code>minimum_should_match</code> nos introduc\u00eda una variaci\u00f3n sobre una b\u00fasqueda normal con <code>match</code>, ahora tenemos el par\u00e1metro de <code>slop</code> que nos indica el n\u00famero de t\u00e9rminos que esperamos encontrar m\u00ednimo de entre todos los t\u00e9rminos indicados. </p> <p>O sea, con la siguiente b\u00fasqueda:</p> <pre><code>GET restaurantes/_search\n{\n  \"query\":{\n    \"match_phrase\": {\n      \"DESCRIPCION\": {\n        \"query\": \"cocina tradicional\",\n        \"slop\": 10\n      }\n    }\n  }\n}\n</code></pre> <p>Permitimos que haya hasta 10 palabras entre \"cocina\" y \"tradicional\" puesto que el <code>slot</code> tiene un valor de 10. Si lo cambiamos a 1 por ejemplo, permitir\u00eda encontrar el literal \"cocina canaria tradicional\" pero no el de \"cocina procedente de canarias tradicional\" puesto que tiene m\u00e1s t\u00e9rminos entre los dos indicados.</p>"},{"location":"ElasticStack/2_elastic_operaciones/#range","title":"<code>range</code>","text":"<p>Las consultas de rangos son adecuadas para hacer consultas num\u00e9ricas y especialmente de fechas.</p> <p>Elastic permit el uso de Date math que es un lenguage user fiendly que nos permite especificar fechas una forma m\u00e1s l\u00f3gica y sencilla de entender.</p> <p>Per ejemplo, la siguiente consulta:</p> <pre><code>GET restaurantes/_search\n{\n  \"query\":{\n    \"range\": {\n      \"FECHA_MODIFICACION\": {\n        \"gte\": \"2024-02-11\",\n        \"lte\": \"2024-08-11\"\n      }\n    }\n  }\n}\n</code></pre> <p>es similar a </p> <pre><code>GET restaurantes/_search\n{\n  \"query\":{\n    \"range\": {\n      \"FECHA_MODIFICACION\": {\n        \"gte\": \"now-6M\"\n      }\n    }\n  }\n}\n</code></pre> <p>Ejemplo de uso de Date math usando la fecha de hoy:</p> <ul> <li>B\u00fasqueda de documentos desde la fecha de hoy hasta hace una semana:</li> </ul> <pre><code>{\n  \"query\": {\n    \"range\": {\n      \"fecha\": {\n        \"gte\": \"now-1w/d\",\n        \"lte\": \"now/d\"\n      }\n    }\n  }\n}\n</code></pre> <p>Esto buscar\u00e1 documentos con un campo \"fecha\" dentro del rango de hace una semana hasta la fecha actual.</p> <ul> <li>B\u00fasqueda de documentos desde el comienzo del mes actual hasta la fecha de hoy:</li> </ul> <pre><code>{\n  \"query\": {\n    \"range\": {\n      \"fecha\": {\n        \"gte\": \"now/M\",\n        \"lte\": \"now/d\"\n      }\n    }\n  }\n}\n</code></pre> <p>Esto buscar\u00e1 documentos con un campo \"fecha\" dentro del rango desde el primer d\u00eda del mes actual hasta la fecha actual.</p> <ul> <li>B\u00fasqueda de documentos desde la fecha de hoy hasta hace tres meses:</li> </ul> <pre><code>{\n  \"query\": {\n    \"range\": {\n      \"fecha\": {\n        \"gte\": \"now-3M/d\",\n        \"lte\": \"now/d\"\n      }\n    }\n  }\n}\n</code></pre> <p>Esto buscar\u00e1 documentos con un campo \"fecha\" dentro del rango de hace tres meses hasta la fecha actual.</p> <p>Si buscamos estrictamente el formato de las fechas en elastic, podemos simplificar en la siguiente imagen:</p> <p>M\u00e1s informaci\u00f3n sobre Date Math en la documentaci\u00f3n de elastic: Date math expressions</p>"},{"location":"ElasticStack/2_elastic_operaciones/#busquedas-combinadas-y-buenas-practicas","title":"B\u00fasquedas combinadas y buenas pr\u00e1cticas","text":"<p>Veamos como podemos operar con todo lo visto anteriormente media su combinaci\u00f3n. </p> <p>Para ello utilizaremos las bool query que est\u00e1n compuestas de varias clausulas como son:  - must, que establece condiciones que son de obligado cumplimiento - must_not, que es lo contrario del anterior. Elementos que no queremos encontrarnos. - should, clausula permisiva que abre la consulta a textos o n\u00fameros que pueden estar o no, o incluso que se asemejen a una condici\u00f3n or. En este caso no se excluyen documentos y se pueden aplicar todas las clausulas vistas anteriormente. - filter, donde filtramos y ordenamos los documentos obtenidos. Esta clausula no afecta al <code>score</code> y se aplica una vez obtenidos los resultados. Es suele filtrar por fecha, n\u00fameros... </p> <p>Por ejemplo:</p> <p>Hacemos una b\u00fasqueda donde se encuentre el literal \"cocina\" o \"tradicional\" y adem\u00e1s queremos aplicar un filtro para que nos muestre solo los resultados entre unas fechas dadas.</p> <pre><code>GET restaurantes/_search\n{\n  \"query\":{\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"DESCRIPCION\": \"cocina tradicional.\"\n          }\n        }\n      ],\n      \"filter\": [\n        {\n          \"range\": {\n            \"FECHA_MODIFICACION\": {\n              \"gte\": \"2024-02-01\",\n              \"lte\": \"now\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre> <p>Recordad que si cambiamos <code>match</code> por <code>match_phrase</code>, entonces exigiremos el literal.</p> <p>En el siguiente ejemplo, buscamos un restaurante de precio gourmet donde puede ser que sirvan o carne o pescado o ninguna de las dos cosas;</p> <pre><code>GET restaurantes/_search\n{\n  \"query\":{\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"PRECIO\": \"gourmet\"\n          }\n        }\n      ],\n      \"should\": [\n        {\n          \"match\": {\n            \"DESCRIPCION\": \"carne\"\n          }\n        },\n        {\n          \"match\": {\n            \"DESCRIPCION\": \"pescado\"\n          }\n        }\n      ]\n    }\n  }\n}\n</code></pre> <p>Si queremos que se cumpla alguna de las condiciones del par\u00e1metro <code>should</code>, debemos introducir el <code>minimun_should_match</code>, y ahora deber\u00e1 cumplirse obligatoriamente la cantidad que se especifique.</p> <pre><code>GET restaurantes/_search\n{\n  \"query\":{\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"PRECIO\": \"gourmet\"\n          }\n        }\n      ],\n      \"should\": [\n        {\n          \"match\": {\n            \"DESCRIPCION\": \"carne\"\n          }\n        },\n        {\n          \"match\": {\n            \"DESCRIPCION\": \"pescado\"\n          }\n        }\n      ],\n      \"minimum_should_match\": 1\n    }\n  }\n}\n</code></pre> <p>En este caso solo se obtiene un resultado, mientras que en el primero obtenemos 7.</p> <p>Realicemos una nueva b\u00fasqueda de un restaurante que abra los lunes, que sea cocina canaria o portuguesa</p> <pre><code>GET restaurantes/_search\n{\n  \"query\":{\n    \"bool\": {\n      \"must\": [\n        {\n          \"match\": {\n            \"HORARIO\": \"lunes\"\n          }\n        }\n      ],\n      \"should\": [\n        {\n          \"match\": {\n            \"DESCRIPCION\": \"canaria\"\n          }\n        },\n        {\n          \"match\": {\n            \"DESCRIPCION\": \"portuguesa\"\n          }\n        }\n      ],\n      \"minimum_should_match\": 1\n    }\n  }\n}\n</code></pre> <p>Al final se trata de ir buscando y revisando los resultado obtenidos, por ejemplo los \u00faltimos registros, para en caso de no ser deseados a\u00f1adir restricciones para que no aparezcan estos documentos.</p> <p>En general, estas son las b\u00fasquedas que podemos realizar. Se podr\u00edan optimizar pero esto ya es cuesti\u00f3n de analizar y afinar tal y como hemos comentado.</p>"},{"location":"ElasticStack/2_elastic_operaciones/#definiendo-un-indice-mappings-y-analizadores","title":"Definiendo un \u00edndice: Mappings y Analizadores","text":""},{"location":"ElasticStack/2_elastic_operaciones/#mappings","title":"Mappings","text":"<p>El mapping es la definici\u00f3n de c\u00f3mo se estructuran los datos dentro de un \u00edndice. Especifica qu\u00e9 campos existen en los documentos, el tipo de datos de cada campo y c\u00f3mo se indexan y almacenan los datos. </p> <p>Cuando creamos un \u00edndice sin especificar nada, elastic crea autom\u00e1ticamente el mapping en funci\u00f3n de los datos introducidos en los documentos, pero si queremos optimizar, entonces podr\u00eda ser adecuado definir nosotros los mappings para despu\u00e9s obtener mejor resultado de nuestras consultas.</p> <p>As\u00ed pues, en el mapping definimos  - Nombre de los campos - Tipos de datos de los campos - Como los campos tienen que guardar los datos.</p> <p>Aqu\u00ed tienes una descripci\u00f3n general de los conceptos relacionados con el mapping en Elasticsearch:</p> <p>Campos: Los campos son los componentes individuales de un documento en Elasticsearch. Cada documento est\u00e1 compuesto por m\u00faltiples campos que representan diferentes atributos o propiedades de los datos.</p> <p>Por ejemplo, en un \u00edndice de documentos de productos, los campos pueden incluir \"nombre\", \"precio\", \"descripci\u00f3n\", etc.</p> <p>Tipos de datos: Elasticsearch admite una variedad de tipos de datos, incluyendo texto, num\u00e9rico, fecha, booleano, geo, y m\u00e1s. Por ejemplo, el tipo de datos \"texto\" se utiliza para campos que contienen texto libre, mientras que el tipo de datos \"num\u00e9rico\" se utiliza para campos que contienen valores num\u00e9ricos.</p> <p>Tipos de datos m\u00e1s comunes</p> Tipo de Datos Tipo en Mapping Descripci\u00f3n Ejemplo Texto \"text\" Cadena de texto analizada token a token \"description\": \"Lorem ipsum dolor sit amet\" Keyword \"keyword\" Cadena de texto no analizada a base de tokens separados \"category\": \"electronics\" Num\u00e9rico \"integer\", \"float\", \"double\", \"long\", etc. Datos num\u00e9ricos \"age\": 30 Fecha \"date\" Fecha y/u hora \"timestamp\": \"2023-11-16T12:00:00\" Booleano \"boolean\" Valor booleano \"is_active\": true Binario \"binary\" Datos binarios, como im\u00e1genes o archivos \"avatar\":  Geo Point \"geo_point\" Punto geogr\u00e1fico \"location\": Geo Shape \"geo_shape\" Forma geoespacial compleja \"area\": Nested \"nested\" Objeto anidado \"comments\": [ {\"user\": \"John\", \"comment\": \"Great post!\"}, {\"user\": \"Alice\", \"comment\": \"Nice article!\"} ] Object \"object\" Objeto JSON \"address\": IP \"ip\" Direcci\u00f3n IP IPv4 o IPv6 \"client_ip\": \"192.168.0.1\" <p>Tambi\u00e9n podemos a encontrarnos con campos que puedan tener multi-tipado, se llaman multi-fields.</p> <p>Com puedes ver, se pueden definir tipos de datos especiales como, coordenadas, figuras, iso pa\u00edses...</p>"},{"location":"ElasticStack/2_elastic_operaciones/#dynamic-mapping","title":"Dynamic Mapping","text":"<p>Elasticsearch genera autom\u00e1ticamente el mapping de campos cuando se indexa un documento por primera vez si se habilita el mapeo din\u00e1mico.</p> <p>Para ver el mapping generado por nuestro ejemplo de restaurantes:</p> <pre><code>GET restaurantes/_mapping\n</code></pre> <p>Como se puede ver, el mapeo se ha creado autom\u00e1ticamente aunque puede no ser muy eficiente y en ocasiones nos suele interesar especificar donde van textos (<code>text</code>) con descripciones largas o campos con claves (<code>keyword</code>) donde posteriormente habr\u00e1 b\u00fasquedas.  </p> <p>Los campos de tipo <code>text</code> se analizan durante el proceso de indexaci\u00f3n. Esto significa que se descomponen en t\u00e9rminos individuales (tokens) para permitir b\u00fasquedas parciales y coincidencias.</p> <p>Los campos de tipo <code>keyword</code> no se analizan durante la indexaci\u00f3n. Se almacenan tal como est\u00e1n.Son ideales para b\u00fasquedas exactas. Puedes buscar valores exactos sin descomponerlos en tokens.</p> <p>El mapeo din\u00e1mico permite que Elasticsearch detecte autom\u00e1ticamente la estructura de los datos cuando se indexa un documento por primera vez y genere el mapping en funci\u00f3n de los campos encontrados en el documento. Esto puede ser conveniente, pero puede llevar a mapping no deseado o inconsistente si los documentos tienen diferentes estructuras.</p>"},{"location":"ElasticStack/2_elastic_operaciones/#explicit-mapping","title":"Explicit mapping","text":"<p>El mapeo expl\u00edcito implica definir manualmente el mapping de un \u00edndice antes de indexar cualquier documento. Esto proporciona un control preciso sobre la estructura de los datos.</p> <p>Por ejemplo, para crear el mapping de nuestro \u00edndice de restaurantes, primero borramos el \u00edndice y despu\u00e9s lo volvemos a crear. Com\u00fanmente lo que se hace es que se importa un conjuntos de datos, se obtienen el mapeo din\u00e1mico y a partir de este, se analiza y optimiza generando un mapeo explicito</p> <pre><code>DELETE restaurantes                 // primer borramos el \u00edndice existente\n\nPUT restaurantes\n{\n  \"mappings\": { \n    \"properties\": {\n      \"DESCRIPCION\":{\n        \"type\": \"text\"              // Se ha dejado solo texto \n      },\n      \"DIRECCION\":{\n        \"type\": \"keyword\"           // Lo dejamos como palabras clave\n      },\n      \"ESPECIALIDAD\": {\n        \"type\": \"keyword\"           // Lo dejamos como palabras clave\n      },\n      \"FECHA_MODIFICACION\": {\n        \"type\": \"date\"\n      },\n      \"HORARIO\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n      \"ID\": {\n        \"type\": \"long\"\n      },\n      \"LATITUD\": {\n        \"type\": \"float\"\n      },\n      \"LONGITUD\": {\n        \"type\": \"float\"\n      },\n      \"NOMBRE\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n      \"POIS\": {\n        \"type\": \"geo_point\"         // punto geodesico\n      },\n      \"PRECIO\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n      \"TELEFONO\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n      \"WEB\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      },\n      \"uri\": {\n        \"type\": \"text\",\n        \"fields\": {\n          \"keyword\": {\n            \"type\": \"keyword\",\n            \"ignore_above\": 256\n          }\n        }\n      }  \n    }\n  }\n}\n</code></pre> <p>Tambi\u00e9n podemos crear el \u00edndice y despu\u00e9s a\u00f1adir al mapping, la sintaxis cambia un poco</p> <pre><code>DELETE restaurantes               // primer borramos el \u00edndice existente\n\nPUT restaurantes                  // creamos el \u00edndice\n\nPUT restaurantes/_mapping         // a\u00f1adimos el mapping\n{\n  //  \"mappings\": {                 // esto ahora se quita   \n  \"properties\": {\n    \"DESCRIPCION\":{\n      \"type\": \"text\"              \n    },\n  ...                             // resto del mapping\n  }\n}\n</code></pre> <p>Puede haber problemas con los datos de fechas y similares. En caso de problema, el documento no se insertar\u00e1 por lo tanto revisamos si hay alg\u00fan problema tras la ingesta para verificar que todo es correcto.</p> <p>No se puede modificar un mapping una vez creado, como mucho podemos modificar el mapping creando campos nuevos y pasando los datos de los viejos a los nuevos e ir eliminando campos viejos, pero esto no suele ser viable. Algunos cambios, como cambiar el tipo de un campo existente, pueden ser restrictivos y requerir reindexaci\u00f3n de los datos existentes, o sea, que para cambiar un tipo de un campo, debemos reintroducir todos los datos del \u00edndice.</p> <p>Lo adecuado es analizar los datos y crear el mapping antes de su ingesta. Podemos ir cambiado campo a campo y realizar la ingesta de una parte peque\u00f1a de los campos tras cada modificaci\u00f3n para verificar que todo funciona correctamente. </p> <p>Por otra parte, es posible actualizar el mapping de un \u00edndice existente para agregar nuevos campos, cambiar tipos de datos, ajustar propiedades de campo, etc.</p>"},{"location":"ElasticStack/2_elastic_operaciones/#analizadores-de-textos","title":"Analizadores de textos","text":"<p>Cuando realizamos una b\u00fasqueda, podemos ver que en ocasiones dos consultas muy parecidas tiene un <code>score</code>id\u00e9ntico: </p> <p>Esto ocurre porque hay un proceso de an\u00e1lisis que ocurre cuando se ingestan los datos. En este proceso, por ejemplo, se pasa todo a min\u00fasculas, por lo que realmente no importa si las consulta se realizan con may\u00fasculas o min\u00fasculas. </p> <p>Este an\u00e1lisis de datos proporciona la generaci\u00f3n de unos tokens que son las palabras principales que podemos extraer de cualquier texto:</p> <p>De esta manera, para que el an\u00e1lisis se se hace hace de los textos grandes sean mejor \u201csearchables\u201d se parte el texto en TOKENS, los cuales se indexan como un \u00fanico documento asociados al ID.</p>"},{"location":"ElasticStack/2_elastic_operaciones/#analizadores-en-elasticsearch","title":"Analizadores en Elasticsearch","text":"<p>Los analizadores en Elasticsearch se componen de varios componentes que trabajan juntos para procesar y normalizar el texto durante la indexaci\u00f3n y la b\u00fasqueda. Los componentes principales de un analizador en Elasticsearch:</p> <ul> <li>Tokenizer (Tokenizador): Divide el texto en unidades m\u00e1s peque\u00f1as llamadas \"tokens\" o \"tokens\". Estos tokens son las unidades b\u00e1sicas de procesamiento en Elasticsearch.Elasticsearch proporciona varios tokenizadores predefinidos, como <code>standard</code>, <code>whitespace</code>, <code>keyword</code>, <code>letter</code>, etc., cada uno de los cuales divide el texto de diferentes maneras.</li> <li>Token Filters (Filtros de Tokens): Procesan los tokens generados por el tokenizador para realizar transformaciones adicionales, como eliminar caracteres especiales, convertir letras a min\u00fasculas, eliminar palabras vac\u00edas (stop words), realizar stemming (reducci\u00f3n de palabras a su forma base), etc. Existe una varios filtros de tokens predefinidos y adem\u00e1s se pueden crear filtros personalizados seg\u00fan sea necesario.</li> <li>Character Filters (Filtros de Caracteres): Se aplican antes del tokenizador y permiten realizar transformaciones en el texto en bruto antes de que se divida en tokens. Permiten realizar operaciones como eliminar caracteres especiales, convertir caracteres a min\u00fasculas o may\u00fasculas, reemplazar caracteres por otros, etc.</li> <li>Analyzer (Analizador): Que encapsula el tokenizador, los filtros de tokens y los filtros de caracteres en una configuraci\u00f3n coherente que se puede aplicar a un campo durante la indexaci\u00f3n y la b\u00fasqueda. Los analizadores son la unidad central de procesamiento de texto en Elasticsearch y se pueden configurar y personalizar para satisfacer las necesidades espec\u00edficas de indexaci\u00f3n y b\u00fasqueda de una aplicaci\u00f3n.</li> <li>Analyzer Chains (Cadenas de Analizadores): Secuencias de analizadores, tokenizadores y filtros que se aplican en orden al texto durante la indexaci\u00f3n y la b\u00fasqueda. Es com\u00fan configurar cadenas de analizadores personalizadas que incluyan una combinaci\u00f3n espec\u00edfica de tokenizadores y filtros para adaptarse a los requisitos de an\u00e1lisis de texto de una aplicaci\u00f3n.</li> </ul>"},{"location":"ElasticStack/2_elastic_operaciones/#analizadores-personalizados","title":"Analizadores personalizados","text":"<p>Podemos crear un analizador personalizado a partir de una analizador est\u00e1ndar para optimizar las b\u00fasquedas en nuestro \u00edndice.</p> <p>Por ejemplo, los analizadores est\u00e1ndar tipo <code>stop</code> (que eliminan ciertos t\u00e9rminos), est\u00e1n muy desarrollados para el idioma ingl\u00e9s en elasticsearch, pero no tanto para nuestro idioma., por lo que podemos definir un analizador personalizado para nuestro \u00edndice.</p> <p>Para crear un analizador personalizado, se realizar\u00e1 dentro de la secci\u00f3n <code>settings</code> en la definici\u00f3n del \u00edndice.</p> <p>No vamos a profundizar mucho en el tema, pero un ejemplo ser\u00eda el siguiente:</p> <pre><code>PUT restaurantes\n{\n  \"settings\": {\n    \"analysis\": {\n      \"filter\": {             // definici\u00f3n de filtro\n        \"spanish_stop\": {\n          \"type\": \"stop\",\n          \"stopwords\": [ \"en\", \"mi\", \"a\", \"y\", \"de\", \"contra\", \"para\"]\n        }\n      },    \n      \"analyzer\": {           // definici\u00f3n de analizador\n        \"mi_analizador\": {\n          \"type\": \"custom\",\n          \"char_filter\": [],\n          \"tokenizer\": \"standard\",\n          \"filter\": [ \"lowercase\", \"spanish_stop\"]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"ElasticStack/2_elastic_operaciones/#definiendo-un-indice-con-mapping-y-analizador","title":"Definiendo un \u00edndice con mapping y analizador","text":"<p>Visto que podemos definir un mapping especificando los datos de los campos que nos interesen y posteriormente podemos indicar el tokenizador, todo esto, nos permite crear \u00edndices optimizados para las b\u00fasquedas que vamos a dise\u00f1ar sobre el \u00edndice en cuesti\u00f3n.</p> <p>As\u00ed pues, para la creaci\u00f3n de un \u00edndice, podemos especificar las dos partes:</p> <pre><code>PUT restaurantes\n{\n  \"settings\": {\n                    // aqui definimos el tokenizador\n  },\n  \"mappings\": {\n                    // aqu\u00ed cremos el mapeo del \u00edndice\n  }\n}\n</code></pre> <p>Note</p> <p>Recordad que NO SE PUEDE CAMBIAR UN MAPPING y que por ellos se debe probar las veces necesaria hasta alinear la definici\u00f3n con nuestra necesidades y poner un \u00edndice en producci\u00f3n.</p>"},{"location":"ElasticStack/3_elastic_kibana/","title":"3. Kibana","text":""},{"location":"ElasticStack/3_elastic_kibana/#que-es-kibana","title":"\u00bfQu\u00e9 es <code>kibana</code>?","text":"<p><code>Kibana</code> es una herramienta de visualizaci\u00f3n de datos y an\u00e1lisis para Elasticsearch. Forma parte del Elastic (Elasticsearch, Logstash, Kibana) y se utiliza com\u00fanmente para explorar, visualizar y analizar datos almacenados en Elasticsearch.</p> <p>Algunas caracter\u00edsticas principales de Kibana son:</p> <ul> <li> <p>Exploraci\u00f3n de Datos: Kibana permite a los usuarios explorar grandes vol\u00famenes de datos almacenados en Elasticsearch mediante la realizaci\u00f3n de consultas y filtros de b\u00fasqueda avanzados.</p> </li> <li> <p>Visualizaciones Interactivas: Proporciona una variedad de opciones de visualizaci\u00f3n, como gr\u00e1ficos de barras, gr\u00e1ficos circulares, mapas, series de tiempo, etc., que pueden ser personalizados y configurados para mostrar datos de manera clara y comprensible.</p> </li> <li> <p>Dashboards Personalizados: Permite a los usuarios crear dashboards personalizados que combinan m\u00faltiples visualizaciones en una sola p\u00e1gina para obtener una visi\u00f3n general completa de los datos.</p> </li> <li> <p>Herramientas de An\u00e1lisis: Kibana ofrece herramientas de an\u00e1lisis como agregaciones, m\u00e9tricas y filtros que permiten a los usuarios realizar an\u00e1lisis de datos avanzados para descubrir patrones, tendencias y anomal\u00edas en los datos.</p> </li> <li> <p>Gesti\u00f3n de Usuarios y Permisos: Proporciona capacidades de autenticaci\u00f3n y autorizaci\u00f3n para gestionar el acceso de los usuarios a los datos y las funcionalidades de Kibana.</p> </li> <li> <p>Integraci\u00f3n con Elasticsearch: Est\u00e1 estrechamente integrado con Elasticsearch, lo que facilita la visualizaci\u00f3n y el an\u00e1lisis de los datos almacenados en los \u00edndices de Elasticsearch.</p> </li> </ul> <p>Para nosotros en concreto, Kibana representa el framework visual de elasticsearch que nos permitir\u00e1:  - Dar forma a los datos almacenados en Elasticsearch - Crear objetos visuales a partir de los \u00edndices llamados indexPattern - Crear Visualizaciones - Crear Dashboards a partir de las Visualizaciones</p>"},{"location":"ElasticStack/3_elastic_kibana/#primera-visualizacion-de-datos-en-kibana-data-view-y-discover","title":"Primera visualizaci\u00f3n de datos en Kibana. <code>data view</code> y <code>discover</code>","text":"<p>Para visualizar datos en Kibana, en primer lugar debemos habilitar un \u00edndice de elasticsearch (en nuestro caso el de restaurantes) para poder ser tratado desde kibana. Esto se hace creando un Data View (anteriormente index pattern)</p> <p>Un Data View en Kibana se refiere al proceso de explorar, visualizar y manipular datos almacenados en Elasticsearch utilizando las herramientas y funcionalidades proporcionadas por la plataforma. Esto incluye la configuraci\u00f3n de index patterns, la creaci\u00f3n de visualizaciones y dashboards, y la realizaci\u00f3n de an\u00e1lisis de datos para extraer informaci\u00f3n significativa de los datos.</p> <p>Para gestionar los Data Views, vamos en la interface de Kibana a buscar la entrada de <code>Stack Management</code> dentro de la secci\u00f3n <code>Management</code> (parte inferior del men\u00fa)</p> <p>Desde este punto podemos administrar nuestras instalaci\u00f3n de Elastic y Kibana, por ejemplo en la secci\u00f3n de <code>Data</code> podemos ver la gesti\u00f3n de todos los \u00edndices en <code>Index Management</code></p> <p>En las secci\u00f3n de <code>kibana</code>, tenemos como primera opci\u00f3n <code>Data Views</code> donde podemos crear un nuevo Data View seleccionando sobre el bot\u00f3n de Create Data View y aparece una ventana para la creaci\u00f3n del Data View o Index Patern.</p> <p>Completamos el nombre del Data View, indicamos el \u00edndice y si este \u00edndice tiene un campo de tipo fecha, lo seleccionamos:</p> <p>Este Data View o Index Partern es un mapeo de nuestro \u00edndice que lo prepara para poder trabajar con el desde Kibana.</p> <p>Llegados a este punto, ya podemos hacer una primera visualizaci\u00f3n de los datos de nuestro \u00edndice, para ello, volvemos sobre el men\u00fa principal y pulsamos sobre el \u00edtem <code>Discover</code> de la secci\u00f3n de <code>Analitics</code>.</p> <p>En nuestro caso, como solo tenemos este Index Pattern directamente ya nos salen datos del \u00edndice. En caso de que no saltan, entonces deberemos cambiar el intervalo de fechas (4)en los cuales buscar, puesto que por defecto solo muestra los \u00faltimos 15 minutos.</p> <p>Una vez ya visualizamos datos, podemos crear vistas seleccionando los datos o filtros que aparecen a la izquierda, se puede reordenar estas vistas e incluso guardar.</p> <p>Tambi\u00e9n podemos exportar los datos del reporte seleccionando <code>share</code> y <code>CSV Reports</code>, entonces nos generar\u00e1 un fichero csv al que podemos acceder desde el men\u00fa general, <code>stack management</code> y <code>reporting</code></p>"},{"location":"ElasticStack/3_elastic_kibana/#visualizaciones-en-kibana","title":"Visualizaciones en <code>kibana</code>","text":"<p>Antes de crear un cuadro de mandos o dashboard necesitamos crear visualizadores que son los elementos que despu\u00e9s incrustaremos en los cuadros de mando.</p> <p>Para acceder a los visualizadores debemos ir al men\u00fa principal, despu\u00e9s <code>Analytics</code> y seleccionamos <code>Visualize library</code>.</p> <p>A partir de aqu\u00ed crear visualizadores.</p> <p>En la documentaci\u00f3n de <code>kibana</code> tenemos un tutorial: Create your first dashboard.</p> <p>Respecto a nuestro ejemplo de restaurantes podemos crear varios visualizadores.</p> <p>Para crearlo, al pulsar el bot\u00f3n de \"Create Visualization\" nos aparece una pantalla donde podemos elegir el tipo visualizaci\u00f3n. Tradicionalmente se ha utilizado la opci\u00f3n de <code>Aggregation Based</code>, aunque la \u00faltima versi\u00f3n a a\u00f1adido la opci\u00f3n de <code>Lens</code></p> <p>Una vez ah\u00ed, silenciamos el tipo de visualizador a realizar: </p> <p>Veamos varios ejemplos.</p> <ul> <li>Tipo M\u00e9trica, que nos cuenta la cantidad de rese\u00f1as (entre las fechas seleccionadas)</li> </ul> <ul> <li>Tipo Barra: Creamos un visualizador que muestra la cantidad de rese\u00f1as seg\u00fan el rango de precios. Para ello seleccionamos en Buckets (divisiones) una agrupaci\u00f3n por un termino (terms) sobre el campo PRECIO</li> </ul> <p>Cuando guardamos el visualizador, nos pregunta si lo queremos a\u00f1adir a un dashboard existente o lo dejamos en la librer\u00eda.</p> <ul> <li>Gr\u00e1fico tipo area</li> </ul> <p>Seleccionamos el tipo data histagram y a partir de ahi jugamos con los elementos que queramos. Nos mostrar\u00e1 una gr\u00e1fica de las reviews por fecha</p> <ul> <li>Horizontal bar afectado por un Filtro.</li> </ul> <p>Seleccionamos termino, especialidad y despu\u00e9s aplicamos un filtro.</p> <ul> <li>Mapa con coordenadas:</li> </ul> <p>Seleccionamos Nueva Visualizaci\u00f3n, y elegimos tipo <code>Maps</code>:</p> <p>Despu\u00e9s seleccionamos add layer para a\u00f1adir una nueva capa, y luego seleccionamos Documents para seleccionar un \u00edndice. A continuaci\u00f3n nos pide un Data View y luego que seleccionamos el campo que contiene las coordenadas. </p> <p>Seleccionamos Add and Continue y entraremos en un men\u00fa con las opciones de visualizaci\u00f3n de la nueva capa del mapa, donde por ejemplo le debemos indicar el nombre como m\u00ednimo.</p> <p>Ya tenemos la nueva capa a\u00f1adida al mapa. </p> <p>Podemos agregar tantas capas como queramos, as\u00ed como visualizarlas o no, incluso redimensionar el mapa para que se ajuste las coordenadas.</p>"},{"location":"ElasticStack/3_elastic_kibana/#dashboards-en-kibana","title":"Dashboards en <code>kibana</code>","text":"<p>Una vez visto c\u00f3mo crear visualizaciones, el siguiente paso es crear dashboards para agruparlas.</p>"},{"location":"ElasticStack/3_elastic_kibana/#creacion-y-gestion-de-dashboards","title":"Creaci\u00f3n y gesti\u00f3n de <code>dashboards</code>","text":"<p>En el men\u00fa principal, secci\u00f3n <code>Analytics</code> tenemos el acceso a <code>Dashboards</code>. Seleccionando la opci\u00f3n nos permite crear un nuevo panel y a su vez crear una visualizaci\u00f3n o seleccionar una de las creadas anteriormente.</p> <p>El proceso es muy intuitivo, se trata de a\u00f1adir los visualizadores o crearlos nuevo. Una vez en el Dashboard se pueden mover, redimensionar, acceder al editor de visualizadores para cambiar lo que creamos necesarios.</p> <p>El Dashboard se puede guardar asignando un nombre y estableciendo o no el intervalo de tiempo por defecto.</p> <p>Observar que si pulsamos sobre cualquier elementos visualizado podemos crear filtros de la misma forma que podemos ir cambiando los rangos de fechas.</p>"},{"location":"ElasticStack/3_elastic_kibana/#espacios-de-trabajo","title":"Espacios de trabajo","text":"<p>Los dashboards se organizan en lo que se llama Spaces. Lo habitual es que cada usuario o grupo de usuarios tiene su espacio de trabajo y ah\u00ed tiene los dashboards con lo que trabaja cada d\u00eda. As\u00ed creamos tantos spaces como tipos de usuarios tengamos en nuestro sistema. </p> <p>Para acceder a los diferentes spaces lo podemos hacer desde el bot\u00f3n verde de la parte superior izquierda: </p> <p>Inicialmente, nuestro primer espacio es el default, podemos cambiar el nombre, y crear nuevos espacios de trabajo.</p> <p>Cuando creamos un nuevo space adem\u00e1s de asignar nombre, descripci\u00f3n, etc.. podemos asignar qu\u00e9 elementos son accesibles desde este espacio de trabajo. </p> <p>Observar que tambi\u00e9n se puede acceder desde el men\u00fa principal, desde <code>kibana</code> y despu\u00e9s <code>Spaces</code>.</p> <p>Una vez vez creados los espacios, ya podemos acceder a los mismo. Veremos que cada Space es como un perfil de <code>Kibana</code> totalmente diferente.</p>"},{"location":"ElasticStack/3_elastic_kibana/#ejemplo","title":"Ejemplo","text":"<p>Video de introducci\u00f3n a kibana: </p> <p>Una vez descargados los datos, lo aconsejable es ir a <code>Analytics</code> - <code>Discover</code> para ver los datos que tenemos</p> <p>Si queremos ver c\u00f3mo un registro hacemos clic sobre cualquier registro y podemos verlo tanto en formato tabla como en formato JSON.</p> <p>En cualquier momento, podemos crear un nuevo campo al pulsar sobre el bot\u00f3n verde que nos indica el \u00edndice en la parte superior izquierda</p>"},{"location":"ElasticStack/4_elastic_python/","title":"4. Elasticsearch. Acceso desde cliente Python","text":"<p>La interacci\u00f3n con Elasticsearch no se limita \u00fanicamente a utilizar las herramientas que nos proporciona, como pueden ser <code>logstash</code> o <code>filebeats</code> para realizar ingestas de datos, si no que tambi\u00e9n podemos hacer uso de elastic desde cualquier aplicaci\u00f3n cliente sea en el lenguaje que sea: java, javascript, perl, python, etc...</p> <p>En este sentido, vamos a ver c\u00f3mo acceder a Elastic desde python. No se ha elegido este lenguaje por nada en especial, simplemente es muy utilizado en la actualidad, es eficiente y esta bien documentado con un gran n\u00famero de documentos y ejemplos.</p> <p>Si alguien domina cualquier otro lenguaje, como puede ser javascript puede intentar transformar todos los ejemplos a realizar a continuaci\u00f3n a su lenguaje predilecto.</p> <p>Sobre python tenemos muchos recursos en la red para conocer un podo m\u00e1s: - Web oficial de python - Python espa\u00f1a: Aprender python. - W3schools: Python tutorial</p>"},{"location":"ElasticStack/4_elastic_python/#uso-de-elasticsearch-desde-cliente-en-python","title":"Uso de Elasticsearch desde cliente en python","text":"<p>Informaci\u00f3n oficinal de Elastic de c\u00f3mo utilizar Elasticsearch desde python:</p> <ul> <li>Elastic Docs - Elasticsearch Python Client</li> </ul>"},{"location":"ElasticStack/4_elastic_python/#ejemplo-de-uso-guiado-restaurantes-de-nueva-york","title":"Ejemplo de uso guiado: Restaurantes de Nueva York","text":"<p>En el siguiente enlace tenemos un ejemplo pr\u00e1ctico de c\u00f3mo hacer la ingesta de un dataset que contiene el listado de restaurantes de Nueva York.</p> <p>Los pasos a a seguir por el ejemplo son : 1. Acceso a API y descarga del fichero con datos de restaurantes 2. Acceso a Elastic 3. Creaci\u00f3n de \u00edndice y mapping 4. Ingesta de datos</p> <p>Aqu\u00ed tienes el enlace al ejemplo de ingesti\u00f3n de datos usando python; Bulk Ingest</p> <p>Nota: Observa que por defecto tenemos instalado <code>python3</code> en los sistemas actuales como Ubuntu, por lo que debes reemplazar el comando <code>python</code> por <code>python3</code></p> <p>Nota: si no tienes la herramienta <code>pip</code> instalada en tu sistema, por ejemplo para instalarla en Ubuntu lo haremos mediante el siguiente comando <pre><code>sudo apt install python3-pip\n</code></pre></p> <p>Una vez introducidos los datos, los analizaremos desde el interface de Kibana.</p> <p>En este ejemplo hay un script que hace todo, lo \u00fanico que se debe actualizar es la direcci\u00f3n del cliente</p> <pre><code>    client = Elasticsearch(\n        \"https://localhost:9200\",\n        ca_certs=\"../http_ca.crt\",\n        basic_auth=(\"elastic\", \"changeme\")\n    )\n</code></pre>"},{"location":"ElasticStack/4_elastic_python/#caso-de-uso-accidentes-en-nueva-york","title":"Caso de uso: Accidentes en Nueva York","text":"<p>Vamos a utilizar la misma fuente de datos, para obtener otro conjunto de datos: los accidente de tr\u00e1fico en Nueva York</p>"},{"location":"ElasticStack/5_elastic_nifi/","title":"5. Elastic stack y Apache Nifi","text":"<p>Utilizando la base que tenemos son elasticsearch y kibana sobre docker, a\u00f1adirmos un nuevo contenedor con nifi.</p> <p>Seguimos las indicaciones de dockerhub: apache/nifi.</p> <p>Estas se reducen a ejecutar:</p> <p>Single User Authentication credentials can be specified using environment variables as follows:</p> <pre><code>docker run --name nifi \\\n  -p 8443:8443 \\\n  -d \\\n  -e SINGLE_USER_CREDENTIALS_USERNAME=admin \\\n  -e SINGLE_USER_CREDENTIALS_PASSWORD=ctsBtRBKHRAx69EqUghvvgEvjnaLjFEB \\\n  apache/nifi:latest\n</code></pre>"},{"location":"ElasticStack/CT_proxmox_Pruebas/","title":"CT proxmox Pruebas","text":"<p>Para la ejecuci\u00f3n de docker en un contenedor</p> <p>En primer lugar se debe crear un contenedor por defecto, en la pesta\u00f1a de <code>general</code> tiene que tener marcado tanto Contenedores sin privilegios como Anidado de forma que al final una vez creado el conenedor tenemos en Opciones <code>Privilegios: Si</code> y <code>Funcionalidades: Nesting=1</code></p> <p>Luego, para ejecutar Docker en un contenedor (CT) de Proxmox con una imagen de Ubuntu 24.04, sigue estos pasos:</p> <ol> <li> <p>Actualizar el sistema:    Aseg\u00farate de que tu sistema est\u00e9 actualizado:    <pre><code>sudo apt update\nsudo apt upgrade -y\n</code></pre></p> </li> <li> <p>Instalar paquetes necesarios:    Docker requiere algunos paquetes esenciales. Inst\u00e1lalos con el siguiente comando:    <pre><code>sudo apt install -y apt-transport-https ca-certificates curl software-properties-common\n</code></pre></p> </li> <li> <p>Agregar la clave GPG y el repositorio de Docker:    <pre><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre></p> </li> <li> <p>Instalar Docker:    Actualiza la lista de paquetes e instala Docker:    <pre><code>sudo apt update\nsudo apt install -y docker-ce docker-ce-cli containerd.io\n</code></pre></p> </li> <li> <p>Iniciar y habilitar Docker:    Aseg\u00farate de que el servicio Docker est\u00e9 activo y se inicie autom\u00e1ticamente al arrancar el sistema:    <pre><code>sudo systemctl start docker\nsudo systemctl enable docker\n</code></pre></p> </li> <li> <p>Probar la instalaci\u00f3n de Docker:    Ejecuta el contenedor de prueba <code>hello-world</code> para verificar que Docker est\u00e9 funcionando correctamente:    <pre><code>sudo docker run hello-world\n</code></pre></p> </li> </ol> <p>Si todo est\u00e1 configurado correctamente, deber\u00edas ver un mensaje de \"Hello from Docker!\"</p>"},{"location":"ElasticStack/elastic_notas_.ejemplos/","title":"5. Enlaces interesantes:","text":"<ul> <li> <p>Cluster ElasticSearch con Docker Paso a Paso Blog que monta un cluster con 3 dockers</p> </li> <li> <p>Elasticsearch paso a paso 2023 - youtube</p> </li> <li> <p>Ejemplo de terremotos</p> </li> <li> <p>Instalar Docker compose</p> </li> <li> <p>Youtube: Montar un sistema docker-compose</p> </li> <li> <p>YouTube: Elasticsearch and Kibana Installation with Docker Compose | How to install Elasticsearch in docker </p> </li> <li> <p>Kibana: Get up and running with sample data</p> </li> <li> <p>Beginner's Crash Course to Elastic Stack Series</p> </li> <li> <p>Generador de datos aleatorios</p> </li> </ul>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#interactuando-con-datos-en-elasticsearch","title":"Interactuando con datos en Elasticsearch","text":"<p>Vamos a ver c\u00f3mo interacturar con los datos Elasticsearch. para ello, vamos a importar la base de datos de demo que nos proporciona el propio sistemas. </p> <p>A partir del interface de Kibana, podemos incorporar los datos de ejemplo:</p> <p>A\u00f1adimos los datos del Sample flight data</p>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#metodos-http-mas-importantes-en-elasticseach","title":"M\u00e9todos HTTP m\u00e1s importantes en Elasticseach","text":"<p>Los m\u00e9todos HTTP m\u00e1s importantes de Elasticsearch:</p> <ol> <li>GET:</li> <li>Descripci\u00f3n: Recupera informaci\u00f3n de Elasticsearch, como documentos, metadatos de \u00edndices, estad\u00edsticas del cl\u00faster, etc.</li> <li> <p>Ejemplo: <code>GET /&lt;\u00edndice&gt;/&lt;tipo&gt;/&lt;identificador&gt;</code></p> </li> <li> <p>PUT:</p> </li> <li>Descripci\u00f3n: Agrega o actualiza un documento en un \u00edndice de Elasticsearch.</li> <li> <p>Ejemplo: <code>PUT /&lt;\u00edndice&gt;/&lt;tipo&gt;/&lt;identificador&gt;</code></p> </li> <li> <p>POST:</p> </li> <li>Descripci\u00f3n: Agrega un nuevo documento a un \u00edndice de Elasticsearch o realiza otras operaciones como buscar documentos, crear \u00edndices, etc.</li> <li> <p>Ejemplo: <code>POST /&lt;\u00edndice&gt;/&lt;tipo&gt;</code></p> </li> <li> <p>DELETE:</p> </li> <li>Descripci\u00f3n: Elimina un documento espec\u00edfico de un \u00edndice de Elasticsearch.</li> <li> <p>Ejemplo: <code>DELETE /&lt;\u00edndice&gt;/&lt;tipo&gt;/&lt;identificador&gt;</code></p> </li> <li> <p>HEAD:</p> </li> <li>Descripci\u00f3n: Realiza una solicitud similar a GET pero devuelve solo los encabezados de respuesta, sin el cuerpo de la respuesta.</li> <li> <p>Ejemplo: <code>HEAD /&lt;\u00edndice&gt;/&lt;tipo&gt;/&lt;identificador&gt;</code></p> </li> <li> <p>OPTIONS:</p> </li> <li>Descripci\u00f3n: Proporciona informaci\u00f3n sobre las opciones de comunicaci\u00f3n disponibles para un recurso o el servidor en general.</li> <li>Ejemplo: <code>OPTIONS /&lt;\u00edndice&gt;/&lt;tipo&gt;/&lt;identificador&gt;</code></li> </ol> <p>Estos son algunos de los m\u00e9todos HTTP m\u00e1s importantes utilizados en Elasticsearch para interactuar con los datos y la configuraci\u00f3n del motor de b\u00fasqueda. Cada uno de estos m\u00e9todos se utiliza para realizar operaciones espec\u00edficas, como agregar, actualizar, recuperar o eliminar datos de Elasticsearch, y son fundamentales para interactuar con el sistema de manera efectiva.</p> <p>M\u00e1s informaci\u00f3n detallada : Query DSL (Domain Specific Language)</p>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#get-consulta-de-datos","title":"<code>GET</code> - Consulta de datos","text":"<p>Lo primero que vamos a hacer es consultar los <code>shards</code> del nuevo indice siguiendo lo visto anteriormente.</p> <pre><code>curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD -XGET 'https://localhost:9200/_cat/shards/kibana_sample_data_flights'\n</code></pre> <p>Desde este punto en adelante, vamos a utilizar la interfaz Management -&gt; Dev Tools que proporciona <code>Kibana</code>.</p> <p>Veamos usos mediante los siguientes ejemplos:</p> <ul> <li> <p>Listado completo de todos los indices <pre><code>GET /_cat/indices\n</code></pre></p> </li> <li> <p>Explorar un \u00edndex un \u00edndice en concreto <pre><code>GET /kibana_sample_data_flights/_mapping\n</code></pre></p> </li> <li> <p>Contar total de registros de un \u00edndice:  <pre><code>GET /kibana_sample_data_flights/_count\n</code></pre></p> </li> <li> <p>Primera b\u00fasqueda en el \u00edndice. Acotamos a solo dos registros. Esto nos permite revisar c\u00f3mo es el \u00edndice. <pre><code>GET /kibana_sample_data_flights/_search\n{\n  \"size\": 2,\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n</code></pre></p> </li> <li> <p>B\u00fasqueda de un registro por el valor de un campo determinad</p> </li> </ul> <pre><code>GET /kibana_sample_data_flights/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"default_field\": \"Dest\",\n      \"query\": \"Sydney\"\n    }\n  }\n}\n</code></pre> <ul> <li> <p>Otra forma diferente <pre><code>GET /kibana_sample_data_flights/_search\n{\n  \"query\": {\n    \"match\": {\n      \"FlightNum\": \"EXEMPLE\"\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>\u00cddem anterior, pero filtrando los campos de salida: <pre><code>GET /kibana_sample_data_flights/_search\n{\n  \"query\": {\n    \"match\": {\n      \"FlightNum\": \"EXEMPLE\"\n    }\n  },\n  \"_source\": false,\n  \"fields\": [\n    \"FlightNum\", \"Dest\", \"Origin\"\n  ]\n}\n</code></pre></p> </li> </ul> <p>Simplificando tambi\u00e9n es posible: </p> <ul> <li> <p>Buscando un dato <pre><code>GET /kibana_sample_data_flights/_search?q=\"EXEMPLE\"\n</code></pre></p> </li> <li> <p>Recuperar documentos con filtro por campo:</p> </li> </ul> <pre><code>GET /kibana_sample_data_flights/_search?q=\"FlightNum\" \"EXEMPLE\"\n</code></pre> <ul> <li> <p>Recuperar documentos con paginaci\u00f3n <pre><code>GET /kibana_sample_data_flights/_search?size=1&amp;from=3\n</code></pre></p> </li> <li> <p>Recuperar documentos con ordenaci\u00f3n:</p> </li> </ul> <pre><code>GET /kibana_sample_data_flights/_search?sort=OriginCountry:ASC\n</code></pre>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#6-recuperar-solo-los-campos-especificados-de-los-documentos","title":"6. Recuperar solo los campos especificados de los documentos:","text":"<pre><code>GET /kibana_sample_data_flights/_search?_source=&lt;field1&gt;,&lt;field2&gt;,...\n</code></pre> <p>Estos son solo algunos ejemplos de c\u00f3mo puedes utilizar el m\u00e9todo HTTP GET para interactuar con los datos del \u00edndice <code>kibana_sample_data_flights</code> en Elasticsearch. Puedes combinar diferentes par\u00e1metros de consulta para realizar consultas m\u00e1s avanzadas seg\u00fan tus necesidades espec\u00edficas de an\u00e1lisis y recuperaci\u00f3n de datos.</p>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#post-agregando-registros","title":"<code>POST</code> - Agregando registros","text":"<p>Para crear nuevos documentos</p> <pre><code>POST /kibana_sample_data_flights/_doc\n{\n    \"FlightNum\": \"EXEMPLE\",\n    \"DestCountry\": \"IT\",\n    \"OriginWeather\": \"Clear\",\n    \"OriginCityName\": \"Cape Town\",\n    \"AvgTicketPrice\": 882.9826615595518,\n    \"DistanceMiles\": 5482.606664853586,\n    \"FlightDelay\": false,\n    \"DestWeather\": \"Sunny\",\n    \"Dest\": \"Venice Marco Polo Airport\",\n    \"FlightDelayType\": \"No Delay\",\n    \"OriginCountry\": \"ZA\",\n    \"dayOfWeek\": 0,\n    \"DistanceKilometers\": 8823.40014044213,\n    \"timestamp\": \"2023-01-30T18:27:00\",\n    \"DestLocation\": {\n        \"lat\": \"45.505299\",\n        \"lon\": \"12.3519\"\n    },\n    \"DestAirportID\": \"VE05\",\n    \"Carrier\": \"Logstash Airways\",\n    \"Cancelled\": false,\n    \"FlightTimeMin\": 464.3894810759016,\n    \"Origin\": \"Cape Town International Airport\",\n    \"OriginLocation\": {\n        \"lat\": \"-33.96480179\",\n        \"lon\": \"18.60169983\"\n    },\n    \"DestRegion\": \"IT-34\",\n    \"OriginAirportID\": \"CPT\",\n    \"OriginRegion\": \"SE-BD\",\n    \"DestCityName\": \"Venice\",\n    \"FlightTimeHour\": 7.73982468459836,\n    \"FlightDelayMin\": 0\n}\n</code></pre> <ul> <li>Modificamos el registro anterior <pre><code>POST  /kibana_sample_data_flights/_update_by_query\n{\n  \"query\": {\n    \"match\": {\n      \"FlightNum\": \"EXEMPLE\"\n    }\n  },\n  \"script\": {\n    \"source\": \"ctx._source.Dest = params.dest\",\n    \"lang\": \"painless\",\n    \"params\": {\"dest\": \"Foo\"}\n  }\n}\n</code></pre></li> </ul> <p>https://github.com/InnocenceAllen/AENA_Info_Vuelos_V2</p>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#consulta-relacionales-en-sql","title":"Consulta relacionales en SQL","text":"<p>Tambi\u00e9n podemos realizar consultas tradiciones de bases de datos relacionales utilizando SQL</p> <ul> <li>Query Sencillo</li> </ul> <pre><code>GET _sql\n{\n\"query\": \"SELECT * FROM kibana_sample_data_flights LIMIT 10\"\n}\n</code></pre> <ul> <li>Query con un filtror</li> </ul> <pre><code>GET _sql\n{\n\"query\": \"\"\"\nSELECT FlightNum as FlightNumber,\nOriginCountry,\nOrigin,\nDestCountry as DestinationCountry,\nDest as Destination\nFROM kibana_sample_data_flights\nWHERE FlightNum = 'EXEMPLE'\n\"\"\"\n}\n</code></pre> <ul> <li>Formato de la salida tipo texto</li> </ul> <pre><code>GET _sql?format=txt\n{\n\"query\": \"\"\"\nSELECT FlightNum as FlightNumber,\n    OriginCountry,\n    Origin,\n    Dest as DestinationAirport\nFROM kibana_sample_data_flights\nWHERE FlightNum = 'EXEMPLE'\n\"\"\"\n}\n</code></pre>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#uso-de-apache-nifi-con-docker-compose-y-ejemplos-de-uso-con-mongodb","title":"Uso de Apache NiFi con docker-compose y ejemplos de uso con MongoDB","text":"<p>https://aitor-medrano.github.io/iabd2223/dataflow/04nifi1.html</p> <p>Ejemplo de uso con Twitter + Nifi</p> <p>Visualizaci\u00f3n de las plantillas introducidas con el script: </p> <pre><code>sudo curl --cacert http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200/_index_template/tweets-template?pretty\n</code></pre> <p>Para darse de alta con Twitter, lo tenemos que hacer en la secci\u00f3n de developer.twitter.com</p> <p>Una vez dado de alta, necesito:</p> <p>Consumer Key (API key): Consumer Secret (API key secret fields): Access Token: Access Token Secret field values. </p> <p>API Key wmivmNCcayIv4leWH0v8KM5J7</p> <p>API Key Secret cJ2sfQI6idv8fb3caSSz3P7unPZ5sCOoryoXPFnQfyalsvf7Qc</p> <p>Bearer Token:  AAAAAAAAAAAAAAAAAAAAAFoQsQEAAAAAQ9TlGdu0VvmZglffTnSorJTBj50%3DNxRNq7HdGKktOTAwGN4dBQkw0AlcQ9VHFQqNlS5b4tqXHHMPk9</p> <p>Access Token 1754939681395945472-A0cFmE1ffJNcIb7WeiHA96JO8UBppn</p> <p>Access Token Secret ypcHzdXJGTYPCJ9MaNuDwsz3cwJqGBmKIBVMQaOA9kJl6</p>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#demo-de-los-terremotos","title":"demo de los terremotos.","text":"<p>Siguiendo lo especificado en Openwebinars: ELK en la parte de kibana para la inserci\u00f3n de registros de terremotos.</p> <p>Pasos que sigo.</p> <p>siguiendo el blog de elastic para instalar todo elastic stack mediante docker-compose.</p> <p>Creo la carpeta y el fichero necesario y le damos permisos a cascoporro</p> <pre><code>sergio@elastic:~$ mkdir logstash_ingest_data\nsergio@elastic:~$ chmod 777 logstash_ingest_data/\nsergio@elastic:~$ mkdir templates\nsergio@elastic:~$ chmod 777 templates/\nsergio@elastic:~$ touch logstash.conf\nsergio@elastic:~$ chmod +r+w logstash.conf \n</code></pre> <p>para este ejemplo, hacer falta</p> <p>En ejemplo: /home/openweb/Documents/dataset/all_month.json  Montado   : /usr/share/logstash/ingest_data/ En real   : ./logstash_ingest_data</p> <p>Se traduce a: </p> <p>En ejemplo: /etc/logstash/templates/earthquake-template.json Montado   : /usr/share/logstash/templates/ En real   : ./templates Este no hace falta, se crea el fichero y se sube</p> <p>Para meter el template, lo hacemos directamente desde Kibana: </p> <pre><code>DELETE _index_template/earthquake\n\nPUT _index_template/earthquake\n{\n  \"index_patterns\" : \"earthquake*\",\n  \"template\" : {\n    \"settings\" : {\n      \"index.refresh_interval\" : \"20s\",\n      \"index.number_of_shards\" : 3,\n      \"index.number_of_replicas\" : 2,\n      \"index.routing.allocation.total_shards_per_node\" : 3,\n      \"index.auto_expand_replicas\": false,\n      \"index.requests.cache.enable\": true\n    },\n    \"mappings\" : {\n        \"dynamic_templates\" : [\n          {\n            \"string_fields\": {\n              \"match\": \"*\",\n              \"match_mapping_type\": \"string\",\n              \"mapping\": {\n                \"type\": \"text\"\n              }\n            }\n          }\n        ],\n        \"properties\" : {\n          \"@timestamp\": { \"type\": \"date\" },\n          \"time\": { \"type\": \"date\" },\n          \"@version\": { \"type\": \"text\", \"index\": \"false\" },\n          \"depth\": { \"type\": \"double\",\"doc_values\" : true },\n          \"mag\": { \"type\": \"double\",\"doc_values\" : true },\n          \"nst\": { \"type\": \"double\",\"doc_values\" : true },\n          \"gap\": { \"type\": \"double\",\"doc_values\" : true },\n          \"felt\": { \"type\": \"double\",\"doc_values\" : true },\n          \"cdi\": { \"type\": \"double\",\"doc_values\" : true },\n          \"horizontalError\": { \"type\": \"double\",\"doc_values\" : true },\n          \"magError\": { \"type\": \"double\",\"doc_values\" : true },\n          \"magNst\": { \"type\": \"double\",\"doc_values\" : true },\n          \"latitude\": { \"type\": \"double\",\"doc_values\" : true },\n          \"longitude\": { \"type\": \"double\",\"doc_values\" : true },\n          \"location\": { \"type\" : \"geo_point\" }\n        }\n\n    }\n  }\n}\n</code></pre> <p>Datos de terremotos:  - Web : https://earthquake.usgs.gov/earthquakes/feed/v1.0/csv.php - Acceso directo Todos terremotos \u00faltimo mes: https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_month.csv</p> <p>Guardamos el CSV y lo convertimos con el siguiente Script: </p> <p>Tal y como se puede comprobar, el formato es .csv el cual se convertir\u00e1 en JSON para una carga en Elasticsearch m\u00e1s sencilla. Para ello ser\u00e1 necesario eliminar la primera l\u00ednea donde se definen los campos y se podr\u00e1 hacer uso del script que a continuaci\u00f3n se aporta.</p> <pre><code>#!/usr/bin/env python\n\nimport os\nimport json\nimport sys\nimport urllib\nimport fileinput\nimport csv\nimport json\n\ndef convert_csv2json():\n  csvfile = open('./all_month.csv', 'r')\n  jsonfile = open('./all_month.json', 'w')\n  fieldnames = (\"time\",\"latitude\",\"longitude\",\"depth\",\"mag\",\"magType\",\"nst\",\"gap\",\"dmin\",\"rms\",\"net\",\"id\",\"updated\",\"place\",\"type\",\"horizontalError\",\"depthError\",\"magError\",\"magNst\",\"status\",\"locationSource\",\"magSource\")\n  reader = csv.DictReader( csvfile, fieldnames)\n  for row in reader:\n    json.dump(row, jsonfile)\n    jsonfile.write('\\n')\n\nif __name__ == \"__main__\":\n\n  convert_csv2json()\n</code></pre> <p>Guardamos el script en la misma carpeta que el fichero de datos por ejemplo con el nombre <code>csv2json.py</code> y lo ejecutamos: </p> <pre><code>python3 csv2json.py\n</code></pre> <p>y tenemos el JSON preparado para su ingesta.</p>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#mini-beginners-crash-course-to-elasticsearch-kibana","title":"Mini Beginner's Crash Course to Elasticsearch &amp; kibana","text":"<p>Blog y videos que esplican como montar un Beginner's guide to building a full stack app (Node.js &amp; React) with Elasticsearch </p>"},{"location":"ElasticStack/elastic_notas_.ejemplos/#examen-curso-de-elasticsearch-y-kibana-para-desarrolladores-de-openwebinar","title":"Examen <code>Curso de Elasticsearch y Kibana para desarrolladores</code> de openwebinar","text":"<ol> <li>Tienes una base de datos. \u00bfQu\u00e9 componente del Elastic Stack puedes usar para ingestar datos dentro de Elasticsearch?</li> <li>Logstash.  <code>&lt;--</code></li> <li>Beats.</li> <li>Kibana.</li> <li> <p>Elasticsearch.</p> </li> <li> <p>Si con los datos anteriores ya ingestados en Elasticsearch. \u00bfC\u00f3mo podriamos visualizarlos?</p> </li> <li>Kibana  <code>&lt;--</code></li> <li>Beats</li> <li>Elasticsearch</li> <li> <p>Logstash</p> </li> <li> <p>\u00bfSobre qu\u00e9 plataforma se apoya Elasticsearch para indexar y buscar datos?</p> </li> <li>Solr.</li> <li>No se apoya sobre ninguna plataforma.</li> <li>Apache Lucene.   <code>&lt;--</code></li> <li> <p>Google.</p> </li> <li> <p>\u00bfQu\u00e9 pasa si indexamos un documento en el un \u00edndice con un ID repetido?</p> </li> <li>Ninguna es correcta.</li> <li>Elasticsearch sobrescribe el documento. Lo actualiza.   <code>&lt;--</code></li> <li>Duplica el documento porque Elasticsearch no tiene en cuenta ID para indexar.</li> <li> <p>Devuelve un error de query porque el documento ya existe.</p> </li> <li> <p>Elasticsearch tiene la posibilidad de desplegarse por medio de docker, kubernetes o Ansible.</p> </li> <li>Falso</li> <li> <p>Verdadero  <code>&lt;--</code></p> </li> <li> <p>\u00bfC\u00f3mo puede Elasticsearch repartir las peticiones a los nodos del cl\u00faster?</p> </li> <li>Definiendo roles de datos, master e ingestar a los nodos.    <code>&lt;--</code></li> <li>Todo lo anterior es correcto.</li> <li>Exponiendo solo el puerto de un nodo.</li> <li> <p>Usando un reverse proxy.</p> </li> <li> <p>Si estamos teniendo problemas con el cl\u00faster de Elasticsearch, \u00bfD\u00f3nde podemos encontrar el problema o la incidencia que est\u00e1 reportando?</p> </li> <li>Directorio de configuraci\u00f3n.</li> <li>Directorio modules.</li> <li>Directorio de logs.   <code>&lt;--</code></li> <li> <p>Directorio de datos .</p> </li> <li> <p>\u00bfCu\u00e1les eran los dos objetivos principales de Elasticsearch?</p> </li> <li>Definir procesos y procedimientos.</li> <li>B\u00fasquedas r\u00e1pidas y sencillas</li> <li>Mejorar todo.</li> <li> <p>Ser un sistema distribuido desde cero y que sea de f\u00e1cil uso por otros lenguajes de programaci\u00f3n como python, php, ruby\u2026  <code>&lt;--</code></p> </li> <li> <p>Si queremos ingestar una cantidad grande de datos \u00bfQu\u00e9 servicio vamos a usar de Elasticsearch?</p> </li> <li>POST con _update</li> <li>Cualquiera de las anteriores ser\u00eda valida</li> <li>POST simple</li> <li> <p>POST con _bulk    <code>&lt;--</code></p> </li> <li> <p>Verdadero o falso. Si un \u00edndice no existe cuando indexamos un documento. Elasticsearch lo genera autom\u00e1ticamente.</p> </li> <li>Verdadero</li> <li> <p>Falso</p> </li> <li> <p>Elasticsearch solo se usa como buscador de datos para p\u00e1ginas web...</p> </li> <li>Falso</li> <li> <p>Verdadero</p> </li> <li> <p>\u00bfC\u00f3mo se puede comunicar una aplicaci\u00f3n con Elasticsearch?</p> </li> <li>Comunicaci\u00f3n Transport por el puerto 9300</li> <li>Todo lo anterior es correcto</li> <li>Le env\u00eda eventos XML</li> <li> <p>Mediante comunicaci\u00f3n HTTP por el puerto 9200</p> </li> <li> <p>\u00bfCu\u00e1l es el n\u00famero m\u00ednimo de hits que devuelve una query por defecto?</p> </li> <li>10</li> <li>1000</li> <li>100</li> <li> <p>Todos los documentos del \u00edndice.</p> </li> <li> <p>Un cl\u00faster de Easticsearch es la colecci\u00f3n de nodos de Elasticsearch...</p> </li> <li>Verdadero</li> <li> <p>Falso</p> </li> <li> <p>Si queremos monitorizar los datos estad\u00edsticos de unos servidores. \u00bfC\u00f3mo podr\u00edamos recolectar los datos e ingestarlos en Elasticsearch?</p> </li> <li>Logstash</li> <li>Beats y Logstash</li> <li>Logstash y Kibana</li> <li> <p>Elasticsearch y Kibana</p> </li> <li> <p>\u00bfC\u00f3mo se comunica un nodo del cl\u00faster con otro?</p> </li> <li>Le env\u00eda eventos XML.</li> <li>Mediante comunicaci\u00f3n HTTP por el puerto 9200.</li> <li>Comunicaci\u00f3n Transport por el puerto 9300.</li> <li> <p>Todo lo anterior es correcto.</p> </li> <li> <p>\u00bfD\u00f3nde puedo configurar la memoria m\u00e1xima y m\u00ednima que va a consumir mi servicio de Elasticsearch?</p> </li> <li>Desde elasticsearch.yml</li> <li>Dentro de bin/elasticsearch</li> <li>Desde Kibana</li> <li> <p>jvm.options</p> </li> <li> <p>\u00bfC\u00f3mo distribuye Elasticsearch los documentos?</p> </li> <li>Ninguna es correcta.</li> <li>Los almacena en ficheros virtuales .db</li> <li>Mediante Shards dentro del nodo master.</li> <li> <p>En los nodos mediante Shards, que es la unidad m\u00ednima en la que un \u00edndice se divide por los nodos del cl\u00faster.</p> </li> <li> <p>Como podemos comprobar la primera vez que el estado del cl\u00faster est\u00e1 correctamente levantado y accesible. (Multirrespuesta). </p> </li> <li>Desplegando Kibana y ver si reconoce el servicio de Elasticsearch</li> <li>Mirando el fichero de logs</li> <li>Lanzando un Curl contra https://localhost:9200</li> <li> <p>Poniendo la direcci\u00f3n http://localhost:9200 en un navegador</p> </li> <li> <p>\u00bfD\u00f3nde puedo configurar el puerto por el que expongo el servicio Rest de Elasticsearch?</p> </li> <li>Desde Kibana.</li> <li>jvm.options</li> <li>Dentro de bin/elasticsearch</li> <li>Desde elasticsearch.yml</li> </ol>"},{"location":"MongoDB/1_mongodb%20Introducci%C3%B3n./","title":"1. Introducci\u00f3n a MongoDB","text":"<p>En el mundo del Big Data, donde la cantidad de datos generados y procesados contin\u00faa creciendo exponencialmente, la elecci\u00f3n de la tecnolog\u00eda adecuada para almacenar, gestionar y analizar estos datos es fundamental. MongoDB, una base de datos NoSQL de c\u00f3digo abierto y orientada a documentos, ha emergido como una soluci\u00f3n poderosa y vers\u00e1til en este panorama en constante evoluci\u00f3n.</p> <p>As\u00ed fue c\u00f3mo naci\u00f3 MongoDB</p> <p>MongoDB fue fundada en 2007 por Dwight Merriman, Eliot Horowitz y Kevin Ryan, el equipo detr\u00e1s de DoubleClick.</p> <p>En la empresa de publicidad en Internet DoubleClick (ahora propiedad de Google), el equipo desarroll\u00f3 y utiliz\u00f3 numerosos almacenes de datos personalizados para solucionar las carencias de las bases de datos existentes. El negocio serv\u00eda 400 000 anuncios por segundo, pero a menudo ten\u00eda problemas de escalabilidad y agilidad. Frustrado, el equipo hall\u00f3 inspiraci\u00f3n para crear una base de datos que abordara los desaf\u00edos a los que se enfrentaban en DoubleClick.</p> <p>MongoDB es una de las bases de datos NoSQL m\u00e1s conocidas. Sigue un modelo de datos documental,</p> <p>Curiosidades</p> <p>Como curiosidad, su nombre viene de la palabra inglesa humongous, que significa gigantesco/enorme.</p> <p>En este apartado exploraremos c\u00f3mo MongoDB se integra perfectamente en los entornos de Big Data, ofreciendo capacidades escalables, flexibles y de alto rendimiento para abordar una variedad de desaf\u00edos y escenarios de datos a gran escala. Desde su modelo de datos flexible hasta su capacidad para manejar grandes vol\u00famenes de datos en tiempo real, MongoDB se ha convertido en una herramienta indispensable para empresas y organizaciones que buscan aprovechar al m\u00e1ximo sus datos en el mundo del Big Data.</p> <p>Fuentes utilizadas</p> <p>Las principales fuentes consultadas para la realizaci\u00f3n de esta secci\u00f3n han sido:</p> <ul> <li>Manual oficial de MongoDB</li> <li>Aitor Medrano. Cursos Inteligencia Artificial y Big Data. MongoDB</li> <li>Algunos ejemplos de uso</li> <li>Manipulaci\u00f3n de datos en MongoDB mediante Aggregation Pipeline</li> </ul>"},{"location":"MongoDB/2_mongodb%20MongoDB/","title":"2. MongoDB","text":"<p>MongoDB es una base de datos NoSQL, de c\u00f3digo abierto y orientada a documentos. En lugar de almacenar datos en tablas, como lo hace una base de datos relacional, MongoDB almacena datos en documentos similares a JSON con un formato llamado BSON (Binary JSON). </p> <p>BSON extiende el formato JSON para incluir tipos de datos adicionales como fechas y binarios, lo que lo hace m\u00e1s adecuado para representar datos complejos.</p> <p>MongoDB destaca porque:</p> <ul> <li>Soporta esquemas din\u00e1micos: diferentes documentos de una misma colecci\u00f3n pueden tener atributos diferentes.</li> <li>Aunque inicialmente ten\u00eda un soporte limitado de joins, desde la versi\u00f3n 5.2 se pueden realizar incluso entre colecciones particionadas. Actualmente MongoDB va por la versi\u00f3n. 8.0.</li> <li>Soporte de transacciones s\u00f3lo a nivel de aplicaci\u00f3n. Lo que en un RDMS puede suponer m\u00faltiples operaciones, con MongoDB se puede hacer en una sola operaci\u00f3n al insertar/actualizar todo un documento de una sola vez, pero si queremos crear una transacci\u00f3n entre dos documentos, la gesti\u00f3n la debe realizar el driver.</li> </ul> <p>MongoDB se utiliza ampliamente en una variedad de aplicaciones, incluidas aquellas con grandes vol\u00famenes de datos, cargas de trabajo de alta velocidad y requisitos de flexibilidad de esquema. Es especialmente popular en aplicaciones web y m\u00f3viles, as\u00ed como en entornos de Big Data y an\u00e1lisis en tiempo real.</p> <p>Ejemplos muestran la versatilidad de MongoDB en diferentes sectores podr\u00edan ser:</p> <ul> <li> <p>eBay: Utiliza MongoDB para gestionar su enorme cat\u00e1logo de productos y facilitar b\u00fasquedas r\u00e1pidas y eficientes.</p> </li> <li> <p>Uber: Emplea MongoDB para almacenar datos de viajes y usuarios, permitiendo un an\u00e1lisis en tiempo real y mejorando la experiencia del cliente.</p> </li> <li> <p>LinkedIn: Utiliza MongoDB para manejar grandes vol\u00famenes de datos de usuarios y conexiones, optimizando la b\u00fasqueda y la interacci\u00f3n en la plataforma.</p> </li> <li> <p>Foursquare: Esta aplicaci\u00f3n de recomendaciones de lugares utiliza MongoDB para gestionar datos de usuarios y localizaciones, permitiendo un acceso r\u00e1pido a informaci\u00f3n geoespacial.</p> </li> <li> <p>The Guardian: El medio de comunicaci\u00f3n utiliza MongoDB para gestionar su contenido digital, facilitando la publicaci\u00f3n y el acceso a art\u00edculos y multimedia de manera eficiente.</p> </li> </ul>"},{"location":"MongoDB/2_mongodb%20MongoDB/#caracteristicas-de-mongodb","title":"Caracter\u00edsticas de MongoDB","text":"<p>Si tuvi\u00e9ramos que resumir a una la principal caracter\u00edstica a destacar de MongoDB, sin duda esta ser\u00eda la velocidad, que alcanza un balance perfecto entre rendimiento y funcionalidad gracias a su sistema de consulta de contenidos. Pero sus caracter\u00edsticas principales no se limitan solo a esto, MongoDB cuenta, adem\u00e1s, con otras que lo posicionan posiblemente como la base de datos NoSQL m\u00e1s popular para muchos desarrolladores.</p> <p>Caracter\u00edsticas principales:</p> <ul> <li>Consultas ad hoc. Con MongoDb podemos realizar todo tipo de consultas. Podemos hacer b\u00fasqueda por campos, consultas de rangos y expresiones regulares. Adem\u00e1s, estas consultas pueden devolver un campo espec\u00edfico del documento, pero tambi\u00e9n puede ser una funci\u00f3n JavaScript definida por el usuario.</li> <li>Indexaci\u00f3n. El concepto de \u00edndices en MongoDB es similar al empleado en bases de datos relacionales, con la diferencia de que cualquier campo documentado puede ser indexado y a\u00f1adir m\u00faltiples \u00edndices secundarios.</li> <li>Replicaci\u00f3n. Del mismo modo, la replicaci\u00f3n es un proceso b\u00e1sico en la gesti\u00f3n de bases de datos. MongoDB soporta el tipo de replicaci\u00f3n primario-secundario. De este modo, mientras podemos realizar consultas con el primario, el secundario act\u00faa como r\u00e9plica de datos en solo lectura a modo copia de seguridad con la particularidad de que los nodos secundarios tienen la habilidad de poder elegir un nuevo primario en caso de que el primario actual deje de responder.</li> <li>Balanceo de carga. Resulta muy interesante c\u00f3mo MongoDB puede escalar la carga de trabajo. MongoDB tiene la capacidad de ejecutarse de manera simult\u00e1nea en m\u00faltiples servidores, ofreciendo un balanceo de carga o servicio de replicaci\u00f3n de datos, de modo que podemos mantener el sistema funcionando en caso de un fallo del hardware.</li> <li>Almacenamiento de archivos. Aprovechando la capacidad de MongoDB para el balanceo de carga y la replicaci\u00f3n de datos, Mongo puede ser utilizado tambi\u00e9n como un sistema de archivos. Esta funcionalidad, llamada GridFS e incluida en la distribuci\u00f3n oficial, permite manipular archivos y contenido.</li> <li>Ejecuci\u00f3n de JavaScript del lado del servidor. MongoDB tiene la capacidad de realizar consultas utilizando JavaScript, haciendo que estas sean enviadas directamente a la base de datos para ser ejecutadas.</li> </ul>"},{"location":"MongoDB/2_mongodb%20MongoDB/#conceptos-basicos","title":"Conceptos b\u00e1sicos","text":"<p>Hay una serie de conceptos que conviene conocer antes de entrar en detalle:</p> <ul> <li>MongoDB tienen el mismo concepto de base de datos que un RDMS. Dentro de una instancia de MongoDB podemos tener 0 o m\u00e1s bases de datos, actuando cada una como un contenedor de alto nivel.</li> <li>Una base de datos tendr\u00e1 0 o m\u00e1s colecciones. Una colecci\u00f3n es muy similar a lo que entendemos como tabla dentro de un RDMS. MongoDB ofrece diferentes tipos de colecciones, desde las normales cuyo tama\u00f1o crece conforme lo hace el n\u00famero de documentos, como las colecciones capped, las cuales tienen un tama\u00f1o predefinido y que pueden contener una cierta cantidad de informaci\u00f3n que se sustituir\u00e1 por nueva cuando se llene.</li> <li>Las colecciones contienen 0 o m\u00e1s documentos, por lo que es similar a una fila o registro de un RDMS.</li> <li>Cada documento contiene 0 o m\u00e1s atributos, compuestos de parejas clave/valor. Cada uno de estos documentos no sigue ning\u00fan esquema, por lo que dos documentos de una misma colecci\u00f3n pueden contener todos los atributos diferentes entre s\u00ed.</li> </ul> <p>As\u00ed pues, tenemos que una base de datos va a contener varias colecciones, donde cada colecci\u00f3n contendr\u00e1 un conjunto de documentos. Podemos hacer una correspondencia r\u00e1pida entre bases de datos Relacionales y NoSQL:</p> <p>Adem\u00e1s, MongoDB soporta \u00edndices, igual que cualquier RDMS, para acelerar la b\u00fasqueda de datos. Al realizar cualquier consulta, se devuelve un cursor, con el cual podemos hacer cosas tales como contar, ordenar, limitar o saltar documentos.</p>"},{"location":"MongoDB/2_mongodb%20MongoDB/#bson","title":"BSON","text":"<p>MongoDB almacena los documentos mediante BSON (Binary JSON).</p> <p>Repasemos el concepto de JSON: JavaScript Object Notation - Formato de texto sencillo para el intercambio de datos. - Subconjunto de la notaci\u00f3n literal de objetos de JavaScript. - Alternativa a XML como lenguaje de intercambio de datos. Mucho m\u00e1s sencillo de leer y escribir. - Uso extendido en bases de datos noSQL, entre ellas JSON: JavaScript Object Notation - Ampliamente soportado por multitud de lenguajes de programaci\u00f3n. - Un objeto JSON est\u00e1 formado por uno o varios pares string: value (cadena:valor). - Soporta diferentes tipos de datos como cadenas de texto, n\u00fameros, fecha, hora, valores nulos y booleanos.  </p> <p>Mediante JavaScript podemos crear objetos que se representan con JSON. Internamente, MongoDB almacena los documentos mediante BSON (Binary JSON). Podemos consultar la especificaci\u00f3n en la web oficial de BSON </p> <p>BSON representa un superset de JSON ya que:</p> <ul> <li>Permite almacenar datos en binario</li> <li>Incluye un conjunto de tipos de datos no incluidos en JSON, como pueden ser ObjectId, Date o BinData.</li> </ul> <p>Podemos consultar todos los tipos que soporta un objeto BSON en http://docs.mongodb.org/manual/reference/bson-types/</p> <p>Un ejemplo de un objeto BSON podr\u00eda ser:</p> <pre><code>var yo = {\n  nombre: \"Aitor\",\n  apellidos: \"Medrano\",\n  fnac: new Date(\"Oct 3, 1977\"),\n  hobbies: [\"programaci\u00f3n\", \"videojuegos\", \"baloncesto\"],\n  casado: true,\n  hijos: 2,\n  contacto: {\n    twitter: \"@aitormedrano\",\n    email: \"a.medrano@edu.gva.es\"\n  },\n  fechaCreacion: new Timestamp()\n}\n</code></pre> <p>Los documentos BSON tienen las siguientes restricciones:</p> <ul> <li>No pueden tener un tama\u00f1o superior a 16 MB.</li> <li>El atributo <code>_id</code> queda reservado para la clave primaria.</li> <li>Desde MongoDB 5.0 los nombres de los campos pueden empezar por <code>$</code> y/o contener el <code>.</code>, aunque en la medida de lo posible, es recomendable evitar su uso.</li> </ul> <p>Adem\u00e1s MongoDB:</p> <ul> <li>No asegura que el orden de los campos se respete.</li> <li>Es sensible a los tipos de los datos</li> <li>Es sensible a las may\u00fasculas.</li> </ul> <p>Por lo que estos documentos son distintos:</p> <pre><code>{\"edad\": \"18\"}\n{\"edad\": 18}\n{\"Edad\": 18}\n</code></pre> <p>Si queremos validar si un documento JSON es v\u00e1lido, podemos usar la web JSONLint Validator and Formatter. Hemos de tener en cuenta que s\u00f3lo valida JSON y no BSON, por tanto nos dar\u00e1 errores en los tipos de datos propios de BSON.</p>"},{"location":"MongoDB/2_mongodb%20MongoDB/#bases-de-datos-relacionales-vs-mongodb","title":"Bases de datos Relacionales vs MongoDB","text":"<p>Aqu\u00ed tenemos un esquema de los elementos de una base de datos representada tanto por un sistema relacional tradicional, frente a la misma estructura con una base de datos en MongoDB</p> <p>Primero la base de datos relacional</p> <p>y ahora la misma representaci\u00f3n en MongoDB</p>"},{"location":"MongoDB/3_mongodb%20ModeladoDatos/","title":"3. Dise\u00f1o de modelado de bases de datos. schemaless","text":""},{"location":"MongoDB/3_mongodb%20ModeladoDatos/#concepto-de-schemaless","title":"Concepto de schemaless","text":"<p>El t\u00e9rmino schemaless (sin esquema) se refiere a la capacidad de una base de datos para manejar datos sin una estructura fija predefinida. En el contexto de MongoDB, esto significa que no est\u00e1s obligado a definir un esquema estricto para tus datos antes de comenzar a almacenarlos. En lugar de eso, los datos se almacenan en documentos BSON (Binary JSON), y estos documentos pueden tener una estructura flexible y pueden variar de un documento a otro dentro de la misma colecci\u00f3n.</p> <p>A continuaci\u00f3n, profundicemos en algunos aspectos clave del concepto de schemaless en MongoDB:</p> <ul> <li> <p>Flexibilidad de estructura: Los documentos en MongoDB pueden contener diferentes campos y tipos de datos. No es necesario que todos los documentos en una colecci\u00f3n tengan la misma estructura. Esto permite adaptarse f\u00e1cilmente a cambios en los requisitos de la aplicaci\u00f3n sin tener que modificar un esquema centralizado.</p> </li> <li> <p>Adici\u00f3n din\u00e1mica de campos: En MongoDB, puedes agregar campos a un documento en cualquier momento sin afectar a otros documentos en la misma colecci\u00f3n. Esto significa que puedes manejar datos evolutivos donde la estructura de los documentos puede cambiar con el tiempo.</p> </li> <li> <p>Consulta sin restricciones: Dado que no hay un esquema fijo que imponga restricciones sobre la estructura de los datos, las consultas en MongoDB pueden ser m\u00e1s flexibles. Puedes realizar consultas sobre cualquier campo en cualquier documento, incluso si esos campos no est\u00e1n presentes en todos los documentos de la colecci\u00f3n.</p> </li> <li> <p>Evita la migraci\u00f3n de esquemas: En las bases de datos tradicionales con esquemas fijos, los cambios en el esquema requieren migraciones de datos costosas. Con MongoDB, puedes evitar este problema ya que no hay un esquema centralizado que necesite ser modificado.</p> </li> <li> <p>Agilidad en el desarrollo: La falta de un esquema fijo permite una mayor agilidad en el desarrollo de aplicaciones, ya que puedes iterar r\u00e1pidamente y ajustar el modelo de datos seg\u00fan sea necesario sin tener que preocuparte por actualizar un esquema centralizado.</p> </li> <li> <p>Rendimiento: La flexibilidad del modelo de datos schemaless puede traducirse en un mejor rendimiento en ciertos casos, ya que elimina la necesidad de realizar un join de datos dispersos en m\u00faltiples tablas, como suele ocurrir en bases de datos relacionales.</p> </li> </ul> <p>A pesar de las ventajas de un modelo de datos schemaless, es importante tener en cuenta que esto tambi\u00e9n puede presentar desaf\u00edos, especialmente en t\u00e9rminos de mantener la coherencia y la integridad de los datos. Por lo tanto, es crucial dise\u00f1ar cuidadosamente la base de datos y utilizar pr\u00e1cticas como la validaci\u00f3n de datos y la indexaci\u00f3n adecuada para garantizar un rendimiento \u00f3ptimo y la integridad de los datos en aplicaciones MongoDB.</p> <p>Observar la diferencia que tenemos en el dise\u00f1o de la bases de datos relacionales con las NoSQL como lo vimos anteriormente</p>"},{"location":"MongoDB/3_mongodb%20ModeladoDatos/#documentos-embebidos","title":"Documentos embebidos","text":"<p>El concepto de embebido hace referencia a guardar una \u2018cosa\u2019 dentro de otra \u2018cosa\u2019. En este caso, guardar un documento JSON dentro de otro como valor de una de sus propiedades.</p> <p>Por ejemplo, supongamos que tenemos una colecci\u00f3n que guarda datos sobre cursos que se est\u00e1n impartiendo, en una base de datos relacional ser\u00edan dos tablas, pero en MongoDB lo hacemos en una \u00fanica colecci\u00f3n:</p> <pre><code>{\n    id: 1,\n    referencia: 'C0001',\n    nombre: 'Curso de especializaci\u00f3n de Inteligencia Artificial y Big Data',\n    fechaInicio: new Date(\"2024-10-01\"),\n    activo: true,\n    asignaturas: [\n        {\n            codAsig: 101,\n            nombre: 'Sistemas de Big Data',\n            horasSemana: 3,\n            profesores: [\n                {\n                    codProf: 302,\n                    nombre: 'Sergio Rey',\n                    rol: 'Profesor'\n                },\n                {\n                    codProf: 901,\n                    nombre: 'Jorge Soro',\n                    rol: 'Especialista'\n                }\n            ]\n        },\n        {\n            codAsig: 102,\n            nombre: 'Big Data Aplicado',\n            horasSemana: 3,\n            profesores: [\n                {\n                    codProf: 301,\n                    nombre: 'Nacho Pach\u00e9s',\n                    rol: 'Profesor'\n                },\n                {\n                    codProf: 901,\n                    nombre: 'Jorge Soro',\n                    rol: 'Especialista'\n                }\n            ]\n        }\n    ],\n    alumnos: []\n}\n</code></pre>"},{"location":"MongoDB/3_mongodb%20ModeladoDatos/#documentos-referenciados","title":"Documentos referenciados","text":"<p>El concepto referenciado  a diferencia de los documentos embebidos, en un documento JSON se guarda solo el valor de una o varias propiedades, en lugar del documento completo.</p> <p>Normalmente, se guarda el valor de una propiedad que identifica un\u00edvocamente, al documento referenciado: por ejemplo en el caso anterior podr\u00edamos tener: </p> <pre><code>{\n    id: 1,\n    referencia: 'C0001',\n    nombre: 'Curso de especializaci\u00f3n de Inteligencia Artificial y Big Data',\n    fechaInicio: new Date(\"2023-10-01\"),\n    activo: true,\n    asignaturas: [ 101, 102],\n    alumnos: []\n}\n</code></pre> <p>En este caso, las asignaturas se refer\u00e9ncian mediante el c\u00f3digo de identificaci\u00f3n, en lugar de guardar en el JSON todos los datos. De esta forma es similar al adoptado por las bases de datos relacionales.</p> <p>Esto plantea varias ventajas y desventajas:</p> <p>As\u00ed pues en los documentos embebidos tenemos: - Ventajas   - Al recuperar un curso, podemos traernos toda la informaci\u00f3n relacionada en otras colecciones (p.e. asignaturas) - Desventajas:   - Al hacer consultas sobre la colecci\u00f3n externa (cursos), si uno de los criterios afecta a la informaci\u00f3n de los documentos embebidos, el tiempo para realizar dicha consulta se ver\u00e1 incrementando.   - Las actualizaciones ser\u00e1n m\u00e1s costosas si alguna de las propiedades a actualizar pertenece al documento embebido.</p> <p>Mientras que en los documentos referenciados - Ventajas   - Las consultas sobre la colecci\u00f3n principal y las relacionadas se ejecutar\u00e1 m\u00e1s r\u00e1pido.   - Al actualizar informaci\u00f3n relacionada, se hace directamente en sus documentos sin tener que revisar la colecci\u00f3n externa. - Desventajas:   - Al recuperar un curso, habr\u00e1 que consultar los identificadores que relacionan a los documentos en otras colecciones y hacer consultas adicionales para obtener la informaci\u00f3n relacionada.</p> <p>Entonces \u00bfQu\u00e9 dise\u00f1o elegir?</p> <p>Depender\u00e1 de: - C\u00f3mo se quiere almacenar la informaci\u00f3n. - la naturaleza y el contexto de las aplicaciones que vayan a consumir la informaci\u00f3n. - Las preferencias de roles como arquitectos de software y de bases de datos teniendo en cuenta factores futuros como la Escalabilidad en cuanto a volumen de datos, usuarios / aplicaciones y sus formas de acceder a la informaci\u00f3n, etc.</p>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/","title":"4. Instalaci\u00f3n","text":""},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#instalacion-de-mongodb","title":"Instalaci\u00f3n de MongoDB","text":"<p>En la actualidad, MongoDB se como base de datos en tres productos diferentes m\u00e1s un conglomerado de servicios y herramientas que complementas a la base de datos.</p> <ol> <li>Mongo Atlas, como plataforma cloud, con una opci\u00f3n gratuita mediante un cluster de 512MB.</li> <li>MongoDB Enterprise Advanced, versi\u00f3n de pago con soporte, herramientas avanzadas de monitorizaci\u00f3n y seguridad, y administraci\u00f3n automatizada.</li> <li>MongoDB Community Edition, versi\u00f3n gratuita para trabajar on-premise, con versiones para Windows, MacOS y Linux. Nosotros de momento trabajaremos con esta versi\u00f3n</li> </ol>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#instalacion-en-sistemas-windows","title":"Instalaci\u00f3n en sistemas Windows","text":"<p>Para instalar MongoDB en un equipo Microsoft Windows, procedemos como siempre.</p> <p>Vamos a la web de MongoDB en su secci\u00f3n de descargas (versi\u00f3n Community)</p> <p>Descargamos e instalamos.</p> <p>Nota:. Al realizar esta instalaci\u00f3n al mismo tiempo instalamos MongoDB Compass que veremos m\u00e1s adelante. Se trata de una interface de acceso s MongoDB</p>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#instalacion-en-ubuntu","title":"Instalaci\u00f3n en Ubuntu","text":"<p>Para la instalaci\u00f3n de MongoDB Community Edition en un sistema Ubuntu vamos a proceder tal y como se espedifica en la propia web de mongodb. Install MongoDB Community Edition </p> <p>Realizaremos los siguientes pasos:</p> <pre><code># Requisitos previos\nsudo apt-get install gnupg curl                   # Requisitos previos\n\n# Importar claves p\u00fablicas GPG de MongoDB\ncurl -fsSL https://www.mongodb.org/static/pgp/server-8.0.asc | \\\n   sudo gpg -o /usr/share/keyrings/mongodb-server-8.0.gpg \\\n   --dearmor\n\n# a\u00f1adir las fuentes\necho \"deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-8.0.gpg ] https://repo.mongodb.org/apt/ubuntu noble/mongodb-org/8.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-8.0.list\n\n# recargar paquetes\nsudo apt-get update\n\n# e instalar\nsudo apt-get install -y mongodb-org\n</code></pre> <p>Con esto ya tenemos MongoDB instalado en nuestro sistema.</p> <p>Ahora nos falta ponerlo en marcha, para ello habilitamos e iniciamos el servicio</p> <pre><code># recargamos los nuevos servicios\nsudo systemctl daemon-reload\n\n# Habilitamos el servicios (esto es opcional, solo si queremos que se inicie al arrancar el equipo)\nsudo systemctl enable mongod\n\n# Iniciamos el servicio\nsudo systemctl start mongod\n\n# Comprobamos que el servicio se ha iniciado correctamente\nsudo systemctl status mongod\n</code></pre> <p>Mediante el siguiente comando tambi\u00e9n verificamos que esta activa y su versi\u00f3n. </p> <pre><code>mongod --version                                  # Comprobamos la versi\u00f3n\n</code></pre> <p>Nota: MongoDB tambi\u00e9n lo podemos instalar descargando el paquete .deb desde la web de MongoDB, pero suele dar mas problemas que con la instalaci\u00f3n presentada</p>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#instalacion-mongodb-en-los-contenedores-de-proxmox-de-clase","title":"Instalaci\u00f3n MongoDB en los Contenedores de Proxmox de clase","text":"<p>Para poder utilizar MongoDB en los contenedores, necesitamos realizar las siguientes acciones.</p> <ul> <li>En primer lugar, es adecuado asignar una IP est\u00e1tica a cada uno de los contenedores, o al menos conocer la IP del contenedor para poder conectar con el. </li> <li>En segundo lugar, instalamos Mongo*DB siguiendo los mismos pasos indicados anteriormente para su instalaci\u00f3n en Ubuntu. Podemos referirnos directamente a las indicaciones en la propia web de MongoDB</li> <li>Para finalizar, necesitamos configurar MongoDB para permitir el acceso desde un equipo externo. Esto tambi\u00e9n se debe hacer en la instalaci\u00f3n en Ubuntu si queremos acceder desde otro equipo.</li> </ul> <p>Para ello, editamos el fichero de configuraci\u00f3n <code>/etc/mongod.conf</code> </p> <pre><code>sudo nano /etc/mongod.conf\n</code></pre> <p>Especificamos la IP de nuestro equipo en el apartado correspondiente (<code>net</code>). En el ejemplo siguiente la IP del contenedor o servidor MongoDB es la 10.20.90.150</p> <pre><code># network interfaces\nnet:\n  port: 27017\n  bindIp: 127.0.0.1,10.20.90.150\n</code></pre> <ul> <li>Reiniciamos el servicio</li> </ul> <pre><code>sudo systemctl restart mongod\n</code></pre>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#otras-instalaciones-posibles","title":"Otras instalaciones posibles","text":"<p>En vez de instalarlo como un servicio en nuestra m\u00e1quina, a d\u00eda de hoy, es mucho m\u00e1s c\u00f3modo hacer uso de contenedores Docker o utilizar una soluci\u00f3n cloud, aunque nosotros por simplicidad, de momento, realizaremos una instalaci\u00f3n tradicional.</p> <pre><code>docker run --name mongodb -d -p 27017:27017 -v mongodb_data:/data/db mongo\n</code></pre>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#probando-la-instalacion","title":"Probando la instalaci\u00f3n","text":"<p>Independientemente de nuestro sistema operativo, por defecto, el servicio o demonio (daemon) se lanza sobre el puerto 27017. Una vez instalado, si accedemos a http://localhost:27017 podremos ver que nos indica c\u00f3mo estamos intentando acceder mediante HTTP a MongoDB mediante el puerto reservado al driver nativo.</p>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#mongo-atlas","title":"Mongo Atlas","text":"<p>Si por el motivo que sea no deseamos instalar MongoDB, si no que queremos utilizar su versi\u00f3n cloud, tenemos Mongo Atlas</p> <p>La versi\u00f3n de Mongo Atlas nos ofrece de manera gratuita un cluster compartido de servidores con 3 nodos y 512 MB para datos. Si queremos una soluci\u00f3n serverless o un servidor dedicado, ya tendremos que pasar por caja.</p> <p>Obviamente para hacer uso de esta versi\u00f3n, necesitas registrarte en la web de MongoDB</p>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#herramientas-visuales-para-interactuar-con-mongodb","title":"Herramientas visuales para interactuar con MongoDB","text":"<p>Podemos interactuar con MongoDB con su propia consola MongoDB shell  que nos ofrece la base de datos, pero para interactuar de una forma m\u00e1s flexible e intuitiva existen herramientas visuales que nos facilitan el trabajo diario con MongoDB</p>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#mongodb-compass","title":"MongoDB Compass","text":"<p>Una de ellas es MongoDB Compass, que facilita la exploraci\u00f3n y manipulaci\u00f3n de los datos. De una manera flexible e intuitiva, Compass ofrece visualizaciones detalladas de los esquemas, m\u00e9tricas de rendimiento en tiempo real as\u00ed como herramientas para la creaci\u00f3n de consultas.</p> <p>Existen tres versiones de Compass, una completa con todas las caracter\u00edsticas, una de s\u00f3lo lectura sin posibilidad de insertar, modificar o eliminar datos (perfecta para anal\u00edtica de datos) y una \u00faltima versi\u00f3n isolated que solo permite la conexi\u00f3n a una instancia local.</p> <p>Enlace a la documentaci\u00f3n oficial de MongoDB Compass: What is MongoDB Compass?</p>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#instalacion","title":"Instalaci\u00f3n","text":"<p>Siguiendo los pasos ofrecidos por la propia web de MongoDB, para la instalaci\u00f3n de MongoDB Compass en Ubuntu seguimos los siguientes pasos:</p> <pre><code># Download *MongoDB* Compass\nwget https://downloads.mongodb.com/compass/mongodb-compass_1.40.4_amd64.deb\n\n# Install *MongoDB* Compass\nsudo dpkg -i mongodb-compass_1.40.4_amd64.deb\n\n# Start *MongoDB* Compass\nmongodb-compass\n</code></pre> <p>Si hacemos caso a lo que nos dicen en la gu\u00eda de instalaci\u00f3n proporcionada por MongoDB, directamente instalamos la \u00faltima versi\u00f3n estable.</p>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#trabajando-con-mongodb-compass","title":"Trabajando con MongoDB Compass","text":"<p>Al iniciar la aplicaci\u00f3n, la primera vez nos ofrece conectarnos a la base de datos local. Tambi\u00e9n nos podemos conectar a una base de datos remota e incluso a Mongo Atlas, que como se coment\u00f3 es la base de datos que ofrece MongoDB en la nube.</p> <p>Una vez conectados a la base de datos, vemos todas las bases de datos exitentes. En la parte inferior tenemos una consola donde podemos actuar de la misma forma que lo hicimos anteriormente.</p> <p>Dentro de una base de datos, podemos acceder a las colecciones, listar los documentos, y realizar todo tipo de operaciones sobre los mismos:</p> <p>As\u00ed como operaciones espec\u00edficas sobre documentos en concreto. Si nos colocamos con el rat\u00f3n sobre un documento aparecen cuatro opciones, para editar, copiar,, duplicar y borrar el documento. Haciendo doble click, tambi\u00e9n lo editamos.</p> <p>Tenemos varias opciones sobre la base de datos, incluso podemos hacer consultas.</p> <p>En la imagen: 1. Dentro de una colecci\u00f3n, seleccionamos la pesta\u00f1a de <code>schema</code> 2. Introducimos el filtro a buscar 3. Obtenemos el detalle de los datos de los documentos obtenidos 4. Tenemos un hist\u00f3rico de todas las b\u00fasquedas realizadas  </p>"},{"location":"MongoDB/4_mongodb%20Instalaci%C3%B3n/#mongodb-for-vscode","title":"MongoDB for VSCode","text":"<p>Tambi\u00e9n podemos utilizar la extensi\u00f3n que lleva VSCode para trabajar con MongoDB.</p> <p>Para su instalaci\u00f3n</p> <p>Si no disponemos de VSCode: - podemos instalarlo siguiendo los pasos de la propia web de Microsoft: Visual Studio Code on Linux - si tenemos la versi\u00f3n completa de Ubuntu, la podemos instalar desde el gestor de aplicaciones:</p> <p>Una vez instalado VSCode, instalamos la extensi\u00f3n de MongoDB for VS Code, Aqu\u00ed seguimos los pasos de la web oficial donde tenemos c\u00f3mo instalar y configurar la conexi\u00f3n: VSCode: Working with MongoDB. Para la conexi\u00f3n, pulsamos sobre el bot\u00f3n de Advanced y la conexi\u00f3n es sencilla</p> <p>Una vez conectados, podremos recorrer las colecciones con los datos as\u00ed como utilizar un playground para interactuar de manera similar al shell:</p> <p>Realmente, esta extensi\u00f3n este pensada para trabajar con opciones avanzadas, como crear \u00edndices, generar c\u00f3digo en lenguajes como javascript, python o cualquier otro para realizar todo tipo de operaciones en MongoDB, o crear variables con datos y estos utilizarlos en nuestras operaciones. Para m\u00e1s informaci\u00f3n en la web de la extension: MongoDB for VS Code. MongoDB Without Leaving Your IDE</p>"},{"location":"MongoDB/5_mongodb%20PrimerosPasos/","title":"5. Primeros pasos con MongoDB","text":"<p>Una vez instalada la base de datos, vamos a interactuar desde su propia consola.</p>"},{"location":"MongoDB/5_mongodb%20PrimerosPasos/#trabajando-con-mongodb-desde-la-consola","title":"Trabajando con MongoDB desde la consola","text":"<p>En la m\u00e1quina donde tenemos instalada la base de datos, podemos acceder a la consola de MongoDB escribimos:</p> <pre><code>mongosh\n</code></pre> <p>O para acceder en MongoDB Compass</p> <p>Algunas de las operaciones b\u00e1sicas que podemos realizar son : </p> <ul> <li>Salir de la consola (<code>quit()</code> o pulsando Ctrl+C)</li> <li>Limpiar la consola (Ctrl+L)</li> <li>Listar las bases de datos (<code>show dbs</code>)</li> <li>Cambiarse de base de datos (<code>use &lt;dbname&gt;</code>)</li> <li>Listar las colecciones de una base de datos (<code>show collections</code> / <code>show tables</code>)</li> </ul> <ul> <li>Mostrar el nombre de la base de datos (<code>db.getName()</code> o <code>db</code>)</li> <li>Listar metadata sobre una base de datos (<code>db.stats()</code>)</li> <li>Solicitar ayuda sobre comandos (<code>db.help()</code>)</li> <li>Mostrar fecha y hora del sistema (<code>Date()</code>)</li> <li>Dar formato JSON (<code>db.&lt;collectionName&gt;.find().pretty()</code>)</li> <li>Mostrar informaci\u00f3n sobre el servidor (<code>db.hostInfo()</code>)</li> </ul> <p>Observar que al poner <code>.pretty()</code> al final, hace que la salida tenga un formato f\u00e1cilmente reconocible, aunque hay ocasiones que la salida por defecto ya viene con este m\u00e9todo.</p> <p>Observa tambi\u00e9n que hay un diferencia entre invocar la funci\u00f3n con los par\u00e9ntesis <code>()</code>:</p> <p>Funciones y atributos</p> <p><code>db.hostInfo</code>: Este es un atributo que hace referencia a la informaci\u00f3n del sistema, pero no ejecuta la funci\u00f3n. Al usarlo de esta manera, no se obtienen datos directamente; simplemente se esta accediendo a una propiedad.</p> <p><code>db.hostInfo()</code>: Este es un m\u00e9todo que, al ser llamado con par\u00e9ntesis, ejecuta la funci\u00f3n y devuelve un documento con informaci\u00f3n detallada sobre el sistema subyacente en el que se est\u00e1 ejecutando el servidor MongoDB. En este caso se incluyen datos como el nombre del host, la arquitectura de la CPU, la memoria disponible, entre otros</p>"},{"location":"MongoDB/5_mongodb%20PrimerosPasos/#creacion-y-gestion-de-bases-de-datos","title":"Creaci\u00f3n y gesti\u00f3n de Bases de Datos","text":""},{"location":"MongoDB/5_mongodb%20PrimerosPasos/#creacion-use","title":"Creaci\u00f3n : <code>use</code>","text":"<p>El comando para crear una base de datos es el mismo que visto anteriormente para cambiar de base de datos: <code>use</code></p> <p>As\u00ed pues si intentamos entrar en una base de datos que no existe, directamente la prueba</p> <p>Hasta que no insertes al menos un documento en una de sus colecciones, no estar\u00e1 disponible. Esto lo podemos hacer en el siguiente ejemplo:</p> <p>Todo esto es debido a que MongoDB planifica la existencia de una base de datos, pero hasta que no tenga su primer dato, no va a designar ning\u00fan tipo de recursos a la misma. En la captura anterior, se ve que ya le ha asignado 8 KiB a nuestra primera base de datos porque ya tiene alg\u00fan dato.</p> <p>Por otra parte, para la creaci\u00f3n de una colecci\u00f3n e inclusi\u00f3n de un documento en concreto, observar que simplemente al insertar el documento, si la colecci\u00f3n no existe, la crea directamente, de la misma forma que ha hecho con la base de datos.</p> <p>M\u00e1s adelante ya veremos con m\u00e1s detenimiento las diferentes forma de insertar registro en un tabla (colecci\u00f3n), de momento hemos usando el comando:</p> <pre><code>db.primeraColeccion.insertOne({ id: 1, nombre: 'sergio' })\n</code></pre>"},{"location":"MongoDB/5_mongodb%20PrimerosPasos/#eliminacion-de-base-de-datos-dbdropdatabase","title":"Eliminaci\u00f3n de base de datos: <code>db.dropDatabase()</code>","text":"<p>Para eliminar una base de datos, en primer lugar debemos estar ubicados dentro de la propia base de datos a eliminar y ah\u00ed ejecutamos el comando </p> <pre><code>db.dropDatabase()\n</code></pre> <p>Podemos hacer uso de los comandos <code>use</code> y <code>db</code> para pasar ubicarnos en una base de datos y comprobar que efectivamente lo estamos, aunque en el prompt de la propia shell de MongoDB directamente ya nos dice que estamos ah\u00ed.</p>"},{"location":"MongoDB/5_mongodb%20PrimerosPasos/#mongodb-database-tools","title":"MongoDB Database Tools","text":"<p>Adem\u00e1s del propio servidor de MongoDB y el cliente para conectarse a \u00e9l, MongoDB ofrece un conjunto de herramientas para interactuar con las bases de datos, permitiendo crear y restaurar copias de seguridad.</p> <p>Si estamos interesados en introducir o exportar una colecci\u00f3n de datos mediante JSON, podemos emplear los comandos <code>mongoimport</code> y <code>mongoexport</code>:</p> <pre><code>mongoimport -d nombreBaseDatos -c coleccion \u2013-file nombreFichero.json\nmongoexport -d nombreBaseDatos -c coleccion nombreFichero.json\n</code></pre> <p>Estas herramientas interact\u00faan con datos JSON y no sobre toda la base de datos.</p> <p>Note</p> <p>Estos comando se ejecutan desde la shell donde tenemos instalados MongoDB, no desde la propia shell de MongoDB</p> <p>Un caso particular y muy com\u00fan es importar datos que se encuentran en formato CSV/TSV. Para ello, emplearemos el par\u00e1metro -<code>-type csv</code>:</p> <pre><code>mongoimport --type tsv -d test -c poblacion --headerline --drop poblacionEspanya2013.tsv\n</code></pre> <p>Donde   - <code>--type</code> : indica el tipo de fichero; csv o tsv   - <code>-d</code> o <code>--db</code>: indica la base de datos   - <code>-c</code> o <code>--collection</code> : indica la colecci\u00f3n   - <code>--headerline</code>: en un csv o tsv indica que la primera l\u00ednea contiene los nombres de los campos   - <code>--drop</code>: elimina la colecci\u00f3n si existe antes de la importaci\u00f3n del fichero   </p> <p>Mas informaci\u00f3n en el Manual de MongoDB: mongoimport</p> <p>En vez de realizar un export, es m\u00e1s conveniente realizar un backup en binario mediante <code>mongodump</code>, el cual genera ficheros BSON. Estos archivos posteriormente se restauran mediante <code>mongorestore</code>:</p> <pre><code>mongodump -d nombreBaseDatos nombreFichero.bson\nmongorestore -d nombreBaseDatos nombreFichero.bson\n</code></pre> <p>Si necesitamos transformar un fichero BSON a JSON (de binario a texto), tenemos el comando <code>bsondump</code>:</p> <pre><code>bsondump file.bson &gt; file.json\n</code></pre> <p>M\u00e1s informaci\u00f3n sobre copias de seguridad en la documentaci\u00f3n oficial de MongoDB: MongoDB Backup Methods .</p> <p>Para poder trabajar con MongoDB desde cualquier aplicaci\u00f3n necesitamos un driver. MongoDB ofrece drivers oficiales para casi todos los lenguajes de programaci\u00f3n actuales. En una sesi\u00f3n posterior trabajaremos con PyMongo.</p> <p>En cuanto a la monitorizaci\u00f3n, tanto <code>mongostat</code> como <code>mongotop</code> permiten visualizar el estado del servidor MongoDB, as\u00ed como algunas estad\u00edsticas sobre su rendimiento. Si trabajamos con MongoAtlas estas herramientas est\u00e1n integradas en las diferentes herramientas de monitorizaci\u00f3n de la plataforma.</p>"},{"location":"MongoDB/5_mongodb%20PrimerosPasos/#tipos-de-datos","title":"Tipos de datos","text":"<p>Aqu\u00ed tienes una lista de algunos tipos de datos comunes en MongoDB, junto con ejemplos de c\u00f3mo se representan en formato de tabla:</p> Tipo de Datos Descripci\u00f3n Ejemplo String Cadena de texto \"Hello World\" Number N\u00famero 42 Boolean Valor booleano (true/false) true Date Fecha y hora ISODate(\"2024-03-01T12:00:00.000Z\") Array Array de valores [1, 2, 3] Object Objeto o documento anidado {\"nombre\": \"Juan\", \"edad\": 30} ObjectId Identificador \u00fanico de documento ObjectId(\"61e4c3055b17967d02a9c3d7\") Null Valor nulo null BinData Datos binarios BinData(0, \"ABC123==\") Regular Expressions Expresiones regulares /pattern/g <p>Recuerda que MongoDB es una base de datos NoSQL orientada a documentos, por lo que no tiene una estructura de tabla como las bases de datos relacionales. En MongoDB, los datos se almacenan en documentos BSON (Binary JSON), que pueden contener campos con diferentes tipos de datos, incluidos los mencionados anteriormente.</p> <p>Es importante destacar que en MongoDB, los datos binarios y las expresiones regulares se representan de manera especial. Los datos binarios se representan mediante el tipo <code>BinData</code>, que incluye un tipo y una cadena de datos codificados en base64. </p> <p>Las expresiones regulares se representan utilizando el formato <code>/pattern/flags</code>, donde <code>pattern</code> es el patr\u00f3n de la expresi\u00f3n regular y <code>flags</code> son los modificadores de la expresi\u00f3n regular, como <code>i</code> para ignorar may\u00fasculas y min\u00fasculas o <code>g</code> para realizar una b\u00fasqueda global.</p> <p>El formato <code>/pattern/flags</code></p> <p>se utiliza com\u00fanmente en expresiones regulares (regex) en varios lenguajes de programaci\u00f3n, incluyendo JavaScript y MongoDB. Aqu\u00ed te explico cada parte:</p> <p><code>/pattern/</code>: Esta parte define el patr\u00f3n que deseas buscar. El patr\u00f3n puede incluir caracteres literales, metacaracteres y secuencias de escape. Por ejemplo, /abc/ buscar\u00eda la secuencia \u201cabc\u201d.</p> <p><code>flags</code>: Los flags son modificadores que alteran el comportamiento de la b\u00fasqueda. Algunos de los flags m\u00e1s comunes son:   - <code>i</code>: Ignora may\u00fasculas y min\u00fasculas (case insensitive).   - <code>g</code>: Realiza una b\u00fasqueda global, es decir, encuentra todas las coincidencias en lugar de detenerse en la primera.   - <code>m</code>: Permite que el car\u00e1cter <code>^</code> y <code>$</code> coincidan con el inicio y el final de cada l\u00ednea, no solo del texto completo.  </p> <p>Ejemplo   - Si quisieras buscar la palabra \u201chola\u201d sin importar si est\u00e1 en may\u00fasculas o min\u00fasculas en un texto, usar\u00edas <code>/hola/i</code>.</p> <p>Mas info en MongoDb docs $regex</p> <p>Aqu\u00ed tienes un ejemplo de c\u00f3mo se podr\u00eda representar un documento en MongoDB utilizando algunos de estos tipos de datos:</p> <pre><code>{\n  \"_id\": ObjectId(\"61e4c3055b17967d02a9c3d7\"),\n  \"nombre\": \"Juan\",\n  \"edad\": 30,\n  \"activo\": true,\n  \"intereses\": [\"programaci\u00f3n\", \"m\u00fasica\", \"viajes\"],\n  \"ubicacion\": {\n    \"ciudad\": \"Barcelona\",\n    \"pais\": \"Espa\u00f1a\"\n  },\n  \"fechaRegistro\": ISODate(\"2024-03-01T12:00:00.000Z\"),\n  \"comentarios\": [\n    {\n      \"usuario\": \"Ana\",\n      \"texto\": \"\u00a1Hola Juan!\"\n    },\n    {\n      \"usuario\": \"Carlos\",\n      \"texto\": \"Saludos desde X\u00e0tiva.\"\n    }\n  ]\n}\n</code></pre> <p>En este ejemplo, que ya hemos visto previamente para ilustrar qu\u00e9 es un BSON, el documento representa un usuario con campos como nombre, edad, activo, intereses, ubicaci\u00f3n, fecha de registro y comentarios. Cada campo tiene un tipo de datos diferente, como string, number, boolean, array, object, date, etc.</p> <p>M\u00e1s informaci\u00f3n sobre tipos de datos en tutorialspoint MongoDB  - Datatypes</p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/","title":"6. Operaciones con datos CRUD","text":"<p>En MongoDB , las operaciones CRUD (Crear, Leer, Actualizar, Eliminar) se realizan utilizando m\u00e9todos espec\u00edficos. Aqu\u00ed te muestro c\u00f3mo realizar cada una de estas operaciones.</p> <p>Antes de comenzar a trabajar, debemos entrar en una de las bases de datos con <code>use</code> y en todo momento podemos ver las colecciones que tenemos en esta base de datos con <code>use collections</code> </p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#insertar","title":"Insertar :","text":"<p>Para insertar documentos en una colecci\u00f3n, se utiliza el m\u00e9todo <code>insertOne()</code> o <code>insertMany()</code>.</p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#insertone","title":"<code>insertOne()</code>","text":"<ul> <li><code>db.collectionName.insertOne(&lt;json&gt;);</code>: Inserta un solo documento</li> </ul> MongoDB. Inserci\u00f3n en colecci\u00f3n. <p>Ejemplos</p> <ul> <li>Insertar un solo documento en la colecci\u00f3n 'usuarios'</li> </ul> <pre><code>db.usuarios.insertOne({\n    nombre: \"Juan\",\n    edad: 30,\n    ciudad: \"Barcelona\"\n});\n</code></pre> <ul> <li>En un \u00fanica l\u00ednea, y con comillas dobles, tambi\u00e9n...</li> </ul> <pre><code>db.usuarios.insertOne({ nombre: \"Toni\", edad: 15, ciudad: \"Valencia\"});\n</code></pre> <ul> <li>Insertar un solo documento en la colecci\u00f3n 'usuarios'</li> </ul> <pre><code>db.usuarios.insertOne({\n    nombre: \"Juan\",\n    edad: 30,\n    ciudad: \"Barcelona\",\n    intereses: [\"f\u00fatbol\", \"m\u00fasica\"],\n    direccion: { calle: \"Calle Mayor\", numero: 123 },\n    fechaRegistro: new Date(),\n    activo: true\n});\n</code></pre> <ul> <li>Insertar un documento con la fecha actual. Ojo con el \u00faltimo campo</li> </ul> <pre><code>db.usuarios.insertOne({\n    nombre: \"Dorotea\",\n    fecha: new Date(),\n    lugar: \"X\u00e0tiva\"\n});\n</code></pre> <ul> <li>Insertar un documento con una fecha espec\u00edfica </li> </ul> <p>Los campos antes no estaban entre comillas y en este ejemplo si lo estan; es totalmente indiferente. Pero si debemos tener en cuenta que es case-sensitive (May\u00fasculas/Min\u00fasculas) es importante y adcem\u00e1s tambi\u00e9n cambia si ponemos los datos n\u00famericos entre comillas o no, ya que se convierten en texto</p> <pre><code>db.usuarios.insertOne({\n    \"nombre\": \"Filiberto\",\n    \"edad\" : \"33\",\n    \"fechaNacimiento\": new Date(\"2004-03-15\"),\n    \"ciudad\": \"Alacant\"\n});\n</code></pre> <p>Cada vez que hacemos una inserci\u00f3n, si es correcta, nos devuelve el resultado del con el siguiente formato</p> <pre><code>{\n  acknowledged: true,\n  insertedId: ObjectId('65e818360ad094ed2ab24245')\n}\n</code></pre>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#insertmany","title":"<code>insertMany()</code>","text":"<ul> <li><code>db.collectionName.insertMany(&lt;json&gt;);</code>. Inserci\u00f3n de varios elementos.</li> </ul> <p>Como se trata de una inserci\u00f3n de un conjunto de documentos, lo que hacemos en pasar un array y esto se hace mediante el uso de corchetes : <code>[]</code>.</p> <p>Ejemplos: </p> <ul> <li> <p>Insertar varios documentos en la colecci\u00f3n 'usuarios' <pre><code>db.usuarios.insertMany([\n    { nombre: \"Ana\", edad: 25, ciudad: \"Madrid\" },\n    { nombre: \"Carlos\", edad: 35, ciudad: \"Valencia\" }\n]);\n</code></pre></p> </li> <li> <p>Insertar varios documentos en la colecci\u00f3n 'usuarios' <pre><code>db.usuarios.insertMany([\n    { nombre: \"Ana\", edad: 25, ciudad: \"Madrid\", intereses: [\"viajes\"], fechaRegistro: new Date(), activo: false },\n    { nombre: \"Carlos\", edad: 35, ciudad: \"Valencia\", intereses: [\"lectura\"], fechaRegistro: new Date(), activo: true }\n]);\n</code></pre></p> </li> </ul> <p>Observar c\u00f3mo se trabaja con las fechas</p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#leerconsultas","title":"Leer/Consultas:","text":"<p>Uno de los aspectos m\u00e1s interesantes de las bases de datos es la capacidad para realizar consultas, por lo que ahora vamos a ver de forma muy breve como leer datos, pero m\u00e1s adelante profundizaremos en la realizaci\u00f3n de consultas m\u00e1s elaboradas.</p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#find","title":"<code>find()</code>","text":"<p>Para leer datos de una colecci\u00f3n, se utiliza el m\u00e9todo <code>find()</code>.</p> <ul> <li> <p>Leer todos los documentos de la colecci\u00f3n 'usuarios' <pre><code>db.usuarios.find();\n</code></pre></p> </li> <li> <p>Leer todos los documentos de la colecci\u00f3n 'usuarios' y formatear la salida json <pre><code>db.usuarios.find().pretty();\n</code></pre></p> </li> </ul> <p>funcion <code>pretty()</code></p> <p>La funci\u00f3n <code>pretty()</code> en MongoDB sigue siendo efectiva, pero su uso ha cambiado con las versiones m\u00e1s recientes. En mongosh (la nueva shell de MongoDB), <code>pretty()</code> no altera el formato de salida, mientras que en la shell legacy (mongo shell), s\u00ed lo hac\u00eda, mostrando los resultados de manera m\u00e1s legible</p> <ul> <li> <p>Leer documentos que coincidan con un criterio espec\u00edfico <pre><code>db.usuarios.find({ ciudad: \"Barcelona\" });\n</code></pre></p> </li> <li> <p>Leer documentos que coincidan con un criterio espec\u00edfico (por ejemplo, ciudad igual a 'Barcelona' y activo igual a true) <pre><code>db.usuarios.find({ ciudad: \"Barcelona\", activo: true });\n</code></pre></p> </li> </ul> <p>Podemos especificar los campos que queremos recuperar en la consulta: </p> <ul> <li>Leer documentos con proyecci\u00f3n (seleccionar campos espec\u00edficos) <pre><code>db.usuarios.find({}, { nombre: 1, edad: 1 });\n</code></pre></li> </ul> <p>El formato a utilizar es <code>NombreDeCampo: 1</code>, o sea, escribimos el nombre del campo, seguido de dos puntos y un uno. </p> <p>L\u00f3gicamente si ponemos un 1, significa que queremos ver el campo, y por lo tanto si ponemos un 0, significa que no queremos que se muestre el campo, pero esto solo funciona con el campo objetctID (_id)</p> <ul> <li> <p>Leer documentos con proyecci\u00f3n (seleccionar campos espec\u00edficos) <pre><code>db.usuarios.find({}, { nombre: 1, edad: 1, _id: 0 });\n</code></pre></p> </li> <li> <p>Leer todos los eventos que ocurrieron despu\u00e9s de una fecha espec\u00edfica <pre><code>db.eventos.find({ fecha: { $gt: new Date(\"2024-01-01\") } });\n</code></pre></p> </li> <li> <p>Leer eventos que ocurrieron en un rango de fechas <pre><code>db.eventos.find({ fecha: { $gte: new Date(\"2024-01-01\"), $lte: new Date(\"2024-12-31\") } });\n</code></pre></p> </li> </ul> <p>Nota</p> <p>No profundizamos m\u00e1s en las b\u00fasquedas porque m\u00e1s adelante dedicaremos un punto completo a explicar las b\u00fasquedas en m\u00e1s profundidad.</p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#actualizar","title":"Actualizar:","text":"<p>Para actualizar documentos en una colecci\u00f3n, se utiliza el m\u00e9todo <code>updateOne()</code> o <code>updateMany()</code>.</p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#updateone","title":"<code>updateOne()</code>","text":"<ul> <li><code>db.collection.updateOne(&lt;filter&gt;, &lt;update&gt;)</code></li> </ul> <p>El m\u00e9todo <code>updateOne()</code> se utiliza para actualizar un solo documento que coincida con un criterio espec\u00edfico. Si hay varios documentos que coinciden con el criterio, solo se actualizar\u00e1 el primero que se encuentre.</p> <p>En la clausula de actualizaci\u00f3n tenemos el comando <code>$set</code>. Adem\u00e1s debemos tener en cuenta que tanto el <code>&lt;filter&gt;</code> como la <code>&lt;update&gt;</code> son json por lo que deben estar comprendidos entre corchetes:</p> <pre><code>db.collection.updateOne({}, {$set:{}});\n</code></pre> <p>Veamos algunos ejemplos</p> <ul> <li> <p>Ejemplo de <code>updateOne()</code> <pre><code>db.usuarios.updateOne(\n    { nombre: \"Juan\" },\n    { $set: { edad: 31 } }\n);\n</code></pre></p> </li> <li> <p>Cambiamos m\u00e1s de un valor <pre><code>db.usuarios.updateOne(\n    { nombre: \"Juan\" },\n    { $set: { edad: 31, ciudad: 'Albacete' } }\n);\n</code></pre></p> </li> <li> <p>Actualizar la fecha de un evento espec\u00edfico <pre><code>db.usuarios.updateOne(\n    { nombre: \"Alberto\" },\n    { $set: { fechaRegistro: new Date(\"2024-03-20\") } }\n);\n</code></pre></p> </li> </ul> <p>En el primer ejemplo, se actualizar\u00e1 el primer documento de la colecci\u00f3n \"usuarios\" que tenga el campo <code>nombre</code> igual a \"Juan\". Si hay varios documentos con ese nombre, solo se actualizar\u00e1 uno.</p> <p>Una vez realizada la actualizaci\u00f3n, MongoDB avisa: </p> <pre><code>{\n  acknowledged: true,\n  insertedId: null,\n  matchedCount: 1,\n  modifiedCount: 1,\n  upsertedCount: 0\n}\n</code></pre>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#updatemany","title":"<code>updateMany()</code>","text":"<ul> <li><code>db.collection.updateMany(&lt;filter&gt;, &lt;update)</code></li> </ul> <p>Por otro lado, el m\u00e9todo <code>updateMany()</code> se utiliza para actualizar m\u00faltiples documentos que coincidan con un criterio espec\u00edfico. Todos los documentos que cumplan el criterio ser\u00e1n actualizados.</p> <ul> <li>Ejemplo de <code>updateMany()</code> <pre><code>db.usuarios.updateMany(\n    { ciudad: \"J\u00e1tiva\" },\n    { $set: { ciudad: \"X\u00e0tiva\" } }\n);\n</code></pre></li> </ul> <p>En este ejemplo, se actualizar\u00e1n todos los documentos de la colecci\u00f3n \"usuarios\" que tengan el campo <code>ciudad</code> igual a \"J\u00e1tiva\", estableciendo su valor a \"X\u00e0tiva\".</p> <p>O sea, <code>updateOne()</code> es \u00fatil cuando solo quieres actualizar un \u00fanico documento, mientras que <code>updateMany()</code> es \u00fatil cuando necesitas actualizar m\u00faltiples documentos que cumplan un criterio espec\u00edfico.</p> <p>Adem\u00e1s de establecer un valor en una actualizaci\u00f3n, tenemos otras opciones para las modificaciones a aplicar. Utiliza operadores como: - <code>$set</code>: Para establecer un nuevo valor. - <code>$unset</code>: Para eliminar un campo. - <code>$inc</code>: Para incrementar un valor num\u00e9rico. - <code>$push</code>: Para agregar un elemento a un array.  </p> <ul> <li>En el siguiente ejemplo, incrementamos un a\u00f1o la edad de los usuarios con un determinado nombre.</li> </ul> <pre><code>db.usuarios.updateMany(\n    { nombre : \"Alberto\" },\n    { $inc: { edad: 1 } }\n);\n</code></pre>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#replaceone","title":"<code>replaceOne()</code>","text":"<ul> <li><code>db.collection.replaceOne(&lt;filter&gt;, &lt;update&gt;)</code></li> </ul> <p>Reemplazo completo de un documento. En este caso, al reemplazar el documento que encontramos por una nuevo, no necesitamos el comando <code>$set</code></p> <ul> <li>Ejemplo donde reemplaza el primer registro qe encuentra con el filtro aplicado <pre><code>db.usuarios.replaceOne(\n    { nombre: \"Alberto\" },\n    {     \n        nombre: \"Alberto D.\",\n        edad: 18,\n        ciudad: \"X\u00e0tiva\",\n        intereses: [\"Baloncesto\", \"Inform\u00e1tica\"],\n        fechaRegistro: new Date(),\n        activo: true \n    }\n);\n</code></pre></li> </ul> <p>En este caso, vamos a ver su ejecuci\u00f3n, donde veremos incluso que al cambiar el nombre, este campo tambi\u00e9n cambia aunque se el utilizado para hacer la b\u00fasqueda:</p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#eliminar","title":"Eliminar:","text":"<p>Para eliminar documentos de una colecci\u00f3n, se utiliza el m\u00e9todo <code>deleteOne()</code> o <code>deleteMany()</code>.</p> <p>En MongoDB, tanto <code>deleteOne()</code> como <code>deleteMany()</code> son m\u00e9todos utilizados para eliminar documentos de una colecci\u00f3n. Aqu\u00ed tienes las diferencias entre ellos:</p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#deleteone","title":"<code>deleteOne()</code>","text":"<p>El m\u00e9todo <code>deleteOne()</code> se utiliza para eliminar un solo documento que coincida con un criterio espec\u00edfico. Si hay varios documentos que coinciden con el criterio, solo se eliminar\u00e1 el primero que se encuentre.</p> <ul> <li>Ejemplo de <code>deleteOne()</code> <pre><code>db.usuarios.deleteOne({ nombre: \"Juan\" });\n</code></pre></li> </ul> <p>En este ejemplo, se eliminar\u00e1 el primer documento de la colecci\u00f3n \"usuarios\" que tenga el campo <code>nombre</code> igual a \"Juan\". Si hay varios documentos con ese nombre, solo se eliminar\u00e1 uno.</p>"},{"location":"MongoDB/6_mongodb%20OperacionesCRUD/#deletemany","title":"<code>deleteMany()</code>","text":"<p>Por otro lado, el m\u00e9todo <code>deleteMany()</code> se utiliza para eliminar varios documentos que coincidan con un criterio espec\u00edfico. Todos los documentos que cumplan el criterio ser\u00e1n eliminados.</p> <ul> <li>Ejemplo de <code>deleteMany()</code> <pre><code>db.usuarios.deleteMany({ activo: false });\n</code></pre></li> </ul> <p>En este ejemplo, se eliminar\u00e1n todos los documentos de la colecci\u00f3n \"usuarios\" que tengan el campo <code>activo</code> igual a false.</p> <p>Otros ejemplos</p> <ul> <li> <p>Eliminar varios documentos que cumplan el criterio especificado <pre><code>db.usuarios.deleteMany({ ciudad: \"X\u00e0tiva\" });\n</code></pre></p> </li> <li> <p>Eliminar eventos que ocurrieron antes de una fecha espec\u00edfica <pre><code>db.eventos.deleteMany({ fecha: { $lt: new Date(\"2024-03-01\") } });\n</code></pre></p> </li> </ul> <p>Cuidado</p> <p>Como siempre debemos tener cuidad a la hora de borrar, de echo si hacemos </p> <p><pre><code>db.usuarios.deleteMany({});\n</code></pre> Nos borrar\u00e1 todos los documentos de la colecci\u00f3n.</p> <p>Nota</p> <p>Siempre se debe pasar un json dentro de la condici\u00f3n, o sea, dentro de los par\u00e9ntesis deben haber llaves:</p> <pre><code>db.usuarios.deleteMany();     //error\ndb.usuarios.deleteMany({});   //correcto: json vac\u00edo\n</code></pre> <p>Ahora que ya tenemos m\u00e1s herramientas y hemos visto las operaciones b\u00e1sicas de MongoDB en la siguiente secci\u00f3n vamos a profundizar sobre las consultas de los datos, aunque ya las hemos visto brevemente con anterioridad.</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/","title":"7. Operaciones con datos: Consultas y m\u00e9todos","text":"<p>Ahora que ya tenemos m\u00e1s herramientas y hemos visto las operaciones b\u00e1sicas de MongoDB vamos a profundizar sobre las consultas de los datos, aunque ya las hemos visto brevemente con anterioridad.</p> <p>Aprovechamos para introducir una base de datos con una colecci\u00f3n con datos de prueba. En el siguiente enlace tenemos una base de datos de pel\u00edculas que podemos introducir en MongoDB por ejemplo usando en MongoDB Compass. Para ello creamos una base de datos llamada consultas e importamos el fichero descargado.</p> <p>El comando b\u00e1sico es <code>.find()</code></p> <pre><code>db.collection.find()            // devuelve todos los documentos\ndb.collection.find(&lt;filter&gt;)    // devuelve los documentos que cumplen el filtro\n</code></pre>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#operadores-mongodb","title":"Operadores MongoDB","text":"<p>Antes de continuar, en la siguiente tabla esta el listado de los principales operadores utilizados en consultas MongoDB para la construcci\u00f3n de los filtros:</p> Operador Descripci\u00f3n Ejemplo <code>$eq</code> Igualdad <code>db.collection.find({campo: {$eq: valor}})</code> <code>$ne</code> No igual <code>db.collection.find({campo: {$ne: valor}})</code> <code>$gt</code> Mayor que <code>db.collection.find({campo: {$gt: valor}})</code> <code>$gte</code> Mayor o igual que <code>db.collection.find({campo: {$gte: valor}})</code> <code>$lt</code> Menor que <code>db.collection.find({campo: {$lt: valor}})</code> <code>$lte</code> Menor o igual que <code>db.collection.find({campo: {$lte: valor}})</code> <code>$in</code> Igual a cualquiera de los valores en un array <code>db.collection.find({campo: {$in: [valor1, valor2]}})</code> <code>$nin</code> No igual a ninguno de los valores en un array <code>db.collection.find({campo: {$nin: [valor1, valor2]}})</code> <code>$exists</code> Verifica si el campo existe <code>db.collection.find({campo: {$exists: true/false}})</code> <code>$type</code> Verifica el tipo de datos del campo <code>db.collection.find({campo: {$type: tipo}})</code> <code>$regex</code> Realiza una b\u00fasqueda de expresi\u00f3n regular <code>db.collection.find({campo: {$regex: /patr\u00f3n/}})</code> <code>$or</code> Realiza una disyunci\u00f3n l\u00f3gica <code>db.collection.find({$or: [{condici\u00f3n1}, {condici\u00f3n2}]})</code> <code>$and</code> Realiza una conjunci\u00f3n l\u00f3gica <code>db.collection.find({$and: [{condici\u00f3n1}, {condici\u00f3n2}]})</code> <code>$not</code> Niega una expresi\u00f3n <code>db.collection.find({campo: {$not: {condici\u00f3n}}})</code> <code>$nor</code> Realiza una disyunci\u00f3n negada <code>db.collection.find({$nor: [{condici\u00f3n1}, {condici\u00f3n2}]})</code> <p>Estos operadores son fundamentales para realizar consultas avanzadas en MongoDB, permitiendo filtrar y buscar documentos en funci\u00f3n de diferentes criterios. Puedes combinar estos operadores para construir consultas complejas y poderosas que se adapten a tus necesidades espec\u00edficas.</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#consultas-con-find-ejemplos-practicos","title":"Consultas con <code>.find</code>. Ejemplos pr\u00e1cticos.","text":"<p>Utilizando la tabla de movies vamos a realizar algunas consultas que servir\u00e1n para ilustrar los aspectos m\u00e1s interesantes de las b\u00fasquedas en MongoDB. </p> <p>Veamos en primer lugar un registro tipo</p> <pre><code>{\n  \"_id\": {\n    \"$oid\": \"573a1390f29313caabcd4135\"\n  },\n  \"plot\": \"Three men hammer on an anvil and pass a bottle of beer around.\",\n  \"genres\": [\n    \"Short\"\n  ],\n  \"runtime\": 1,\n  \"cast\": [\n    \"Charles Kayser\",\n    \"John Ott\"\n  ],\n  \"num_mflix_comments\": 1,\n  \"title\": \"Blacksmith Scene\",\n  \"fullplot\": \"A stationary camera looks at a large anvil with a blacksmith behind it and one on either side. The smith in the middle draws a heated metal rod from the fire, places it on the anvil, and all three begin a rhythmic hammering. After several blows, the metal goes back in the fire. One smith pulls out a bottle of beer, and they each take a swig. Then, out comes the glowing metal and the hammering resumes.\",\n  \"countries\": [\n    \"USA\"\n  ],\n  \"released\": {\n    \"$date\": {\n      \"$numberLong\": \"-2418768000000\"\n    }\n  },\n  \"directors\": [\n    \"William K.L. Dickson\"\n  ],\n  \"rated\": \"UNRATED\",\n  \"awards\": {\n    \"wins\": 1,\n    \"nominations\": 0,\n    \"text\": \"1 win.\"\n  },\n  \"lastupdated\": \"2015-08-26 00:03:50.133000000\",\n  \"year\": 1893,\n  \"imdb\": {\n    \"rating\": 6.2,\n    \"votes\": 1189,\n    \"id\": 5\n  },\n  \"type\": \"movie\",\n  \"tomatoes\": {\n    \"viewer\": {\n      \"rating\": 3,\n      \"numReviews\": 184,\n      \"meter\": 32\n    },\n    \"lastUpdated\": {\n      \"$date\": \"2015-06-28T18:34:09Z\"\n    }\n  }\n}\n</code></pre> <p>Comenzamos con consultas sencillas y vamos incrementando el nivel.</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#consultas-basicas","title":"Consultas b\u00e1sicas","text":"<ul> <li>Obtener pel\u00edculas lanzadas en el a\u00f1o 1983 :</li> </ul> <pre><code>db.movies.find({ year: 1993 })\n</code></pre> <ul> <li>Buscar pel\u00edculas de genero \"corto\" (genres = short)</li> </ul> <pre><code>db.movies.find({ \"genres\": \"Short\" })\n</code></pre>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#consultas-con-operadores-logicos","title":"Consultas con operadores l\u00f3gicos","text":"<ul> <li>Buscar pel\u00edculas de duraci\u00f3n (runtime) superior a 120 minutos</li> </ul> <pre><code>db.movies.find({ \"runtime\": { $gt: 120 } })\n</code></pre> <ul> <li>Buscar pel\u00edculas cuya duraci\u00f3n se encuentre entre 90 y 100 minutos ambos incluidos</li> </ul> <pre><code>db.movies.find({ \"runtime\": { $gte: 90, $lte: 100} })\n</code></pre> <ul> <li>Buscar pel\u00edculas que no tengan puntuaci\u00f3n (UNRATED) y que duren m\u00e1s de 100 minutos. </li> </ul> <p>En este caso, podemos utilizar <code>$and</code> de forma impl\u00edcita a\u00f1adiendo la condici\u00f3n con un array o explicita si poner nada m\u00e1s que los campos que deben cumplirse</p> <pre><code>db.movies.find({ rated: \"PASSED\", runtime: { $gt: 100} })\n</code></pre> <p>Equivale exactamente a </p> <pre><code>db.Movies.find({ \n    $and: [ \n        { rated: \"PASSED\"}, \n        { runtime: { $gt: 100}} \n    ]\n})\n</code></pre> <p>A partir de esta el <code>$or</code> es similar.</p> <p>Nota</p> <p>Podemos utilizar <code>.count()</code> al final para hacer recuento y verificar que el resultado de las dos consultas anteriores son iguales.</p> <ul> <li>Buscar las pel\u00edculas que han recibido alg\u00fan premio: </li> </ul> <p>En este caso debe existir la propiedad awards</p> <pre><code>db.movies.find({\n  \"awards\": { $exists: true }\n})\n</code></pre> <ul> <li> <p>Buscar las pel\u00edculas donde ha intervenido el actor \"Charles Kayser\", pero mostrar solo titulo, fecha de lanzamiento, idioma, director y premios ganados <pre><code>db.movies.find({\n    \"cast\": \"Charles Kayser\"\n    }, {\n    \"title\": 1,\n    \"released\": 1,\n    \"languages\": 1,\n    \"directors\": 1,\n    \"awards.wins\": 1,\n})\n</code></pre></p> </li> <li> <p>Buscar pel\u00edculas lanzadas en una fecha determinada, y mostrando unos campos determinados.:</p> </li> </ul> <pre><code>db.movies.find({\n    released: ISODate(\"1997-05-01T00:00:00.000Z\")\n    }, {\n    title: 1,\n    languages: 1,\n    released: 1,\n    directors: 1,\n    writers: 1,\n    countries: 1\n  }\n)\n</code></pre>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#consultas-sobre-objetos-anidados","title":"Consultas sobre objetos anidados","text":"<ul> <li>Buscar pel\u00edculas que han recibido m\u00e1s de 1000 votos en IMDb</li> </ul> <pre><code>db.movies.find({ \"imdb.votes\": { $gt: 1000 } })\n</code></pre> <ul> <li>Buscar pel\u00edculas que tienen una puntuaci\u00f3n IMDb superior a 7</li> </ul> <pre><code>db.movies.find({ \"imdb.rating\": { $gt: 7 } })\n</code></pre> <ul> <li>Buscar las pel\u00edculas que han sido nominadas y mostrar solo el titulo, directores y a\u00f1o. </li> </ul> <p>En este caso como novedad seleccionamos solo unos campos</p> <pre><code>db.movies.find({\n    \"awards.nominations\": { $gt: 0 }\n  }, {\n    \"title\": 1,\n    \"directors\": 1,\n    \"year\": 1\n  })\n</code></pre> <ul> <li>Buscar pel\u00edculas con una puntuaci\u00f3n entre 3 y 4 en el rating de viewer en tomatoes. Mostrar solo algunos campos por simplicidad</li> </ul> <pre><code>db.movies.find(\n  { 'tomatoes.viewer.rating': { $gte: 3, $lt: 4 } },\n  { \n    title: 1, \n    languages: 1, \n    released: 1, \n    directors: 1, \n    'tomatoes.viewer': 1, \n    writers: 1, \n    countries: 1 \n  }\n)\n</code></pre>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#consultas-con-expresiones-regulares","title":"Consultas con expresiones regulares","text":"<p>Si queremos realizar consultas sobre partes de un campo de texto, hemos de emplear expresiones regulares. Para ello, tenemos el operador <code>$regexp</code> o, de manera m\u00e1s sencilla, indicando como valor la expresi\u00f3n regular a cumplir:</p> <ul> <li>Buscar todas las pel\u00edculas (titulo, lenguaje, lanzamiento, directores y guionistas) que tenga el literal \"scene\" en el titulo</li> </ul> <pre><code>db.movies.find(\n    { title: { $regex: /scene/i } },\n    { title: 1, \n      languages: 1, released: 1, directors: 1, writers: 1, countries: 1 }\n)\n</code></pre> <p>equivale a </p> <pre><code>db.movies.find(\n    { title: /scene/i },\n    { title: 1, languages: 1, released: 1, directors: 1, writers: 1, countries: 1 }\n)\n</code></pre> <ul> <li>Buscar pel\u00edculas que tengan en su sinopsis (fullplot) la palabra \"fire\", sean anteriores al 1980 y que sean del genero \"Shorts\"</li> </ul> <pre><code>db.movies.find(\n  {\n    fullplot: { $regex: /fire/i },\n    year: { $lt: 1980 },\n    genres: \"Short\"\n  }, \n  { \n    title: 1, \n    year: 1, \n    languages: 1, \n    fullplot: 1, \n    released: 1, \n    directors: 1, \n    writers: 1, \n    countries: 1 \n  }\n)\n</code></pre> <ul> <li>Buscar pelicuas que tengan cualquiera de estos actores: \"Tom Cruise\", \"Tom Hanks\" o un actor que contenga \"Smith\" en su nombre</li> </ul> <pre><code>db.movies.find({\n    cast: /Tom Cruise|Tom Hanks|Smith/\n  }, \n  { \n    _id: 0,\n    title: 1, \n    cast: 1,\n    year: 1\n  }\n)\n</code></pre> <p>Consulta de elementos de un array</p> <p>Mediante las expresiones regulares hemos visto c\u00f3mo se consulta de forma sencialla elementos de un array, sin embargo tenemos un filtro especifico para estos casos <code>$elemMatch</code> </p> <p><pre><code>  db.movies.find({ cast: { $elemMatch: { $eq: \"Leonardo DiCaprio\" }}});\n</code></pre> Mientras que expresiones regulares encuentra elementos que coindicen parcial o totalmente, con este filtro buscamos elementos exactos.</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#otros-metodos-interesantes","title":"Otros m\u00e9todos interesantes.","text":"<p>Existen gran variedad de m\u00e9todos que nos permiten realizar todo tipo de consultas sobre una base de datos y m\u00e1s concretamente sobre una colecci\u00f3n. </p> <p>Un listado detallado de todos estos m\u00e9todos lo encontramos en la web oficial de MongoDB: MongoDB Manual - Collection Methods</p> <p>Pueden resultar interesante las siguientes:</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#metodo-distinct","title":"M\u00e9todo <code>.distinct()</code>","text":"<p>Obtiene los valores diferentes de un campo de una colecci\u00f3n: </p> <p><pre><code>db.movies.distinct( 'genres' );\n</code></pre> <pre><code>db.movies.distinct( 'type' );\n</code></pre> Estos dos ejemplos obtienen los diferentes valores de los atributos indicados.</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#metodo-count","title":"M\u00e9todo <code>.count()</code>","text":"<p>Cuenta la cantidad de documentos que cumplen una condici\u00f3n.</p> <pre><code>db.movies.find({\n    year: { $lt: 2000 },\n    genres: \"Short\"\n}).count()\n</code></pre> <p>Obtenemos la cantidad de cortos de la colecci\u00f3n, posterior al a\u00f1o 2000</p> <ul> <li>Buscar la cantidad pel\u00edculas que tengan en su sinopsis (fullplot) la palabra \"fire\", sean anteriores al 1980 y que sean del genero \"Shorts\"</li> </ul> <pre><code>db.movies.find({\n    fullplot: { $regex: /fire/i },\n    year: { $lt: 1900 },\n    genres: \"Short\"\n}).count()\n</code></pre>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#metodo-limit","title":"M\u00e9todo <code>.limit()</code>","text":"<p>Muestra una cantidad de documentos indicada que cumplan una condici\u00f3n.</p> <pre><code>db.movies.find({\n    year: { $lt: 2000 },\n    genres: \"Short\"\n}).limit(5)\n</code></pre> <p>Obtenemos 5 documentos de cortos de la colecci\u00f3n, posterior al a\u00f1o 2000</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#metodo-sort","title":"M\u00e9todo <code>.sort()</code>","text":"<p>Ordena los documentos devueltos por una consulta. </p> <p>La sintaxis b\u00e1sica es la siguiente:</p> <pre><code>db.collection.find().sort({ campo: orden })\n</code></pre> <ul> <li><code>campo</code>: El nombre del campo por el cual deseas ordenar.  </li> <li><code>orden</code>: Puede ser <code>1</code> para orden ascendente o <code>-1</code> para orden descendente.</li> </ul> <p>Por ejemplo, para ordenar por a\u00f1o de lanzamiento de forma ascendente:</p> <pre><code>db.movies.find().sort({ year: 1 })\n</code></pre> <p>Si deseas ordenarlas de forma descendente:</p> <pre><code>db.movies.find().sort({ year: -1 })\n</code></pre> <p>Tambi\u00e9n podemos ordenar por m\u00e1s de un campo. Por ejemplo, para ordenar primero por <code>genre</code> y luego por <code>rating</code>:</p> <pre><code>db.movies.find().sort({ genre: 1, rating: -1 })\n</code></pre> <p>Tambi\u00e9n podemos encadenar estas funciones: </p> <ul> <li>Listado de las 5 mejores pel\u00edculas seg\u00fan la puntuaci\u00f3n \"imdb\"</li> </ul> <pre><code>db.movies.find({}, {\n    title: 1,\n    imdb: 1\n}).sort({ imdb: 1 }).limit(5)\n</code></pre> <p>Mejoramos el resultado anterior y le quitamos los valores vacios en la puntuaci\u00f3n:</p> <pre><code>db.movies.find({ \"imdb.rating\": {\"$ne\": \"\"}}, {\n    title: 1,\n    imdb: 1\n}).sort({ imdb: -1 }).limit(5)\n</code></pre>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#cursores","title":"Cursores","text":"<p>Al hacer una consulta en el shell se devuelve un cursor. Este cursor lo podemos guardar en un variable, y partir de ah\u00ed trabajar con \u00e9l como har\u00edamos mediante cualquier lenguaje de programaci\u00f3n. Si <code>cur</code> es la variable que referencia al cursor, podremos utilizar los siguientes m\u00e9todos:</p> M\u00e9todo Uso Lugar de ejecuci\u00f3n <code>cur.hasNext()</code> true/false para saber si quedan elementos Cliente <code>cur.next()</code> Pasa al siguiente documento Cliente <code>cur.limit(*cantidad*)</code> Restringe el n\u00famero de resultados a cantidad Servidor <code>cur.sort({*campo*:1})</code> Ordena los datos por campo: 1 ascendente o -1 o descendente Servidor <code>cur.skip(*cantidad*)</code> Permite saltar cantidad elementos con el cursor Servidor <code>cur.count()</code> Obtiene la cantidad de documentos Servidor <p>Como tras realizar una consulta con <code>find</code> realmente se devuelve un cursor, un uso muy habitual es encadenar una operaci\u00f3n de <code>find</code> con <code>sort</code> y/o <code>limit</code> y/o <code>count</code> para ordenar el resultado por uno o m\u00e1s campos y posteriormente limitar el n\u00famero de documentos a devolver o simplemente contar.</p> <p>Estos cursores se utilizan cuando accedemos a consulta desde script en javascript u otros lenguajes. </p> <p>Por ejemplo, a continuacion tenemos c\u00f3digo en javascript que imprime por consola todos los documentos obtenidos tras una consulta</p> javascriptpython <pre><code>// necesario node.js\n// Conectar a la base de datos test\nconst db = connect(\"mongodb://localhost:27017/test\");\n\n// Realizar una consulta para encontrar todas (5) las pel\u00edculas\nvar cursor = db.movies.find().limit( 5);\n\n// Iterar sobre los resultados usando un cursor\nwhile (cursor.hasNext()) {\n    printjson(cursor.next());\n}\n</code></pre> <pre><code># necesaria librer\u00eda pymongo: \nfrom pymongo import MongoClient\n\n# Conectar a la base de datos\nclient = MongoClient(\"mongodb://localhost:27017/\")\n# Utilizamos la base de datos \"text\"\ndb = client.test\n\n# Realizar una consulta para encontrar todas (5) las pel\u00edculas\ncursor = db.movies.find().limit(5)\n\n# Iterar sobre los resultados usando un cursor\nfor document in cursor:\n    print(document)\n</code></pre> <p>Se puede tambi\u00e9n usar m\u00e9todos como <code>limit</code> y <code>skip</code> para controlar la cantidad de resultados:</p> <ul> <li>Ejemplo: obtenemos 5 registros de una coleccc\u00f3n</li> </ul> javascriptpython <pre><code>// Obtener las primeras 5 pel\u00edculas\nvar limitedCursor = db.movies.find().limit(5);\n\n// Iterar sobre los resultados limitados\nwhile (limitedCursor.hasNext()) {\n    printjson(limitedCursor.next());\n}\n\n// Saltar las primeras 2 pel\u00edculas y obtener las siguientes 3\nvar paginatedCursor = db.movies.find().skip(2).limit(3);\n\nwhile (paginatedCursor.hasNext()) {\n    printjson(paginatedCursor.next());\n}\n</code></pre> <pre><code>from pymongo import MongoClient\n\n# Conectar a la base de datos\nclient = MongoClient(\"mongodb://localhost:27017/\")\ndb = client.test\n\n# Obtener las primeras 5 pel\u00edculas\nlimited_cursor = db.movies.find().limit(5)\n\n# Iterar sobre los resultados limitados\nfor document in limited_cursor:\n    print(document)\n\n# Saltar las primeras 2 pel\u00edculas y obtener las siguientes 3\npaginated_cursor = db.movies.find().skip(2).limit(3)\n\n# Iterar sobre los resultados paginados\nfor document in paginated_cursor:\n    print(document)\n</code></pre> <p>En todo caso, estos ejemplos quedan fuera del alcance de nuestro objetivo para este curso. Para m\u00e1s informaci\u00f3n MongoDB Documentation \u2192 MongoDB Drivers \u2192 Node.js</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#agregaciones","title":"Agregaciones","text":"<p>Las agregaciones en MongoDB son una poderosa herramienta que permite procesar y analizar grandes vol\u00famenes de documentos en una colecci\u00f3n. A trav\u00e9s de un proceso conocido como pipeline de agregaci\u00f3n, puedes realizar diversas operaciones en los datos, como filtrar, agrupar, ordenar y transformar documentos. Aqu\u00ed te explico m\u00e1s sobre su funcionamiento y usos:</p> <p>Las agregaciones permiten realizar c\u00e1lculos y transformaciones sobre los datos. En lugar de simplemente recuperar documentos, puedes aplicar funciones que devuelven resultados calculados, como sumas, promedios, conteos, etc. Esto es similar a las consultas SQL que utilizan <code>GROUP BY</code>.</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#usos-y-ventajas-de-las-agreraciones","title":"Usos y ventajas de las agreraciones","text":"<ol> <li> <p>Agrupaci\u00f3n de Datos: Puedes agrupar documentos por un campo espec\u00edfico y realizar c\u00e1lculos sobre esos grupos. Por ejemplo, sumar las ventas por cada vendedor.</p> </li> <li> <p>Filtrado de Datos: Utilizando la etapa <code>$match</code>, puedes filtrar documentos antes de realizar otras operaciones, lo que optimiza el rendimiento.</p> </li> <li> <p>Transformaci\u00f3n de Datos: Puedes remodelar documentos usando la etapa <code>$project</code>, lo que te permite seleccionar y renombrar campos.</p> </li> <li> <p>An\u00e1lisis de Tendencias: Las agregaciones son \u00fatiles para analizar cambios en los datos a lo largo del tiempo, como el crecimiento de ventas mensuales.</p> </li> <li> <p>Operaciones Complejas: Puedes realizar operaciones m\u00e1s complejas, como uniones entre colecciones, utilizando la etapa <code>$lookup</code>.</p> </li> </ol> <p>Las agregaciones son esenciales para obtener <code>insights</code> significativos de tus datos en MongoDB, acotando el rango de los datos sobre los que trabajamos y facilitando el an\u00e1lisis y la toma de decisiones. </p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#metodo-aggregate","title":"M\u00e9todo <code>.aggregate()</code>","text":"<p>Para poder agrupar datos y realizar c\u00e1lculos sobre \u00e9stos, MongoDB ofrece diferentes alternativas una de ellas es mediante el m\u00e9todo <code>.aggretate</code></p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#pipeline-de-agregacion","title":"Pipeline de agregaci\u00f3n","text":"<p>Las agregaciones usan un pipeline, conocido como Aggregation Pipeline, de ah\u00ed que el m\u00e9todo aggregate use un array con [ ] donde cada elemento es una fase del pipeline, de modo que la salida de una fase es la entrada de la siguiente:</p> <pre><code>db.coleccion.aggregate([op1, op2, ... opN])\n</code></pre> <p>En la siguiente imagen se resumen los pasos de una agrupaci\u00f3n donde primero se eligen los elementos que vamos a agrupar mediante <code>$match</code>, el resultado saliente se agrupan con <code>$group</code>, y sobre los agrupado mediante <code>$sum</code> se calcula el total:</p> Ejemplo de pipeline con $match y $group <p>Al realizar un pipeline dividimos las consultas en fases, donde cada fase utiliza un operador para realizar una transformaci\u00f3n. Aunque no hay l\u00edmite en el n\u00famero de fases en una consulta, es importante destacar que el orden importa, y que hay optimizaciones para ayudar a que el pipeline tenga un mejor rendimiento (por ejemplo, hacer un <code>$match</code> al principio para reducir la cantidad de datos)</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#operadores-del-pipeline","title":"Operadores del pipeline","text":"<p>Antes de nada cabe destacar que las fases se pueden repetir, por lo que una consulta puede repetir operadores para encadenar diferentes acciones.</p> <p>A continuaci\u00f3n vamos a estudiar todos estos operadores:</p> Operador Descripci\u00f3n Cardinalidad <code>$project</code> Proyecci\u00f3n de campos, es decir, propiedades en las que estamos interesados. Tambi\u00e9n nos permite modificar un documento, o crear un subdocumento (reshape) 1:1 <code>$match</code> Filtrado de campos, similar a where N:1 <code>$group</code> Para agrupar los datos, similar a group by N:1 <code>$sort</code> Ordenar 1:1 <code>$skip</code> Saltar N:1 <code>$limit</code> Limitar los resultados N:1 <code>$unwind</code> Separa los datos que hay dentro de un array 1:N"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#ejemplos-de-uso","title":"Ejemplos de uso","text":"<p>La fase group agrupa los documentos con el prop\u00f3sito de calcular valores agregados de una colecci\u00f3n de documentos. Por ejemplo, podemos usar $group para calcular la cantidad de peliculas por tipo.</p> <pre><code>db.movies.aggregate([\n    { $group: {\n        _id: \"$type\",\n        total: { $sum:1}\n      }\n    }\n])\n</code></pre> <p>Warning</p> <p>Cuidado: La salida de <code>$group</code> esta desordenada</p> <p>La salida de <code>$group</code> depende de c\u00f3mo se definan los grupos. Se empieza especificando un identificador (por ejemplo, un campo <code>_id</code>) para el grupo que creamos con el pipeline. Para este campo <code>_id</code>, podemos especificar varias expresiones, incluyendo un \u00fanico campo proveniente de un documento del pipeline, un valor calculado de una fase anterior, un documento con muchos campos y otras expresiones v\u00e1lidas, tales como constantes o campos de subdocumentos. </p> <p>Note</p> <p>Cuando referenciemos al valor de un campo lo haremos poniendo entre comillas un <code>$</code> delante del nombre del campo. As\u00ed pues, para referenciar al fabricante de un producto lo haremos mediante <code>$fabricante</code>.</p> <p>Si lo que queremos es que el valor del identificador contenga un objeto, lo podemos asociar como valor:</p> <pre><code>db.movies.aggregate([\n    { $group: {\n        _id: { \"Tipo\" : \"$type\"},\n        total: { $sum:1}\n      }\n    }\n])\n</code></pre> <p>Tambi\u00e9n podemos agrupar m\u00e1s de un atributo, de tal modo que tengamos un <code>_id</code> compuesto. Por ejemplo:</p> <pre><code>db.movies.aggregate([\n    { $group: {\n        _id: { \n            \"Tipo\" : \"$type\",\n            \"G\u00e9nero\": \"$genres\"},\n        total: { $sum:1}\n      }\n    }\n])\n</code></pre> <p>El operador <code>$sum</code> acumula los valores y devuelve la suma. En los ejemplos anteriores, sumaba 1 por cada elemento contrado, pero podr\u00eda sumar cualquier valor num\u00e9rico. </p> <p>Por ejemplo, calculamos las nominaciones y premios ganados de las peliculas seg\u00fan el tipo: </p> <pre><code>db.movies.aggregate([\n    { $group: {\n        _id: { \"Tipo\" : \"$type\"},\n        Nominaciones: { $sum: \"$awards.nominations\"},\n        Ganadas: { $sum: \"$awards.wins\"}\n\n      }\n    }\n])\n</code></pre> <p>No solo tenemos el operados <code>$sume</code>; en la siguiente tabla se indican algunos de los operadores acumuladores m\u00e1s comunes que puedes utilizar en el m\u00e9todo <code>.aggregate</code> de MongoDB:</p> Operador Descripci\u00f3n Ejemplo <code>$sum</code> Suma de valores. <code>{$group: { _id: null, total: { $sum: \"$cantidad\" } }}</code> <code>$avg</code> Promedio de valores. <code>{$group: { _id: \"$categoria\", promedio: { $avg: \"$precio\" } }}</code> <code>$min</code> Valor m\u00ednimo. <code>{$group: { _id: \"$categoria\", minimo: { $min: \"$precio\" } }}</code> <code>$max</code> Valor m\u00e1ximo. <code>{$group: { _id: \"$categoria\", maximo: { $max: \"$precio\" } }}</code> <code>$push</code> A\u00f1ade valores a un array. <code>{$group: { _id: \"$categoria\", items: { $push: \"$nombre\" } }}</code> <code>$addToSet</code> A\u00f1ade valores \u00fanicos a un array. <code>{$group: { _id: \"$categoria\", itemsUnicos: { $addToSet: \"$nombre\" } }}</code> <code>$first</code> Primer valor en el grupo. <code>{$group: { _id: \"$categoria\", primerValor: { $first: \"$nombre\" } }}</code> <code>$last</code> \u00daltimo valor en el grupo. <code>{$group: { _id: \"$categoria\", ultimoValor: { $last: \"$nombre\" } }}</code> <code>$stdDevPop</code> Desviaci\u00f3n est\u00e1ndar de la poblaci\u00f3n de los valores. <code>{$group: { _id: \"$categoria\", desviacion: { $stdDevPop: \"$precio\" } }}</code> <code>$stdDevSamp</code> Desviaci\u00f3n est\u00e1ndar de la muestra de los valores. <code>{$group: { _id: \"$categoria\", desviacion: { $stdDevSamp: \"$precio\" } }}</code> <p>Estos operadores son muy \u00fatiles para realizar c\u00e1lculos y transformaciones en los datos dentro de un pipeline de agregaci\u00f3n en MongoDB MongoDB Aggregation Operators</p> <p>Por ejemplo, mediante <code>$avg</code> podemos obtener el promedio de los valores de un campo num\u00e9rico.</p> <ul> <li>Calculamos la media de la puntuaci\u00f3n por pa\u00eds</li> </ul> <pre><code>db.movies.aggregate([\n    { $group: {\n        _id: { \"Pais\" : \"$countries\"},\n        Cantidad: { $sum: 1},\n        Puntuaci\u00f3n: { $avg: \"$imdb.rating\"}\n      }\n    }\n])\n</code></pre> <p>El operador <code>$sort</code> ordena los documentos recibidos por el campo, y el orden indicado por la expresi\u00f3n indicada al pipeline.</p> <p>Si queremos ordenar el listado anterior por la cantidad de pel\u00edculas descendentemente (-1):</p> <pre><code>db.movies.aggregate([\n    { $group: {\n        _id: { \"Pais\" : \"$countries\"},\n        Cantidad: { $sum: 1},\n        Puntuaci\u00f3n: { $avg: \"$imdb.rating\"}\n      }\n    },\n    { $sort: {Cantidad: -1}}\n])\n</code></pre> <p>El operador <code>$match</code> se utiliza principalmente para filtrar los documentos que pasar\u00e1n a la siguiente etapa del pipeline o a la salida final.</p> <p>Por ejemplo, filtramos las pel\u00edculas que lanzadas a partir de 2015:</p> <pre><code>db.movies.aggregate([\n    { $match: { \n        released: {$gt: new Date('2015-01-01')}\n      }\n    },\n    { $group: {\n        _id: { \"Pais\" : \"$countries\"},\n        Cantidad: { $sum: 1},\n        Puntuaci\u00f3n: { $avg: \"$imdb.rating\"}\n      }\n    },\n    { $sort: {Cantidad: -1}}\n])\n</code></pre> <p>Observar el concepto de pipeline con el siguiente ejemplo. Vamos a contabilizar las pel\u00edculas lanzadas desde 2015 por paises, pero solo si hay al menos 30 peliculas de cada pais. Para esto, a\u00f1adimos tras el c\u00e1lculo de la <code>Cantidad</code> un nuevo filtro:</p> <pre><code>db.movies.aggregate([\n    { $match: { \n        released: {$gt: new Date('2015-01-01')}\n      }\n    },\n    { $group: {\n        _id: { \"Pais\" : \"$countries\"},\n        Cantidad: { $sum: 1},\n        Puntuaci\u00f3n: { $avg: \"$imdb.rating\"}\n      }\n    },\n    { $match: { \n        Cantidad: { $gt: 30 }\n      }\n    },\n    { $sort: {Cantidad: -1}}\n])\n</code></pre> <p>Si queremos realizar una proyecci\u00f3n sobre el conjunto de resultados y quedarnos con un subconjunto de los campos usaremos el operador <code>$project</code>. Como resultado obtendremos el mismo n\u00famero de documentos, y en el orden indicado en la proyecci\u00f3n.</p> <p>Veamos el resultado del siguiente ejemplo:</p> <p>Donde:</p> <ol> <li>Ocultamos el campo <code>_id</code></li> <li>Pasamos un campo a may\u00fasculas</li> <li>Agrupamos varios campos</li> <li>Renombramos un campo</li> <li>Realizamos operaciones matem\u00e1ticas</li> <li>Obtenemos un n\u00famero determinado de documentos, aunque el m\u00e9todo <code>$limit</code> no es un modificador de <code>$project</code></li> </ol> <p>As\u00ed pues, como hemos visto el abanico de posiblidades es muy extenso, aunque en nuestro curso no tenemos tiempo suficiente para profundizar en todas las posibilidades, nos quedamos aqui. </p> <p>Para m\u00e1s informaci\u00f3n tienes el Manual de MongoDB: Aggregation Operations</p>"},{"location":"MongoDB/7_mongodb%20OperacionesDatos/#base-de-datos-de-ejemplo","title":"Base de datos de ejemplo","text":"<ol> <li> <p>Bases de datos que ofrece directamente MongoDB:    https://github.com/neelabalan/mongodb-sample-dataset/tree/main/</p> </li> <li> <p>Datos de AirBnB de Valencia: Inside AirBnB</p> </li> </ol> <p>Tabla con enlaces a datos a fecha de 22/06/2024</p> Country/City File Name Description Valencia listings.csv.gz Detailed Listings data Valencia calendar.csv.gz Detailed Calendar Data Valencia reviews.csv.gz Detailed Review Data"},{"location":"PowerBI/PowerBI_ExamenesOpenWebinars/","title":"Ex\u00e1menes de OpenWebinars:","text":""},{"location":"PowerBI/PowerBI_ExamenesOpenWebinars/#02-curso-de-power-bi-preparacion-de-los-datos","title":"02. Curso de Power BI: Preparaci\u00f3n de los datos","text":"<p>\u00bfCu\u00e1l de las siguientes afirmaciones es cierta? - Importando un Dataset como Datasource, te proporciona los mismos reports que otros existentes. - Los datasets, son un conjunto de datos generados \u00fanicamente por PowerBI cuando se aplican optimizaciones de carga. - El Dataset de nuestro PowerBI nunca podr\u00e1 ser reutilizado para la creaci\u00f3n de nuevos reports.. - Importando un Dataset obtienes los c\u00e1lculos en DAX del dataset original. <code>&lt;----</code></p> <p>Power Query Editor nos permite... - Navegar, definir y realizar operaciones de transformaci\u00f3n de datos en un origen de datos. <code>&lt;----</code> - Conectarnos a m\u00faltiples datasource como fuente de origen. - Todas las respuestas son correctas. - Generar Datasets en PBI Service.</p> <p>A la hora de importar datos con Excel, \u00bfes posible crear relaciones entre tablas de manera autom\u00e1tica? - S\u00ed, PowerBi, te genera por defecto las relaciones que encuentra en funci\u00f3n del nombre de las columnas.  <code>&lt;----</code> - S\u00ed, pero solamente crea relaciones tipo estrella. - No, ya que hay que definir previamente las primary keys de las tablas. - No, cuando utilizamos un datasource de Excel, esta funci\u00f3n no est\u00e1 habilitada.</p> <p>\u00bfQu\u00e9 es Direct Query? - Es una t\u00e9cnica de Modelado de Datos, m\u00e1s eficiente que Import Query. - M\u00e9todo de conectividad de datos, que mantiene los datos en la base de datos origen, y lanza queries para obtener los datos necesarios. <code>&lt;----</code> - Es un tipo de DataSource de los que dispone PowerBI. - Acci\u00f3n disponible desde PowerBI Service, para conectarse a fuentes de datos de tipo bases de datos.</p> <p>Tratamos de cargar datos a trav\u00e9s de una p\u00e1gina web, pero el servicio web est\u00e1 ca\u00eddo, \u00bfc\u00f3mo podemos a\u00f1adir eficiencia en los tiempos de consulta? - Ninguna de las otras respuestas. - Creando par\u00e1metros \"What-If\" en nuestra URL. - Creando schedule refresh desde el Power Service. - A\u00f1adiendo un \"Comand Time out\" en minutos, para establecer un tiempo de consulta m\u00e1ximo.  <code>&lt;----</code></p> <p>\u00bfPuedo a\u00f1adir una columna condicional dentro de PowerQuery Editor? - S\u00ed, aunque solamente es posible si estamos importando un fichero Excel. - S\u00ed, aunque solamente es posible si estamos utilizando el m\u00e9todo de Direct Query. - No. - S\u00ed, dentro la pesta\u00f1a \"Add Column\", encontramos la opci\u00f3n a\u00f1adir columna condicional. <code>&lt;----</code></p> <p>\u00bfCu\u00e1ndo nos aparece el editor de consultas en PowerBI? - Cuando queremos editar consultas. - Cuando cargamos nuevos datos. - Todas las respuestas son correctas. <code>&lt;----</code> - Cuando queremos crear nuevas consultas.</p> <p>\u00bfUtilizando Direct Query necesito activar el schedule refresh desde PowerBI Service para actualizar los datos? - S\u00ed, pero es necesario hacerlo desde el Desktop. - S\u00ed, se podr\u00e1 actualizar hasta 8 veces al d\u00eda. - No. <code>&lt;----</code> - S\u00ed, solamente ser\u00e1 necesario una vez al d\u00eda.</p> <p>\u00bfQu\u00e9 es el Dataset de nuestro PowerBI? - El Dataset es un visual en PowerBI, que te permite visualizar datos en cualquier informe. - El Dataset es un m\u00e9todo de conectividad de datos m\u00e1s eficiente que el Import Query. - Todas las respuestas son ciertas. - Dataset es un conjunto de datos que se utilizan para crear visualizaciones e informes en PowerBI. <code>&lt;----</code></p> <p>\u00bfCu\u00e1l de las siguientes acciones no es posible realizar en PowerQuery Editor? - Crear una nueva tabla e introducir manualmente los datos que desees. - Ejecutar un script en Python. <code>&lt;---- **Incorrecta**</code> - Modificar el tipo de datos de cualquier columna. - Modificar el m\u00e9todo de conectividad de Import Query a Direct Query.</p> <p>Si queremos conectarnos a una base de datos SQL, \u00bfc\u00f3mo podemos hacerlo?: - Mediante los modos de conectividad Direct Query e Import Query. - Mediante modo de conectividad de Direct Query. - Podemos hacer un mix mediante Direct Query e Import Query para optimizar recursos. <code>&lt;---- **Incorrecta**</code> - Mediante modo de conectividad de Import Query.</p> <p>Cuando cargamos una tabla (mediante Import Query), \u00bftenemos la opci\u00f3n de transformar los datos antes de cargarlos? - S\u00ed, aunque esta opci\u00f3n solo est\u00e1 disponible para las licencias de pago. - No, primera hay que cargar la tabla en memoria, y luego podremos transformar los datos que queramos.  - Todas las respuestas son incorrectas. - No, es mejor modificar/transformar los datos previamente en origen, y luego importarlos directamente a PowerBI. <code>&lt;----**Incorrecta**</code></p> <p>\u00bfQu\u00e9 es PowerBi DataFlow? - Es un conjunto de entities que se crean y administran dentro de nuestros Workspace en PowerBI Service. - Todas las respuestas anteriores son ciertas. <code>&lt;----**Incorrecta**</code> - PowerBI Dataflow nos permite reutilizar grandes flujos de datos en formato CDM. - PowerBi DataFlow es un datasource disponible en PowerBI Desktop.</p> <p>\u00bfCu\u00e1l/es de las siguientes acciones est\u00e1 disponible en PowerQuery Editor? - Podemos escoger las columnas que nos interesan mantener. - Utilizar la primera fila del fichero/tabla como cabecera de nuestra tabla. - Todas las respuestas son ciertas. <code>&lt;----</code> - Podemos eliminar las filas de la tabla que no nos interesen.</p> <p>Para detectar el datatype de las columnas, de un fichero TXT, disponemos de las opciones... - Podemos elegir no detectar el tipo de datos de las columnas. - Utilizando todas las filas del fichero del documento.  - Todas las respuestas son correctas. <code>&lt;----</code> - Bas\u00e1ndose en las primeras 200 filas del documento.</p> <p>\u00bfC\u00f3mo funciona la extracci\u00f3n de datos mediante Direct Query? - Deja los datos en la base de datos de origen, y ejecuta querys para cargar la informaci\u00f3n necesaria. <code>&lt;----</code> - Genera cubos virtuales (MOLAP), que son almacenados en Power BI Azure. - PowerBI genera ficheros .JSON, donde los datos son codificados por temas de seguridad. - Extrae la informaci\u00f3n de tu database y la introduce dentro del Power BI Service.</p> <p>\u00bfPara qu\u00e9 sirve Applied Steps dentro de PowerQuery Editor? - Todas las otras respuestas son ciertas. <code>&lt;----</code> - Nos permite modificar el Origen (Source) de nuestra tabla. - Nos permite visualizar en que order se han aplicado transformaciones en nuestra tabla. - Nos permite eliminar uno o varios pasos aplicados en nuestra tabla.</p> <p>Cuando nos conectamos a una base de datos SQL, las \"Advanced Options\" nos permiten... - Utilizando el modo de conectividad Import query, adem\u00e1s nos permite incluir relaciones entre columnas en nuestro relationship model. - Crear una quick measure que se adapte a nuestra base de datos. - A\u00f1adir un timeout, para establecer un l\u00edmite de tiempo en respuesta para nuestras consultas. <code>&lt;----</code> - Los dataSource de bases de datos de SQL, no permiten utilizar las advanced options.</p> <p>Cuando estamos importando datos en PowerBI, y los queremos transformar, \u00bfqu\u00e9 ventana emergente nos aparece? - Power Query Editor. <code>&lt;----</code> - Transform Editor Query. - Import Query Editor - Direct Query Editor.</p> <p>A la hora de importar datos con un TXT, \u00bfqu\u00e9 par\u00e1metros son necesarios para detectar las columnas del archivo?: - Relationships entre columnas. - Timeout de consulta. - Ninguna de las otras respuestas. - Delimitador del archivo. <code>&lt;----</code></p>"},{"location":"PowerBI/PowerBI_Notas/","title":"PowerBI","text":"PowerBI Tabla de contenidos"},{"location":"PowerBI/PowerBI_Notas/#1-que-es-powerbi","title":"1. \u00bfQu\u00e9 es PowerBI?","text":"<p>Es una herramienta de Bussines Inteligence, pero no de Big Data, por el motivo de que todo lo guardar en memora.</p> <p>De hecho, es capaz de hacer un poco de ETL, te puedes conectar a muchas bases de datos y ficheros, pero al final, se tiene un conjunto de datos y trabaja sobre ellos, no soporta que estos datos se actualicen en caliente.</p>"},{"location":"PowerBI/PowerBI_Notas/#2-bases-de-datos-de-ejemplos","title":"2. Bases de datos de Ejemplos:","text":""},{"location":"PowerBI/PowerBI_Notas/#21-contoso-sales-sample-for-powerbi","title":"2.1. Contoso Sales Sample for PowerBI","text":"<p>Esta base de datos la proporciona Microsoft para poder hacer pruebas con PowerBI.</p> <p>La encontramos en la web de (Microsoft)[https://www.microsoft.com/en-us/download/details.aspx?id=46801]</p>"},{"location":"PowerBI/PowerBI_Notas/#22-adventureworks-sample-databases-para-sql-server-express","title":"2.2. AdventureWorks sample databases para SQL Server (Express)","text":"<p>Tambi\u00e9n podemos encontrar esta base de datos en la Web de (Microsoft Learn)[https://learn.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver15&amp;tabs=ssms]</p>"},{"location":"PowerBI/PowerBI_Notas/#preparacion-de-datos","title":"Preparaci\u00f3n de datos","text":""},{"location":"PowerBI/PowerBI_Notas/#datassource-de-tipo-database-sql-server","title":"DatasSource de tipo Database: SQL Server","text":""},{"location":"PowerBI/PowerBI_Notas/#import-query","title":"<code>Import Query</code>","text":"<p>Los datos los extrae de SQL y los mete en PowerBI.  Cualquier cambio se hace solo en PowerBI, no en SQL server. Se puede actualizar hasta 8 veces al d\u00eda, no m\u00e1s. Tiene Limitaci\u00f3n de 1GB, no se puede m\u00e1s.</p> <p>Se hace el ejercicio: 1. Meter una tabla con los valors de Ventas de internet y la Categoria de product Category.  2. A continuaci\u00f3n se va a SQL Server y se modifican las cantidades... podemos ver que se han modificado en SQL Server pero no en PowerBI. 3. Ser hace una actualizaci\u00f3n de los valores de la tabla. Con '<code>Home-&gt;Actualizar</code>' podemos actualizar todas las tablas. Esto es excesivo, porque sabemos que solo se han modificado una tabla. Hacemos sobre la parte de <code>Datos</code> en los tres botones de la tabla y le damos a <code>actualizar datos</code> y se actualizan y ahora ya salen los datos nuevos de la tabla 4. Todo esto se puede automatizar, para hacer refresco cada tiempo en el Schedules. Se puede hacer a nivel de tabla o de toda la BBDD.</p> <p>Recordar que se han actualizado, porque lo hemos hecho manual.</p>"},{"location":"PowerBI/PowerBI_Notas/#direct-query","title":"<code>Direct Query</code>","text":"<p>En este caso los datos no estan en la memoria de PowerBI. Constantemente se est\u00e1n lanzando Querys sobre la BBDD. Cada vez que detecta datos nuevos, los actualiza en los reports que tenemos.</p> <p>Esta es la opci\u00f3n que debemos elegir si queremos Reportes en Vivo</p> <p>Desventajas: - No se pueden manipular los datos en PowerBI Ventajas: - La actualizaci\u00f3n es en vivo y no cuando lo ordenemos.</p> <p>En las opciones Avanzadas, tenemos un <code>timeout</code>, tal vez es adecuado si lo conectamos de forma Directa, asignar un timeout para que un problema con una tabla no deje todo congelado.</p> <p>Para hacer una prueba, conectamos de nuevo las mismas tablas que en el caso anterior: - DimDate - DimProduct - DimProductCategory - DimProductSubCategory - FactInternetSales</p> <p>Pero ahora de forma directa y seguimos los siguientes pasos: 1. Creamos una medida r\u00e1pida.  2.    </p> <ol> <li>Lo anterior, parece que antes daba problemas y solo se pod\u00eda hacer por ImporQuery, pero ahora parece que ya va sin problemas</li> <li>En la parte inferior, en la barra de status, tenemos la opci\u00f3n de cambiar de Direct a Import (no se puede al rev\u00e9s).</li> <li>Realmente, lo que pasa es que esta haciendo una copia tipo <code>import</code> de la tabla para poder hacer esta medida. Esto se ver\u00e1 mas tarde, porque si intentamos utilizar este campo, no se actualizar\u00e1n los datos, y si no lo utilizamos se actualizar\u00e1n.</li> <li>Hacemos el mismo ejemplo que antes y mostramos las ventas por categor\u00eda.</li> <li>Ahora modificamos de nuevo los datos de SQL Server.</li> <li>Como se ha comentado, los datos se actualizan directamente, salvo que se utilicen datos como el de medida creado anteriormente.</li> </ol> <p>Veamos las diferencias ente los dos sistemas:</p>"},{"location":"PowerBI/PowerBI_Notas/#fichero-excel","title":"Fichero Excel","text":"<p>Tenemos ejemplos para hacer pruebas que proporciona (Microsoft en GitHub)[https://github.com/microsoft/powerbi-desktop-samples/tree/main/powerbi-service-samples]</p>"},{"location":"PowerBI/PowerBI_Notas/#web","title":"Web","text":"<p>Es rese\u00e1ble cuando la fuentes es Web, porque coge una p\u00e1gina y es capaz de leer las tablas y poder incorporar esas tablas a PowerBI.</p> <p>Por ejemplo la demo se hace con la web de la Wikipedia Wikipedia: Anexo: Comunidades y Ciudades aut\u00f3nomas de Espa\u00f1a</p>"},{"location":"PowerBI/PowerBI_Notas/#optimizacion-de-la-carga-de-datos","title":"Optimizaci\u00f3n de la carga de datos","text":"<p>HAy ciertos aspectos que puede mejorar la fluidez del sistema, por ejemplo: - Utilizar una query donde obtenemos todos los datos, en lugar de importar un mont\u00f3n de tablas. Ahorramos tiempo y espacio - Si tenemos columnas o datos de m\u00e1s, eliminarlos. - Asegurarnos de que los tipos de datos de las columnas son las correctas. - Evitar tener filtros en los informes, ya que al final ralentiza. Si tenemos muchos filtros, al intentar aplicar uno, PowerBI lo que va a hacer es revisar todos los filtros; perdemos eficiencia. - Optimizar funcionalidades mediante DAX, donde tambi\u00e9n hay filtrados y en este caso hay ciertos filtros que permiten optimizar, esto es porque para ejecutar una funci\u00f3n, primero coge todos los datos una vez filtrados y a partir de ah\u00ed comienza a calcular; si antes filtramos, el c\u00e1lculo es m\u00e1s r\u00e1pido. - Intentar mostrar informaci\u00f3n con la granularidad necesaria, o sea, no mostrar informaci\u00f3n de m\u00e1s o tablas con muchos registros</p> <p>Por \u00faltimo, si todo lo anterior no funciona, PowerBI tiene un analizador de rendimiento, que una vez activada, ejecutamos un informe y analiza los tiempos que precisa cada una de sus partes, as\u00ed a partir de ah\u00ed podemos ver d\u00f3nde tenemos un uso excesivo de tiempo</p>"},{"location":"PowerBI/PowerBI_Notas/#power-query-editor","title":"<code>Power Query Editor</code>","text":"<p>Se abre en varios casos.</p> <p>Un ejercicio es importar datos y revisar - Cambio de datos - Pasar may\u00fasculas a min\u00fasculas - Split o dividir celdas - A\u00f1adir una columna condicional</p> <p>El ejercicio que hace lo hace con una base de dato desconocida, pero esta bien hacer el ejercicio con la base de datos de las comunidades aut\u00f3nomas de la opci\u00f3n de Web, hay que reemplazar unos valores (&amp;) y dividir columnas, y cambiar tipos.</p> <p>Muy interesante la web de Wikipedia: Anexo:Evoluci\u00f3n demogr\u00e1fica de los municipios de Espa\u00f1a para incorporar todas las tablas que tiene y trabajar con ellas</p>"},{"location":"PowerBI/PowerBI_Notas/#power-bi-dataflow","title":"Power BI Dataflow","text":"<p>Solo esta disponible en la versi\u00f3n Desktop</p> <p>Utiliza todos los datos y los almacena como si fueran entidades o tabla y son gestionados por el Power BI Service (aunque no se pueden hacer en el).</p> <p>Toda la parte de ETL se podr\u00eda sustituir por Power BI Dataflow</p>"},{"location":"PowerBI/PowerBI_Notas/#power-bi-dataset","title":"Power BI DataSet","text":"<p>Un PBIX hac\u00edan que un DataSet fuera como un DataSource.</p> <p>Dentro del Power BI Service, se pueden acceder a todos los DataSets que tenemos, y descargarlos como <code>pbix</code> o cualquier otro formato</p>"},{"location":"noSQL/1_NoSQL%20Intro/","title":"1. Introducci\u00f3n","text":"<p>Se puede decir que estamos en la tercera plataforma del almacenamiento de datos. La primera lleg\u00f3 con los primeros computadores y se materializ\u00f3 en las bases de datos jer\u00e1rquicas y en red, as\u00ed como en el almacenamiento ISAM. La segunda vino de la mano de Internet y las arquitecturas cliente-servidor, lo que dio lugar a las bases de datos relacionales.</p> <p>La tercera se ve motivada por el Big Data, los dispositivos m\u00f3viles, las arquitecturas cloud, las redes de IoT y las tecnolog\u00edas/redes sociales. Es tal el volumen de datos que se genera que aparecen nuevos paradigmas como NoSQL, NewSQL y las plataformas de Big Data. En esta sesi\u00f3n nos vamos a centrar en NoSQL.</p> <p>NoSQL aparece como una necesidad debida al creciente volumen de datos sobre usuarios, objetos y productos que las empresas tienen que almacenar, as\u00ed como la frecuencia con la que se accede a los datos. Los SGDB relacionales existentes no fueron dise\u00f1ados teniendo en cuenta la escalabilidad ni la flexibilidad necesaria por las frecuentes modificaciones que necesitan las aplicaciones modernas; tampoco aprovechan que el almacenamiento a d\u00eda de hoy es muy barato, ni el nivel de procesamiento que alcanzan las m\u00e1quinas actuales.</p> Evoluci\u00f3n del volumen de datos. <p>La soluci\u00f3n es el despliegue de las aplicaciones y sus datos en cl\u00fasteres de servidores, distribuyendo el procesamiento en m\u00faltiples m\u00e1quinas.</p> <p>Fuentes utilizadas</p> <p>Las principales fuentes consultadas para la realizaci\u00f3n de esta secci\u00f3n han sido:</p> <ul> <li>Aitor Medrano. Cursos Inteligencia Artificial y Big Data. Almacen de datos NoSQL</li> <li>Next Generation Databases: NoSQL, NewSQL, and Big Data</li> <li>NoSQL Distilled: A Brief Guide to the Emerging World of Polyglot Persistence</li> <li>Row vs Column Oriented Databases</li> <li>Understanding Database Sharding</li> </ul>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/","title":"2. No Solo SQL","text":""},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#definicion-nosql","title":"Definici\u00f3n NoSQL","text":"<p>Si definimos NoSQL formalmente, podemos decir que se trata de un conjunto de tecnolog\u00edas que permiten el procesamiento r\u00e1pido y eficiente de conjuntos de datos dando la mayor importancia al rendimiento, la fiabilidad y la agilidad.</p> <p>Si nos basamos en el acr\u00f3nimo, el t\u00e9rmino da la sensaci\u00f3n que se refiere a cualquier almac\u00e9n de datos que no sigue un modelo relacional, los datos no son relacionales y por tanto no utilizan SQL como lenguaje de consulta. Realmente implica que el No hace referencia a not only, es decir, que los sistemas NoSQL se centran en sistemas complementarios a los SGBD relacionales, que fijan sus prioridades en la escalabilidad y la disponibilidad en contra de la atomicidad y consistencia de los datos.</p> <p>Es decir, m\u00e1s que sustitutos de los sistemas relacionales, las soluciones NoSQL se plantean como alternativas y complementarias a los sistemas gestores de bases de datos relacionales.</p> <p>ACID</p> <p>Las bases de datos relacionales cumplen las caracter\u00edsticas ACID para ofrecer transaccionalidad sobre los datos: - Atomicidad: las transacciones implican que se realizan todas las operaciones o no se realiza ninguna. - Consistencia: la base de datos asegura que los datos pasan de un estado v\u00e1lido o otro tambi\u00e9n. - Isolation (Aislamiento): Una transacci\u00f3n no afecta a otras transacciones, de manera que la modificaci\u00f3n de un registro / documento no es visible por otras lecturas. - Durabilidad: La escritura de los datos asegura que una vez finalizada una operaci\u00f3n, los datos no se perder\u00e1n.  </p> <p>Los diferentes tipos de bases de datos NoSQL existentes se pueden agrupar en cuatro categor\u00edas:</p> <ul> <li> <p>Clave-Valor: Los almacenes clave-valor son las bases de datos NoSQL m\u00e1s simples. Cada elemento de la base de datos se almacena con un nombre de atributo (o clave) junto a su valor, a modo de diccionario. Los almacenes m\u00e1s conocidos son Redis, Riak y AWS DynamoDB. Algunos almacenes, como es el caso de Redis, permiten que cada valor tenga un tipo (por ejemplo, integer) lo cual a\u00f1ade funcionalidad extra.</p> </li> <li> <p>Documentales: Cada clave se asocia a una estructura compleja que se conoce como documento. Este puede contener diferentes pares clave-valor, o pares de clave-array o incluso documentos anidados, como en un documento JSON. Los ejemplos m\u00e1s conocidos son MongoDB, CouchDB o Elastic Search.</p> </li> <li> <p>Grafos: Los almacenes de grafos se usan para almacenar informaci\u00f3n sobre redes, como pueden ser conexiones sociales. Los ejemplos m\u00e1s conocidos son Neo4j, Amazon Neptune y ArangoDB.</p> </li> <li> <p>Basados en columnas: Los almacenes basados en columnas como Hypertabla de Google, Apache Cassandra y Apache HBase est\u00e1n optimizados para consultas sobre grandes conjuntos de datos, y almacenan los datos como columnas en vez de como filas.</p> </li> </ul>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#caracteristicas","title":"Caracter\u00edsticas","text":"<p>Si nos centramos en sus beneficios y los comparamos con las base de datos relacionales, las bases de datos NoSQL son m\u00e1s escalables, ofrecen un rendimiento mayor y sus modelos de datos resuelven varios problemas que no se plantearon al definir el modelo relacional:</p> <ul> <li>Grandes vol\u00famenes de datos estructurados, semi-estructurados y sin estructurar. Casi todas las implementaciones NoSQL ofrecen alg\u00fan tipo de representaci\u00f3n para datos sin esquema, lo que permite comenzar con una estructura y con el paso del tiempo, a\u00f1adir nuevos campos, ya sean sencillos o anidados a datos ya existentes.</li> <li>Sprints \u00e1giles, iteraciones r\u00e1pidas y frecuentes commits/pushes de c\u00f3digo, al emplear una sintaxis sencilla para la realizaci\u00f3n de consultas y la posibilidad de tener un modelo que vaya creciendo al mismo ritmo que el desarrollo.</li> <li>Arquitectura eficiente y escalable dise\u00f1ada para trabajar con clusters en vez de una arquitectura monol\u00edtica y costosa. Las soluciones NoSQL soportan la escalabilidad de un modo transparente para el desarrollador y ofrecen una soluci\u00f3n cloud.</li> </ul> <p>Una caracter\u00edstica adicional que comparten los sistemas NoSQL es que ofrecen un mecanismo de cach\u00e9 de datos integrado (en los sistemas relacionales se pueden configurar de manera externa), pudiendo configurar los sistemas para que los datos se mantengan en memoria y se persistan de manera peri\u00f3dica. El uso de una cach\u00e9 conlleva que la consistencia de los datos no sea completa y podamos tener una consistencia eventual.</p>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#esquemas-dinamicos","title":"Esquemas din\u00e1micos","text":"<p>Las bases de datos relacionales requieren definir los esquemas antes de a\u00f1adir los datos. Una base de datos SQL necesita saber de antemano los datos que vamos a almacenar; por ejemplo, si nos centramos en los datos de un cliente, ser\u00edan el nombre, apellidos, n\u00famero de tel\u00e9fono, etc\u2026\u200b</p> <p>Esto casa bastante mal con los enfoques de desarrollo \u00e1gil, ya que cada vez que a\u00f1adimos nuevas funcionalidades, el esquema de la base de datos suele cambiar. De modo que si a mitad de desarrollo decidimos almacenar los productos favoritos de un cliente del cual guard\u00e1bamos su direcci\u00f3n y n\u00fameros de tel\u00e9fono, tendr\u00edamos que a\u00f1adir una nueva columna a la base de datos y migrar la base de datos entera a un nuevo esquema.</p> <p>Si la base de datos es grande, conlleva un proceso lento que implica parar el sistema durante un tiempo considerable. Si frecuentemente cambiamos los datos que la aplicaci\u00f3n almacena (al usar un desarrollo iterativo), tambi\u00e9n tendremos per\u00edodos frecuentes de inactividad del sistema, a no ser que utilicemos un despliegue azul/verde y tengamos redundancia de nuestro sistema de almacenamiento. As\u00ed pues, no hay un modo efectivo mediante una base de datos relacional de almacenar los datos que est\u00e1n desestructurados o que no se conocen de antemano.</p> <p>Las bases de datos NoSQL se construyen para permitir la inserci\u00f3n de datos sin un esquema predefinido. Esto facilita la modificaci\u00f3n de la aplicaci\u00f3n en tiempo real, sin preocuparse por interrupciones de servicio. Aunque no tengamos un esquema al guardar la informaci\u00f3n, s\u00ed que podemos definir esquemas de lectura (schema-on-read) para comprobar que la informaci\u00f3n almacenada tiene el formato que espera cargar cada aplicaci\u00f3n.</p> <p>De este modo se consigue un desarrollo m\u00e1s r\u00e1pido, integraci\u00f3n de c\u00f3digo m\u00e1s robusto y menos tiempo empleado en la administraci\u00f3n de la base de datos.</p> <p>Aunque lo veremos en profundidad en las siguientes sesiones, los modelos de datos NoSQL priman la redundancia de los datos, denormalizando los datos para evitar el uso de joins. Por ello, es importante que la definici\u00f3n de los esquemas sea flexible para poder a\u00f1adir campos conforme la aplicaci\u00f3n evolucione.</p>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#particionado","title":"Particionado","text":"<p>Dado el modo en el que se estructuran las bases de datos relacionales, normalmente escalan verticalmente mediante un \u00fanico servidor que almacena toda la base de datos para asegurar la disponibilidad continua de los datos. Esto se traduce en costes que se incrementan r\u00e1pidamente, con un l\u00edmites definidos por el propio hardware, y en un peque\u00f1o n\u00famero de puntos cr\u00edticos de fallo dentro de la infraestructura de datos.</p> <p>La soluci\u00f3n es escalar horizontalmente, a\u00f1adiendo nuevos servidores en vez de concentrarse en incrementar la capacidad de un \u00fanico servidor, lo que permite tratar con conjuntos de datos m\u00e1s grandes de lo que ser\u00eda capaz cualquier m\u00e1quina por s\u00ed sola. Este escalado horizontal se conoce como Sharding o Particionado.</p> <p>El particionado no es \u00fanico de las bases de datos NoSQL. Las bases de datos relacionales tambi\u00e9n lo soportan. Si en un sistema relacional queremos particionar los datos, podemos distinguir entre particionado:</p> <ul> <li>Horizontal: diferentes filas en diferentes particiones.</li> <li>Vertical: diferentes columnas en particiones distintas.</li> </ul> <p>En el caso de las bases de datos NoSQL, el particionado depende del modelo de la base de datos:</p> <ul> <li>Los almacenes clave-valor y las bases de datos documentales normalmente se particionan horizontalmente.</li> <li>Las bases de datos basados en columnas se pueden particionar horizontal o verticalmente.</li> </ul> <p>Escalar horizontalmente una base de datos relacional entre muchas instancias de servidores se puede conseguir pero normalmente conlleva el uso de SANs (Storage Area Networks) y otras triqui\u00f1uelas para hacer que el hardware act\u00fae como un \u00fanico servidor.</p> <p>Como los sistemas SQL no ofrecen esta prestaci\u00f3n de forma nativa, los equipos de desarrollo se las tienen que ingeniar para conseguir desplegar m\u00faltiples bases de datos relacionales en varias m\u00e1quinas. Para ello:</p> <ul> <li>Los datos se almacenan en cada instancia de base de datos de manera aut\u00f3noma</li> <li>El c\u00f3digo de aplicaci\u00f3n se desarrolla para distribuir los datos y las consultas y agregar los resultados de los datos a trav\u00e9s de todas las instancias de bases de datos</li> <li>Se debe desarrollar c\u00f3digo adicional para gestionar los fallos sobre los recursos, para realizar joins entre diferentes bases de datos, balancear los datos y/o replicarlos, etc\u2026\u200b</li> </ul> <p>Adem\u00e1s, muchos beneficios de las bases de datos como la integridad transaccional se ven comprometidos o incluso eliminados al emplear un escalado horizontal.</p>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#auto-sharding","title":"Auto-sharding","text":"<p>Por contra, las bases de datos NoSQL normalmente soportan auto-sharding, lo que implica que de manera nativa y autom\u00e1ticamente se dividen los datos entre un n\u00famero arbitrario de servidores, sin que la aplicaci\u00f3n sea consciente de la composici\u00f3n del pool de servidores. Los datos y las consultas se balancean entre los servidores.</p> <p>El particionado se realiza mediante un m\u00e9todo consistente, como puede ser:</p> <ul> <li>Por rangos de su id: por ejemplo \"los usuarios del 1 al mill\u00f3n est\u00e1n en la partici\u00f3n 1\" o \"los usuarios cuyo nombre va de la A a la L\" en una partici\u00f3n, en otra de la M a la Q, y de la R a la Z en la tercera.</li> </ul> <ul> <li> <p>Por listas: dividiendo los datos por la categor\u00eda del dato, es decir, en el caso de datos sobre libros, las novelas en una partici\u00f3n, las recetas de cocina en otra, etc..</p> </li> <li> <p>Mediante un funci\u00f3n hash, la cual devuelve un valor para un elemento que determine a qu\u00e9 partici\u00f3n pertenece.</p> </li> </ul>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#cuando-particionar","title":"Cuando particionar","text":"<p>El motivo para particionar los datos se debe a:</p> <ul> <li>limitaciones de almacenamiento: los datos no caben en un \u00fanico servidor, tanto a nivel de disco como de memoria RAM.</li> <li>rendimiento: al balancear la carga entre particiones las escrituras ser\u00e1n m\u00e1s r\u00e1pidas que al centrarlas en un \u00fanico servidor.</li> <li>disponibilidad: si un servidor esta ocupado, otro servidor puede devolver los datos. La carga de los servidores se reduce.</li> </ul> <p>No particionaremos los datos cuando la cantidad sea peque\u00f1a, ya que el hecho de distribuir los datos conlleva unos costes que pueden no compensar con un volumen de datos insuficiente. Tampoco esperaremos a particionar cuando tengamos much\u00edsimos datos, ya que el proceso de particionado puede provocar sobrecarga del sistema.</p> <p>La nube facilita de manera considerable este escalado, mediante proveedores como AWS o Azure los cuales ofrecen virtualmente una capacidad ilimitada bajo demanda, y despreocup\u00e1ndose de todas las tareas necesarias para la administraci\u00f3n de la base de datos.</p> <p>Los desarrolladores ya no necesitamos construir plataformas complejas para nuestras aplicaciones, de modo que nos podemos centrar en escribir c\u00f3digo de aplicaci\u00f3n. Una granja de servidores con commodity hardware puede ofrecer el mismo procesamiento y capacidad de almacenamiento que un \u00fanico servidor de alto rendimiento por mucho menos coste.</p>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#replicacion","title":"Replicaci\u00f3n","text":"<p>La replicaci\u00f3n mantiene copias id\u00e9nticas de los datos en m\u00faltiples servidores, lo que facilita que las aplicaciones siempre funcionen y los datos se mantengan seguros, incluso si alguno de los servidores sufre alg\u00fan problema.</p> <p>La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n soportan la replicaci\u00f3n autom\u00e1tica, lo que implica una alta disponibilidad y recuperaci\u00f3n frente a desastres sin la necesidad de aplicaciones de terceros encargadas de ello. Desde el punto de vista del desarrollador, el entorno de almacenamiento es virtual y ajeno al c\u00f3digo de aplicaci\u00f3n.</p> <p>Existen dos formas de realiza la replicaci\u00f3n:</p>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#maestro-esclavo-primario-secundario","title":"Maestro-esclavo / Primario-secundario","text":"<p>Todas las escrituras se realizan en el nodo principal y despu\u00e9s se replican a los nodos secundarios. El nodo primario es un SPOF (single point of failure).</p>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#peer-to-peer","title":"Peer-to-Peer","text":"<p>Todos los nodos tienen el mismo nivel jer\u00e1rquico, de manera que todos admiten escrituras. Al poder haber escrituras simult\u00e1neas sobre el mismo datos en diferentes nodos, pueden darse inconsistencia en los datos.</p> <p>La replicaci\u00f3n de los datos se utiliza para alcanzar:</p> <ul> <li>escalabilidad, incrementando el rendimiento al poder distribuir las consultas en diferentes nodos, y mejorar la redundancia al permitir que cada nodo tenga una copia de los datos.</li> <li>disponibilidad, ofreciendo tolerancia a fallos de hardware o corrupci\u00f3n de la base de datos. Al replicar los datos vamos a poder tener una copia de la base de datos, dar soporte a un servidor de datos agregados, o tener nodos a modo de copias de seguridad que pueden tomar el control en caso de fallo.</li> <li>aislamiento (la i en ACID - isolation), entendido como la propiedad que define cuando y c\u00f3mo al realizar cambios en un nodo se propagan al resto de nodos. Si replicamos los datos podemos crear copias sincronizadas para separar procesos de la base de datos de producci\u00f3n, pudiendo ejecutar informes, anal\u00edtica de datos o copias de seguridad en nodos secundarios de modo que no tenga un impacto negativo en el nodo principal, as\u00ed como ofrecer un sistema sencillo para separar el entorno de producci\u00f3n del de preproducci\u00f3n.</li> </ul> <p>Replicaci\u00f3n vs Particionado</p> <p>No hay que confundir la replicaci\u00f3n (copia de los datos en varias m\u00e1quinas) con el particionado (cada m\u00e1quina tiene un subconjunto de los datos). El entorno m\u00e1s seguro y con mejor rendimiento es aquel que tiene los datos particionados y replicados (cada m\u00e1quina que tiene un subconjunto de los datos est\u00e1 replicada en 2 o m\u00e1s).</p>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#implantando-nosql","title":"Implantando NoSQL","text":"<p>Normalmente, las empresas empezar\u00e1n con una prueba de baja escalabilidad de una base de datos NoSQL, de modo que les permita comprender la tecnolog\u00eda asumiendo muy poco riesgo. La mayor\u00eda de las bases de datos NoSQL tambi\u00e9n son open-source, y por tanto se pueden probar sin ning\u00fan coste extra. Al tener unos ciclos de desarrollo m\u00e1s r\u00e1pidos, las empresas pueden innovar con mayor velocidad y mejorar la experiencia de sus cliente a un menor coste.</p> <p>Elegir la base de datos correcta para el proyecto es un tema importante. Se deben considerar las diferentes alternativas a las infraestructuras legacy teniendo en cuenta varios factores:</p> <ul> <li>la escalabilidad o el rendimiento m\u00e1s all\u00e1 de las capacidades del sistema existente.</li> <li>identificar alternativas viables respecto al software propietario.</li> <li>incrementar la velocidad y agilidad del proceso de desarrollo.</li> </ul> <p>As\u00ed pues, al elegir un base de datos hemos de tener en cuenta las siguientes dimensiones:</p> <ul> <li>modelo de datos: A elegir entre un modelo documental, basado en columnas, de grafos o mediante clave-valor.</li> <li>modelo de consultas: Dependiendo de la aplicaci\u00f3n, puede ser aceptable un modelo de consultas que s\u00f3lo accede a los registros por su clave primaria. En cambio, otras aplicaciones pueden necesitar consultar por diferentes valores de cada registro. Adem\u00e1s, si la aplicaci\u00f3n necesita modificar los registros, la base de datos necesita consultar los datos por un \u00edndice secundario.</li> <li>modelo de consistencia: Los sistemas NoSQL normalmente mantienen m\u00faltiples copias de los datos para ofrecer disponibilidad y escalabilidad al sistema, lo que define la consistencia del mismo. Los sistemas NoSQL tienden a ser consistentes o eventualmente consistentes.</li> <li>APIs: No existe un est\u00e1ndar para interactuar con los sistemas NoSQL. Cada sistema presenta diferentes dise\u00f1os y capacidades para los equipos de desarrollo. La madurez de un API puede suponer una inversi\u00f3n en tiempo y dinero a la hora de desarrollar y mantener el sistema NoSQL.</li> <li> <p>soporte comercial y de la comunidad: Los usuarios deben considerar la salud de la compa\u00f1\u00eda o de los proyectos al evaluar una base de datos. El producto debe evolucionar y mantenerse para introducir nuevas prestaciones y corregir fallos. Una base de datos con una comunidad fuerte de usuarios:</p> <ul> <li>permite encontrar y contratar desarrolladores con destrezas en el producto.</li> <li>facilita encontrar informaci\u00f3n, documentaci\u00f3n y ejemplos de c\u00f3digo.</li> <li>ayuda a las empresas a retener el talento.</li> <li>favorece que otras empresas de software integren sus productos y participen en el ecosistema de la base de datos.</li> </ul> </li> </ul>"},{"location":"noSQL/2_NoSQL%20NoSoloSQL/#casos-de-uso","title":"Casos de uso","text":"<p>Una vez conocemos los diferentes sistemas y qu\u00e9 elementos puede hacer que nos decidamos por una soluci\u00f3n u otra, conviene repasar los casos de uso m\u00e1s comunes:</p> <ul> <li>si vamos a crear una aplicaci\u00f3n web cuyo campos sean personalizables, usaremos una soluci\u00f3n documental.</li> <li>como una capa de cach\u00e9, mediante un almac\u00e9n clave-valor.</li> <li>para almacenar archivos binarios sin preocuparse de la gesti\u00f3n de permisos del sistema de archivos, y poder realizar consultas sobre sus metadatos, ya sea mediante una soluci\u00f3n documental o un almac\u00e9n clave-valor.</li> <li>para almacenar un enorme volumen de datos, donde la consistencia no es lo m\u00e1s importante, pero si la disponibilidad y su capacidad de ser distribuida, mediante una soluci\u00f3n documental o basada en columnas.</li> </ul>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/","title":"3. Modelos de datos","text":""},{"location":"noSQL/3_NoSQL%20ModeloDatos/#modelos-de-datos","title":"Modelos de Datos","text":"<p>La principal clasificaci\u00f3n de los sistemas de bases de datos NoSQL se realiza respecto a los diferentes modelos de datos:</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#documental","title":"Documental","text":"<p>Mientras las bases de datos relacionales almacenan los datos en filas y columnas, las bases de datos documentales emplean documentos. Estos documentos utilizan una estructura JSON, ofreciendo un modo natural e intuitivo para modelar datos de manera similar a la orientaci\u00f3n a objetos, donde cada documento es un objeto.</p> <p>Los documentos se agrupan en colecciones o bases de datos, dependiendo del sistema, lo que permite agrupar documentos.</p> <p>Los documentos contienen uno o m\u00e1s campos, donde cada campo contiene un valor con un tipo, ya sea cadena, entero, flotante, fecha, binario o array u otro documento. En vez de extender los datos entre m\u00faltiples columnas y tablas, cada registro y sus datos asociados se almacenan de manera unida en un \u00fanico documento. Esto simplifica el acceso a los datos y reduce (y en ocasiones elimina) la necesidad de joins y transacciones complejas.</p> <p>Dicho de otra manera, en las bases de datos documentales, los datos que van juntos y se emplean juntos, se almacenan juntos.</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#caracteristicas","title":"Caracter\u00edsticas","text":"<p>En una base de datos documental, la noci\u00f3n de esquema es din\u00e1mico: cada documento puede contener diferentes campos. Esta flexibilidad puede ser \u00fatil para modelar datos desestructurados y polim\u00f3rficos, lo que facilita la evoluci\u00f3n del desarrollo al permitir a\u00f1adir nuevos campos de manera din\u00e1mica.</p> <p>Perfectamente podemos tener dos documentos que pertenecen a la misma colecci\u00f3n, pero con atributos diferentes. Por ejemplo, un primer documento puede ser el siguiente:</p> <pre><code>{   \n    \"_id\": \"BW001\",   \n    \"nombre\": \"Bruce\",   \n    \"apellido\": \"Wayne\",   \n    \"edad\": 35,   \n    \"salario\": 10000000 \n}\n</code></pre> <p>Mientras que un segundo documento dentro de la misma colecci\u00f3n podr\u00eda ser:</p> <pre><code>{\n    \"_id\": \"JK1\",   \n    \"nombre\": \"Joker\",   \n    \"edad\": 34,   \n    \"salario\": 5000000,   \n    \"direccion\":                        (1)\n    {               \n        \"calle\": \"Asilo Arkham\",    \n        \"ciudad\": \"Gotham\"   \n    },   \n    \"proyectos\":                        (2)\n    [                  \n        \"desintoxicacion-virus\",     \n        \"top-secret-007\"   \n    ] \n}\n</code></pre> <p>Donde:   1.  Un objeto o subdocumento permite agrupar informaci\u00f3n similar a una relaci\u00f3n 1:1 de un modelo relacional. De esta manera, no necesitamos una tabla <code>Direccion</code>.  2.  Un array puede contener valores o documentos, de manera que podr\u00edamos tener un array de documentos, permitiendo agrupar informaci\u00f3n similar a una relaci\u00f3n 1:N de un modelo relacional. De esta manera, no necesitamos una tabla <code>Proyectos</code>. </p> <p>Normalmente, cada documento contiene un elemento clave, sobre el cual se puede obtener un documento de manera un\u00edvoca. De todos modos, las bases de datos documentales ofrecen un completo mecanismo de consultas, posibilitando obtener informaci\u00f3n por cualquier campo del documento. Algunos productos ofrecen opciones de indexado para optimizar las consultas, como pueden ser \u00edndices compuestos, dispersos, con tiempo de vida (TTL), \u00fanicos, de texto o geoespaciales.</p> <p>Adem\u00e1s, estos sistemas ofrecen productos que permiten analizar los datos, mediante funciones de agregaci\u00f3n o implementaci\u00f3n de MapReduce.</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#casos-de-uso","title":"Casos de uso","text":"<p>Las bases de datos documentales sirven para prop\u00f3sito general, v\u00e1lidos para un amplio abanico de aplicaciones gracias a la flexibilidad que ofrece el modelo de datos, lo que permite consultar cualquier campo y modelar de manera natural de manera similar a la programaci\u00f3n orientada a objetos.</p> <p>Entre los casos de \u00e9xito de estos sistemas cabe destacar:</p> <ul> <li>Sistemas de flujo de eventos: entre diferentes aplicaciones dentro de una empresa</li> <li>Gestores de Contenido, plataformas de Blogging: al almacenar los documentos mediante JSON, facilita la estructura de datos para guardar los comentarios, registros de usuarios, etc\u2026\u200b</li> <li>Anal\u00edticas Web, datos en Tiempo Real: al permitir modificar partes de un documento, e insertar nuevos atributos a un documento cuando se necesita una nueva m\u00e9trica</li> <li>Aplicaciones eCommerce: conforme las aplicaciones crecen, el esquema tambi\u00e9n lo hace</li> </ul> <p>Si nos centramos en aquellos casos donde no conviene este tipo de sistemas podemos destacar:</p> <ul> <li>Sistemas operacionales con transacciones complejas.</li> <li>Sistemas con consultas agregadas que modifican su estructura. Si los criterios de las consultas no paran de cambiar, acabaremos normalizando los datos.</li> </ul> <p>Los productos m\u00e1s destacados son:</p> <ul> <li>MongoDB: http://www.mongodb.com. </li> <li>CouchDB: http://couchdb.apache.org</li> </ul>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#clave-valor","title":"Clave-Valor","text":"<p>Un almac\u00e9n clave-valor es una simple tabla hash donde todos los accesos a la base de datos se realizan a trav\u00e9s de la clave primaria.</p> <p>Desde una perspectiva de modelo de datos, los almacenes de clave-valor son los m\u00e1s b\u00e1sicos.</p> <p>Su funcionamiento es similar a tener una tabla relacional con dos columnas, por ejemplo <code>id</code> y <code>nombre</code>, siendo <code>id</code> la columna utilizada como clave y <code>nombre</code> como valor. Mientras que en una base de datos en el campo <code>nombre</code> s\u00f3lo podemos almacenar datos de tipo cadena o num\u00e9rico, en un almac\u00e9n clave-valor, el valor puede ser de un dato simple o un objeto. En muchos casos, se almacena un objeto binario BLOB (Binary Large Object). Cuando una aplicaci\u00f3n accede mediante la clave y el valor, se almacenan el par de elementos. Si la clave ya existe, el valor se modifica.</p> <p>El cliente puede tanto obtener el valor por la clave, asignar un valor a una clave o eliminar una clave del almac\u00e9n. El valor, sin embargo, es opaco al sistema, el cual no sabe que hay dentro de \u00e9l, ya que los datos s\u00f3lo se pueden consultar por la clave, lo cual puede ser un inconveniente. As\u00ed pues, la aplicaci\u00f3n es responsable de saber qu\u00e9 hay almacenado en cada valor.</p> <p>Por ejemplo, Riak utiliza el concepto de bucket (cubo) como una manera de agrupar claves, de manera similar a una tabla.</p> <p>Por ejemplo, Riak permite interactuar con la base de datos mediante peticiones HTTP:</p> <pre><code>curl -v -X PUT &lt;http://localhost:8091/riak/heroes/ace&gt; -H \"Content-Type: application/json\" -d {\"nombre\" : \"Batman\", \"color\" : \"Negro\"}\n</code></pre> <p>Algunos almacenes clave-valor, como puede ser Redis, permiten almacenar datos con cualquier estructura, como por ejemplos listas, conjuntos, hashes, permitiendo realizar operaciones como la intersecci\u00f3n, uni\u00f3n, diferencia y rango.</p> <p>Comandos Redis <pre><code>SET nombre \"Bruce Wayne\"      // String \nHSET heroe nombre \"Batman\"    // Hash \u2013 set \nHSET heroe color \"Negro\" \nSADD \"heroe:amigos\" \"Robin\" \"Alfred\"   // Set \u2013 create/update\n</code></pre></p> <p>Comandos Python</p> <pre><code>import redis \nr = redis.Redis() \nr.mset({\"Croatia\": \"Zagreb\", \"Bahamas\": \"Nassau\"}) \nr.get(\"Bahamas\") # b'Nassau'\n</code></pre> <p>Estas prestaciones hacen que Redis se extrapole a \u00e1mbitos ajenos a un almac\u00e9n clave-valor. Otra caracter\u00edstica que ofrecen algunos almacenes es que permiten crear un segundo nivel de consulta o incluso definir m\u00e1s de una clave para un mismo objeto.</p> <p>Como los almacenes clave-valor siempre utilizan accesos por clave primaria, de manera general tienen un gran rendimiento y son f\u00e1cilmente escalables.</p> <p>Si queremos que su rendimiento sea m\u00e1ximo, pueden configurarse para que mantengan la informaci\u00f3n en memoria y que se serialice de manera peri\u00f3dica, a costa de tener una consistencia eventual de los datos.</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#diferencias-entre-modelo-documental-y-clave-valor","title":"Diferencias entre modelo Documental y Clave-Valor","text":"<p>Los modelos de datos Documental y Clave-Valor son dos paradigmas comunes en las bases de datos NoSQL. Pueden parecer similares pero existen unas diferencias claras entre ellos: </p> <ul> <li> <p>Estructura de datos: En el modelo documental, los datos se organizan en documentos con una estructura interna, mientras que en el modelo clave-valor, los datos se almacenan como pares de clave-valor simples sin una estructura interna definida.</p> </li> <li> <p>Flexibilidad: El modelo documental ofrece m\u00e1s flexibilidad para almacenar datos semiestructurados o no estructurados, mientras que el modelo clave-valor es m\u00e1s adecuado para datos simples y bien definidos.</p> </li> <li> <p>Consultas: El modelo documental permite consultas m\u00e1s complejas y flexibles utilizando \u00edndices secundarios y lenguajes de consulta avanzados, mientras que en el modelo clave-valor, la recuperaci\u00f3n de datos se realiza principalmente mediante b\u00fasquedas directas por clave.</p> </li> </ul>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#casos-de-uso_1","title":"Casos de uso","text":"<p>Este modelo es muy \u00fatil para representar datos desestructurados o polim\u00f3rficos, ya que no fuerzan ning\u00fan esquema m\u00e1s all\u00e1 de los pares de clave-valor.</p> <p>Entre los casos de uso de estos almacenes podemos destacar el almacenaje de:</p> <ul> <li>Informaci\u00f3n sobre la sesi\u00f3n de navegaci\u00f3n (sessionid)</li> <li>Perfiles de usuario, preferencias</li> <li>Datos del carrito de la compra</li> <li>Cachear datos</li> </ul> <p>Todas estas operaciones van a asociada a operaciones de recuperaci\u00f3n, modificaci\u00f3n o inserci\u00f3n de los datos de una sola vez, de ah\u00ed su elecci\u00f3n.</p> <p>En cambio, no conviene utilizar estos almacenes cuando queremos realizar:</p> <ul> <li>Relaciones entre datos</li> <li>Transacciones entre varias operaciones</li> <li>Consultas por los datos del valor</li> <li>Operaciones con conjuntos de claves</li> </ul> <p>Los almacenes m\u00e1s empleados son:</p> <ul> <li>Riak: https://riak.com</li> <li>Redis: http://redis.io</li> <li>AWS DynamoDB: http://aws.amazon.com/dynamodb</li> <li>Voldemort: http://www.project-voldemort.com/voldemort implementaci\u00f3n open-source de Amazon DynamoDB</li> </ul>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#basado-en-columnas","title":"Basado en columnas","text":"<p>Las bases de datos relacionales utilizan la fila como unidad de almacenamiento, lo que permite un buen rendimiento de escritura. Sin embargo, cuando las escrituras son ocasionales y es m\u00e1s com\u00fan tener que leer unas pocas columnas de muchas filas a la vez, es mejor utilizar como unidad de almacenamiento un grupos de columnas. Es decir, lo que hacemos es girar el modelo 90 grados, de manera que los registros se almacenan en columnas en vez de hacerlo por filas.</p> <p>Supongamos que tenemos los siguientes datos:</p> <p>Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n:</p> <p>En un formato columnar los datos del mismo tipo se agrupan, lo que permite codificarlos/comprimirlos, lo que mejora el rendimiento de acceso y reduce el tama\u00f1o:</p> <p>Sin embargo, a medida que se incrementa la utilizaci\u00f3n de an\u00e1lisis de datos en memoria, con soluciones como Spark, los beneficios relativos de la base de datos basadas en columnas comparados con los de las bases de datos orientadas a filas pueden llegar a ser menos importantes.</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#representacion","title":"Representaci\u00f3n","text":"<p>Un modelo basado en columnas se representa como una estructura agregada de dos niveles. El primer nivel formado por un almac\u00e9n clave-valor, siendo la clave el identificador de la fila, y el valor un nuevo mapa con los datos agregados de la fila (familias de columnas). Los valores de este segundo nivel son las columnas. De este modo, podemos acceder a los datos de un fila, o a una determinada columna:</p> <p>s</p> <p>BigTable</p> <p>Los modelos de datos basados en columnas se basan en la implementaci\u00f3n de Google de la tecnolog\u00eda BigTable (http://research.google.com/archive/bigtable.html), la cual consiste en columnas separadas y sin esquema, a modo de mapa de dos niveles.</p> <p>As\u00ed pues, los almacenes basados en columnas utilizan un mapa ordenado multi-dimensional y distribuido para almacenar los datos. Est\u00e1n pensados para que cada fila tenga una gran n\u00famero de columnas (del orden del mill\u00f3n), almacenando las diferentes versiones que tenga una fila (pudiendo almacenar del orden de miles de millones de filas).</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#familias-de-columnas","title":"Familias de columnas","text":"<p>Una columna consiste en un pareja <code>name</code>-<code>value</code>, donde el nombre hace de clave. Adem\u00e1s, contiene un atributo <code>timestamp</code> para poder expirar datos y resolver conflictos de escritura.</p> <p>Un ejemplo de columna podr\u00eda ser:</p> <pre><code>{\n    name: \"nombre\",   \n    value: \"Bruce\",   \n    timestamp: 12345667890 \n}\n</code></pre> <p>Una fila es una colecci\u00f3n de columnas agrupadas a una clave.</p> <pre><code>{\n    {         \n        name: \"nombre\",         \n        value: \"Bruce\",         \n        timestamp: 12345667890     \n    },     \n    {         \n        name: \"nombre\",         \n        value: \"Clark\",         \n        timestamp: 12345667891     \n    },     \n    {         \n        name: \"nombre\",         \n        value: \"Barbara\",         \n        timestamp: 12345667892     \n    } \n}\n</code></pre> <p>Si agrupamos filas similares tendremos una familia de columnas (similar al concepto de tabla):</p> <pre><code>// familia de columnas \n{   \n    // fila   \n    \"tim-gordon\" : \n    {     \n        nombre: \"Tim\",     \n        apellido: \"Gordon\",     \n        ultimaVisita: \"2015/12/12\"   \n    }   \n    // fila   \n    \"bruce-wayne\" : \n    {     \n        nombre: \"Bruce\",     \n        apellido: \"Wayne\",     \n        lugar: \"Gotham\"   \n    } \n}\n</code></pre> <p>Con este ejemplo, podemos ver como las diferentes filas de la misma tabla (familia de columnas) no tienen por que compartir el mismo conjunto de columnas.</p> <p>Adem\u00e1s, las columnas se pueden anidar dentro de otras formando super-columnas, donde el valor es un nuevo mapa de columnas.</p> <pre><code>{\n    name: \"libro:978-84-16152-08-7\",   \n    value: \n    {     \n        autor: \"Grant Morrison\",     \n        titulo: \"Batman - Asilo Arkham\",     \n        isbn: \"978-84-16152-08-7\"   \n    } \n}\n</code></pre> <p>Cuando se utilizan super columnas para crear familias de columnas tendremos una familia de super columnas.</p> <p>En resumen, las bases de datos basadas en columnas, almacenan los datos en familias de columnas como filas, las cuales tienen muchas columnas asociadas al identificador de una fila. Las familias de columnas son grupos de datos relacionados, a las cuales normalmente se accede de manera conjunta.</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#operaciones","title":"Operaciones","text":"<p>A la hora de consultar los datos, \u00e9stos se pueden obtener por la clave primaria de la familia. As\u00ed pues, podemos obtener toda una familia, o la columna de una familia:</p> <pre><code>// Mediante Cassandra \nGET Clientes['bruce-wayne'];            // familia \nGET Clientes['bruce-wayne']['lugar'];   // columna\n</code></pre> <p>Algunos productos ofrecen un soporte limitado para \u00edndices secundarios, pero con restricciones. Por ejemplo, Cassandra ofrece el lenguaje CQL similar a SQL pero sin joins, ni subconsultas donde las restricciones de where son sencillas:</p> <p><pre><code>SELECT * FROM Clientes \nSELECT nombre,email FROM Clientes \nSELECT nombre,email FROM Clientes WHERE lugar='Gotham'`\n</code></pre> Las actualizaciones se realizan en dos pasos: primero encontrar el registro y segundo modificarlo. En estos sistemas, una modificaci\u00f3n puede suponer una reescritura completa del registro independientemente que hayan cambiado unos pocos bytes del mismo.</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#casos-de-uso_2","title":"Casos de uso","text":"<p>Las bases de datos columnares se han empleado durante d\u00e9cadas ofreciendo beneficios a las aplicaciones de negocio modernas, como la anal\u00edtica de datos, business intelligence y data warehousing.</p> <p>Son multiprop\u00f3sito, aunque su uso se centra en el mercado del big data, la anal\u00edtica de datos, cubos multidimensionales OLAP y/o almacenar metadatos y realizar anal\u00edtica en tiempo real.</p> <p>Adem\u00e1s de poder comprimir los datos, los datos est\u00e1n auto-indexados, lo que implica que utiliza menos espacio en disco, y acelera la ejecuci\u00f3n de consultas agregadas entre m\u00faltiples tablas que implica el uso de joins.</p> <p>En cambio, no se recomienda su uso en aplicaciones de procesamiento transaccional (OLTP), ya que las bases de datos relacionales gestionan mejor el procesamiento concurrente y el aislamiento de las operaciones.</p> <p>Los productos m\u00e1s destacados son:</p> <ul> <li>HBase : http://hbase.apache.org, el cual se basa en Hadoop - http://hadoop.apache.org</li> <li>Cassandra : http://cassandra.apache.org</li> <li>Amazon Redshift: https://aws.amazon.com/es/redshift/</li> </ul>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#grafos","title":"Grafos","text":"<p>Las bases de datos de grafos almacenan entidades y las relaciones entre estas entidades. Las entidades se conocen como nodos, los cuales tienen propiedades. Cada nodo es similar a una instancia de un objeto. Las relaciones, tambi\u00e9n conocidas como v\u00e9rtices, a su vez tienen propiedades, y su sentido es importante.</p> <p>Los nodos se organizan mediante relaciones que facilitan encontrar patrones de informaci\u00f3n existente entre los nodos. Este tipo de organizaci\u00f3n permite almacenar los datos una vez e interpretar los datos de diferentes maneras dependiendo de sus relaciones.</p> <ul> <li>Los nodos son entidades que tienen propiedades, tales como el nombre. Por ejemplo, en el gr\u00e1fico cada nodo tiene una propiedad <code>name</code>. </li> <li>Tambi\u00e9n podemos ver que las relaciones tienen tipos, como <code>label</code>, <code>since</code>, etc\u2026\u200b Estas propiedades permiten organizar los nodos. </li> <li>Las relaciones pueden tener m\u00faltiples propiedades, y adem\u00e1s tienen direcci\u00f3n, con lo cual si queremos incluir bidireccionalidad tenemos que a\u00f1adir dos relaciones en sentidos opuestos. </li> <li>Tanto los nodos como las relaciones tienen un atributo <code>id</code> que los identifica.</li> </ul> <p>Por ejemplo, podemos comenzar a crear el grafo anterior mediante Neo4J de la siguiente manera:</p> <pre><code>Node alice = graphDb.createNode();\nalice.setProperty(\"name\", \"Alice\");\nNode bob = graphDb.createNode();\nbob.setProperty(\"name\", \"Bob\");\n\nalice.createRelationshipTo(bob, FRIEND);\nbob.createRelationshipTo(alice, FRIEND);\n</code></pre> <p>Los nodos permiten tener diferentes tipos de relaciones entre ellos y as\u00ed representar relaciones entre las entidades del dominio, y tener relaciones secundarias para caracter\u00edsticas como categor\u00eda, camino, \u00e1rboles de tiempo, listas enlazas para acceso ordenado, etc\u2026\u200b Al no existir un l\u00edmite en el n\u00famero ni en el tipo de relaciones que puede tener un nodo, todas se pueden representar en la misma base de datos.</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#traversing","title":"Traversing","text":"<p>Una vez tenemos creado un grafo de nodos y relaciones, podemos consultar el grafo de muchas maneras; por ejemplo \"obtener todos los nodos que son miembros del grupo de ajedrez y que tienen m\u00e1s de 20 a\u00f1os\". Realizar una consulta se conoce como hacer un traversing (recorrido) del mismo.</p> <p>Un ejemplo de traversing mediante Neo4J ser\u00eda:</p> <pre><code>Node ajedrez = nodeIndex.get(\"name\", \"chess\").getSingle();\nallRelationships = ajedrez.getRelationships(Direction.INCOMING);\n</code></pre> <p>Una ventaja a destacar de las bases de datos basadas en grafos es que podemos cambiar los requisitos de traversing sin tener que cambiar los nodos o sus relaciones.</p> <p>En las bases de datos de grafos, recorrer las relaciones es muy r\u00e1pido, ya que no se calculan en tiempo de consulta, sino que se persisten como una relaci\u00f3n, y por tanto no hay que hacer ning\u00fan c\u00e1lculo.</p> <p>En cambio, en una base de datos relacional, para crear una estructura de grafo se realiza para una relaci\u00f3n sencilla (\u00bfQuien es mi jefe?\"). Para poder a\u00f1adir otras relaciones necesitamos muchos cambios en el esquema y trasladar datos entre tablas. Adem\u00e1s, necesitamos de antemano saber qu\u00e9 consultas queremos realizar para modelar las tablas y las relaciones acorde a las consultas.</p> <p>As\u00ed pues, estos sistemas ofrecen modelos ricos de consultas donde se pueden investigar las relaciones simples y complejas entre los nodos para obtener informaci\u00f3n directa e indirecta de los datos del sistemas. Los tipos de an\u00e1lisis que se realizan sobre estos sistema se ci\u00f1en a los tipos de relaci\u00f3n existente entre los datos.</p>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#casos-de-uso_3","title":"Casos de uso","text":"<p>Mientras que el modelo de grafos no es muy intuitivo y tiene una importante curva de aprendizaje, se puede usar en un gran n\u00famero de aplicaciones.</p> <p>Su principal atractivo es que facilitan almacenar las relaciones entre entidades de una aplicaci\u00f3n, como por ejemplo en una red social, o las intersecciones existentes entre carreteras. Es decir, se emplean para almacenar datos que se representan como nodos interconectados.</p> <p>Por lo tanto, los casos de uso son:</p> <ul> <li>Datos conectados: redes sociales con diferentes tipos de conexiones entre los usuarios.</li> <li>Enrutamiento, entrega o servicios basados en la posici\u00f3n: si las relaciones almacenan la distancia entre los nodos, podemos realizar consultas sobre lugares cercanos, trayecto m\u00e1s corto, etc\u2026\u200b</li> <li>Motores de recomendaciones: de compras, de lugares visitados, etc\u2026\u200b</li> </ul> <p>En cambio, no se recomienda su uso cuando necesitemos modificar todos o un subconjunto de entidades, ya que modificar una propiedad en todos los nodos es una operaci\u00f3n compleja.</p> <p>Los productos m\u00e1s destacados son:</p> <ul> <li>Neo4j: http://neo4j.com</li> <li>ArangoDB: https://www.arangodb.com/</li> <li>Apache TinkerPop: https://tinkerpop.apache.org/</li> <li>Amazon Neptune: https://aws.amazon.com/es/neptune/</li> </ul>"},{"location":"noSQL/3_NoSQL%20ModeloDatos/#diferencias-entre-los-modelos","title":"Diferencias entre los modelos","text":"Diferencias entre modelos Documental Clave-Valor Basado en Columnas Grafos Estructura de Datos Documentos JSON/BSON/XML Pares de Clave-Valor Columnas con familias Nodos y Relaciones Flexibilidad Flexible Variable Menos flexible Variable Consultas Complejas, utilizando \u00edndices secundarios y lenguajes de consulta avanzados B\u00fasquedas directas por clave Consultas ad-hoc limitadas Consultas complejas de relaciones Escalabilidad Escalabilidad Horizontal Escalabilidad Horizontal Escalabilidad Horizontal Escalabilidad Horizontal y Vertical Transacciones Transacciones ACID Operaciones at\u00f3micas simples Transacciones ACID Transacciones ACID Ejemplos MongoDB, Couchbase Redis, DynamoDB Cassandra, HBase Neo4j, Amazon Neptune"},{"location":"noSQL/4_NoSQL%20Consistencia/","title":"4. Consistencia","text":""},{"location":"noSQL/4_NoSQL%20Consistencia/#consistencia","title":"Consistencia","text":"<p>En un sistema consistente, las escrituras de una aplicaci\u00f3n son visibles en siguientes consultas. Con una consistencia eventual, las escrituras no son visibles inmediatamente.</p> <p>Por ejemplo, en un sistema de control de stock, si el sistema es consistente, cada consulta obtendr\u00e1 el estado real del inventario, mientras que si tiene consistencia eventual, puede que no sea el estado real en un momento concreto pero terminar\u00e1 si\u00e9ndolo en breve.</p> Tipos de consistencia"},{"location":"noSQL/4_NoSQL%20Consistencia/#sistemas-consistentes","title":"Sistemas consistentes","text":"<p>Cada aplicaci\u00f3n tiene diferentes requisitos para la consistencia de los datos. Para muchas aplicaciones, es imprescindible que los datos sean consistentes en todo momento. Como los equipos de desarrollo han estado trabajo con un modelo de datos relacional durante d\u00e9cadas, este enfoque parece natural. Sin embargo, en otras ocasiones, la consistencia eventual es un traspi\u00e9s aceptable si conlleva una mayor flexibilidad en la disponibilidad del sistema.</p> <p>Las bases de datos documentales y de grafos pueden ser consistentes o eventualmente consistentes. Por ejemplo, MongoDB ofrece un consistencia configurable. De manera predeterminada, los datos son consistentes, de modo que todas las escrituras y lecturas se realizan sobre la copia principal de los datos. Pero como opci\u00f3n, las consultas de lectura, se pueden realizar con las copias secundarias donde los datos tendr\u00e1n consistencia eventual. La elecci\u00f3n de la consistencia se realiza a nivel de consulta.</p>"},{"location":"noSQL/4_NoSQL%20Consistencia/#sistemas-de-consistencia-eventual","title":"Sistemas de consistencia eventual","text":"<p>Con los sistemas eventualmente consistentes, hay un per\u00edodo de tiempo en el que todas las copias de los datos no est\u00e1n sincronizados. Esto puede ser aceptable para aplicaciones de s\u00f3lo-lectura y almacenes de datos que no cambian frecuentemente, como los archivos hist\u00f3ricos. Dentro del mismo saco podemos meter las aplicaciones con alta tasa de escritura donde las lecturas sean poco frecuentes, como un archivo de log.</p> <p>Un claro ejemplo de sistema eventualmente consistente es el servicio DNS, donde tras registrar un dominio, puede tardar varios d\u00edas en propagar los datos a trav\u00e9s de Internet, pero siempre est\u00e1n disponibles aunque contenga una versi\u00f3n antigua de los datos.</p> <p>Respecto a las bases de datos NoSQL, los almacenes de clave-valor y los basados en columnas son sistemas eventualmente consistentes. Estos tienen que soportar conflictos en las actualizaciones de registros individuales.</p> <p>Como las escrituras se pueden aplicar a cualquier copia de los datos, puede ocurrir, y no ser\u00eda muy extra\u00f1o, que hubiese un conflicto de escritura.</p> <p>Algunos sistemas como Riak utilizan vectores de reloj para determinar el orden de los eventos y asegurar que la operaci\u00f3n m\u00e1s reciente gana en caso de un conflicto.</p> <p>Otros sistemas como CouchDB, retienen todos los valores conflictivos y permiten al usuario resolver el conflicto. Otro enfoque seguido por Cassandra sencillamente asume que el valor m\u00e1s grande es el correcto.</p> <p>Por estos motivos, las escrituras tienden a comportarse bien en sistemas eventualmente consistentes, pero las actualizaciones pueden conllevar sacrificios que complican la aplicaci\u00f3n.</p>"},{"location":"noSQL/4_NoSQL%20Consistencia/#teorema-cap","title":"Teorema CAP","text":"<p>Propuesto por Eric Brewer en el a\u00f1o 2000, prueba que podemos crear una base de datos distribuida que elija dos de las siguientes tres caracter\u00edsticas:</p> <ul> <li>Consistencia: las escrituras son at\u00f3micas y todas las peticiones posteriores obtienen el nuevo valor, independientemente del lugar de la petici\u00f3n.</li> <li>Disponibilidad (Available): la base de datos devolver\u00e1 siempre un valor. En la pr\u00e1ctica significa que no hay downtime.</li> <li>Tolerancia a Particiones: el sistema funcionar\u00e1 incluso si la comunicaci\u00f3n con un servidor se interrumpe de manera temporal (para ello, ha de dividir los datos entre diferentes nodos). Es decir, implica que se pueden recibir lecturas desde unos nodos que no contienen informaci\u00f3n escrita en otros.</li> </ul> <p>En otras palabras, podemos crear un sistema de base de datos que sea consistente y tolerante a particiones (CP), un sistema que sea disponible y tolerante a particiones (AP), o un sistema que sea consistente y disponible (CA). Pero no es posible crear una base de datos distribuida que sea consistente, disponible y tolerante a particiones al mismo tiempo.</p> Teorema de CAP <p>El teorema CAP es \u00fatil cuando consideramos el sistema de base de datos que necesitamos, ya que nos permite decidir cual de las tres caracter\u00edsticas vamos a descartar. La elecci\u00f3n realmente se centra entre la disponibilidad y la consistencia, ya que la tolerancia a particiones es una decisi\u00f3n de arquitectura (sea o no distribuida).</p> <p>Aunque el teorema dicte que si en un sistema distribuido elegimos disponibilidad no podemos tener consistencia, todav\u00eda podemos obtener consistencia eventual. Es decir, cada nodo siempre estar\u00e1 disponible para servir peticiones, aunque estos nodos no puedan asegurar que la informaci\u00f3n que contienen sea consistente (pero si bastante precisa), en alg\u00fan momento lo ser\u00e1.</p> <p>Algunas bases de datos tolerantes a particiones se pueden ajustar para ser m\u00e1s o menos consistentes o disponible a nivel de petici\u00f3n. Por ejemplo, Riak trabaja de esta manera, permitiendo a los clientes decidir en tiempo de petici\u00f3n qu\u00e9 nivel de consistencia necesitan.</p>"},{"location":"noSQL/4_NoSQL%20Consistencia/#clasificacion-segun-cap","title":"Clasificaci\u00f3n seg\u00fan CAP","text":"<p>El siguiente gr\u00e1fico muestra c\u00f3mo dependiendo de estos atributos podemos clasificar los sistemas NoSQL:</p> Clasificaci\u00f3n seg\u00fan teorema de CAP <p>As\u00ed pues, las bases de datos NoSQL se clasifican en:</p> <ul> <li>CP: Consistente y tolerantes a particiones. Tanto MongoDB como HBase son CP, ya que dentro de una partici\u00f3n pueden no estar disponibles para responder una determinada consulta (por ejemplo, evitando lecturas en los nodos secundarios), aunque son tolerantes a fallos porque cualquier nodo secundario se puede convertir en principal y asumir el rol del nodo ca\u00eddo.</li> <li>AP: Disponibles y tolerantes a particiones. DynamoDB permite replicar los datos entre sus nodos aunque no garantiza la consistencia en ninguno de los sus servidores.</li> <li>CA: Consistentes y disponible. Aqu\u00ed es donde situar\u00edamos a los SGDB relacionales. Por ejemplo, PostreSQL es CA (aunque ofrece un producto complementario para dar soporte al particionado, como PgCluster), ya que no distribuyen los datos y por tanto la partici\u00f3n no es una restricci\u00f3n.</li> </ul> <p>Lo bueno es que la gran mayor\u00eda de sistemas permiten configurarse para cambiar su tipo CAP, lo que permite que MongoDB pase de CP a AP, o CouchDB de AP a CP.</p>"},{"location":"noSQL/4_NoSQL%20Consistencia/#base","title":"BASE","text":"<p>De forma an\u00e1loga al modelo transaccional ACID para las bases de datos relacionales que dan soporte a la transaccionalidad ofreciendo en todo momento un sistema consistente, las bases de datos distribuidas siguen el modelo transaccional BASE, el cual se centra en la alta disponibilidad y significa:</p> <ul> <li>B\u00e1sicamente disponible (Basically Available): la base de datos siempre responde a las solicitudes recibidas, ya sea con una respuesta exitosa o con un error, a\u00fan en el caso de que el sistema soporte la tolerancia a particiones (de manera que caiga alg\u00fan nodo o no est\u00e9 accesible por problemas de la red). Esto puedo implicar lecturas desde nodos que no han recibido la \u00faltima escritura, por lo que el resultado puede no ser consistente.</li> <li>Estado blando (Soft State): la base de datos puede encontrarse en un estado inconsistente cuando se produce una lectura, de modo que es posible realizar dos veces la misma lectura y obtener dos resultados distintos a pesar de que no haya habido ninguna escritura entre ambas operaciones, sino que la escritura se hab\u00eda realizado antes en el tiempo y no se hab\u00eda persistido hasta ese momento.</li> <li>Consistencia eventual (Eventual consistency): tras cada escritura, la consistencia de la base de datos s\u00f3lo se alcanza una vez el cambio ha sido propagado a todos los nodos. Durante el tiempo que tarda en producirse la consistencia, observamos un estado blando de la base de datos.</li> </ul> <p>Una base de datos que sigue el modelo transaccional BASE prefiere la disponibilidad antes que la consistencia (es decir, desde el punto de vista del teorema CAP es AP).</p> ACID vs BASE."}]}